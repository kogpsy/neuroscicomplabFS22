---
title: "Reaktionszeiten: II"
description: |
  Anwendungen.
date: "2022-04-12"
author:
  - first_name: "Andrew"
    last_name: "Ellis"
    url: https://github.com/awellis
    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, UniversitÃ¤t Bern 
    affiliation_url: https://www.kog.psy.unibe.ch
    orcid_id: 0000-0002-2788-936X

citation_url: https://kogpsy.github.io/neuroscicomplab/response-times-2.html
# slug: ellis2021overview
bibliography: bibliography.bib
output: 
    distill::distill_article:
      toc: true
      toc_float: true
      toc_depth: 2
      code_folding: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, xaringanExtra-clipboard, echo=FALSE, include=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clone fa-2x\" style=\"color: #301e64\"></i>",
    success_text = "<i class=\"fa fa-check fa-2x\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times fa-2x\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

:::note
ðŸ‘‰ [R Code fÃ¼r dieses Kapitel downloaden](./files/response-times-2.R)

ðŸ‘‰ [Daten downloaden](./data/session-8.csv)
:::





# Bootstrap estimates

In the previous chapter, we used our knowledge of the sampling distribution of the mean to compute the standard error of the mean. But what if we canâ€™t assume that the estimates are normally distributed, or we donâ€™t know their distribution? The idea of the bootstrap is to use the data themselves to estimate an answer. The name comes from the idea of pulling oneâ€™s self up by oneâ€™s own bootstraps, expressing the idea that we donâ€™t have any external source of leverage so we have to rely upon the data themselves. The bootstrap method was conceived by Bradley Efron of the Stanford Department of Statistics, who is one of the worldâ€™s most influential statisticians.


> "The bootstrap is a computer-based method for assigning measures of accuracy to statistical estimates." Efron & Tibshirani, An introduction to the bootstrap, 1993

> "The central idea is that it may sometimes be better to draw conclusions about the characteristics of a population strictly from the sample at hand, rather than by making perhaps unrealistic assumptions about the population." Mooney & Duval, Bootstrapping, 1993


Like all bootstrap methods, the percentile bootstrap relies on a simple & intuitive idea: instead of making assumptions about the underlying distributions from which our observations could have been sampled, we use the data themselves to estimate sampling distributions. In turn, we can use these estimated sampling distributions to compute confidence intervals, estimate standard errors, estimate bias, and test hypotheses (Efron & Tibshirani, 1993; Mooney & Duval, 1993; Wilcox, 2012). The core principle to estimate sampling distributions is resampling, a technique pioneered in the 1960â€™s by Julian Simon (particularly inspiring is how he used dice and cards to teach resampling in statistics classes). The technique was developed & popularised by Brad Efron as the bootstrap.


Letâ€™s consider an example, starting with this small set of 10 observations:


```{r}
v <- c(0.4, 0.4, 0.4, 0.8, 0.8, 1.1, 1.2, 2.6, 2.6, 2.6)
```


```{r}
v_1 <- sample(v, replace = TRUE)
v_1
```

```{r}
v_2 <- sample(v, replace = TRUE)
v_2
```


Essentially, we are doing fake experiments using only the observations from our sample. And for each of these fake experiments, or bootstrap sample, we can compute any estimate of interest, for instance the median. Because of random sampling, we get different medians from different draws, with some values more likely than other. After repeating the process above many times, we get a distribution of bootstrap estimates, let say 1000 bootstrap estimates of the sample median. That distribution of bootstrap estimates is a data driven estimation of the sampling distribution of the sample median. Similarly, we can use resampling to estimate the sampling distribution of any statistics, without requiring any analytical formula. This is the major appeal of the bootstrap.

```{r}
library(bayesboot)
b1 <- bayesboot(v, mean )
```

```{r}
b1
```


```{r}
heights <- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185)
b1 <- bayesboot(heights, mean, R = 4000)
```

```{r}
f <- function(v) {
  vv <- sample(v, replace = TRUE)
  mean(vv)
  # tibble(mean = mean(vv))
         # q25 = quantile(vv, probs = 0.25),
         # q75 = quantile(vv, probs = 0.25))
}
```

```{r}
b2 <- replicate(n = 4000, f(heights))
```

```{r}

b3 <- map_dbl(1:4000, ~f(heights))
```


```{r}
d <- tibble(b1 = b1$V1, b2 = b2, b3 = b3) |> 
  pivot_longer(everything(), names_to = "bootstrap", values_to = "mean")
d |> 
  ggplot(aes(mean, fill = bootstrap)) + 
  geom_histogram() +
  facet_wrap(~bootstrap)
```


