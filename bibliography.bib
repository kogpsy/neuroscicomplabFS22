
@incollection{alexanderReciprocalInteractionsComputational2015,
  title = {Reciprocal {{Interactions}} of {{Computational Modeling}} and {{Empirical Investigation}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Alexander, William H. and Brown, Joshua W.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {321--338},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_16},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_16},
  urldate = {2019-03-20},
  abstract = {Models in general, and computational neural models in particular, are useful to the extent they fulfill three aims, which roughly constitute a life cycle of a model. First, at birth, models must account for existing phenomena, and with mechanisms that are no more complicated than necessary. Second, at maturity, models must make strong, falsifiable predictions that can guide future experiments. Third, all models are by definition incomplete, simplified representations of the mechanisms in question, so they should provide a basis of inspiration to guide the next generation of model development, as new data challenge and force the field to move beyond the existing models. Thus the final part of the model life cycle is a dialectic of model properties and empirical challenge. In this phase, new experimental data test and refine the model, leading either to a revised model or perhaps the birth of a new model. In what follows, we provide an outline of how this life cycle has played out in a particular series of models of the dorsal anterior cingulate cortex (ACC).},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Anterior cingulate cortex,Cognitive control,Computational neural model,Dialectic,Error likelihood,Performance monitoring,Reinforcement learning}
}

@article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  date = {2019-03},
  journaltitle = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-00857-9},
  url = {https://www.nature.com/articles/d41586-019-00857-9},
  urldate = {2022-05-11},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  issue = {7748},
  langid = {english},
  keywords = {Research data,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Research data, Research management}
}

@article{antonenkoTDCSinducedEpisodicMemory2019,
  title = {{{tDCS-induced}} Episodic Memory Enhancement and Its Association with Functional Network Coupling in Older Adults},
  author = {Antonenko, Daria and Hayek, Dayana and Netzband, Justus and Grittner, Ulrike and Flöel, Agnes},
  date = {2019-02-19},
  journaltitle = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {2273},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-38630-7},
  url = {https://www.nature.com/articles/s41598-019-38630-7},
  urldate = {2021-04-12},
  abstract = {Transcranial direct current stimulation (tDCS) augments training-induced cognitive gains, an issue of particular relevance in the aging population. However, negative outcomes have been reported as well, and few studies so far have evaluated the impact of tDCS on episodic memory formation in elderly cohorts. The heterogeneity of previous findings highlights the importance of elucidating neuronal underpinnings of tDCS-induced modulations, and of determining individual predictors of a positive response. In the present study, we aimed to modulate episodic memory formation in 34 older adults with anodal tDCS (1\,mA, 20\,min) over left temporoparietal cortex. Participants were asked to learn novel associations between pictures and pseudowords, and episodic memory performance was subsequently assessed during immediate retrieval. Prior to experimental sessions, participants underwent resting-state functional magnetic resonance imaging. tDCS led to better retrieval performance and augmented learning curves. Hippocampo-temporoparietal functional connectivity was positively related to initial memory performance, and was positively associated with the magnitude of individual tDCS-induced enhancement. In sum, we provide evidence for brain stimulation-induced plasticity of episodic memory processes in older adults, corroborating and extending previous findings. Our results demonstrate that intrinsic network coupling may determine individual responsiveness to brain stimulation, and thus help to further explain variability of tDCS responsiveness in older adults.},
  issue = {1},
  langid = {english}
}

@incollection{ashbyIntroductionFMRI2015,
  title = {An {{Introduction}} to {{fMRI}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Ashby, F. Gregory},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {91--112},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_5},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_5},
  urldate = {2019-03-20},
  abstract = {Functional magnetic resonance imaging (fMRI) provides an opportunity to indirectly observe neural activity noninvasively in the human brain as it changes in near real time. Most fMRI experiments measure the blood oxygen-level dependent (BOLD) signal, which rises to a peak several seconds after a brain area becomes active. Several experimental designs are common in fMRI research. Block designs alternate periods in which subjects perform some task with periods of rest, whereas event-related designs present the subject with a set of discrete trials. After the fMRI experiment is complete, pre-processing analyses prepare the data for task-related analyses. The most popular task-related analysis uses the General Linear Model to correlate a predicted BOLD response with the observed activity in each brain region. Regions where this correlation is high are identified as task related. Connectivity analysis then tries to identify active regions that belong to the same functional network. In contrast, multivariate methods, such as independent component analysis and multi-voxel pattern analysis identify networks of event-related regions, rather than single regions, so they simultaneously address questions of functional connectivity.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {BOLD response,fMRI,Functional connectivity analysis,General Linear Model,Hemodynamic response function,Multiple comparisons problem,Preprocessing}
}

@article{balotaMovingMeanStudies2011,
  title = {Moving {{Beyond}} the {{Mean}} in {{Studies}} of {{Mental Chronometry}}: {{The Power}} of {{Response Time Distributional Analyses}}},
  shorttitle = {Moving {{Beyond}} the {{Mean}} in {{Studies}} of {{Mental Chronometry}}},
  author = {Balota, David A. and Yap, Melvin J.},
  date = {2011-06},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {20},
  number = {3},
  pages = {160--166},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/0963721411408885},
  url = {http://journals.sagepub.com/doi/10.1177/0963721411408885},
  urldate = {2022-04-11},
  abstract = {Although it is widely recognized that response time (RT) distributions are almost always positively skewed and that mathematical psychologists have developed straightforward procedures for capturing characteristics of RT distributions, researchers continue to rely primarily on mean performance, which can be misleading for such data. We review simple procedures for capturing characteristics of underlying RT distributions and show how such procedures have recently been useful to better understand effects from standard cognitive experimental paradigms and individual differences in performance. These well-studied procedures for understanding RT distributions indicate that effects in means can be produced by (a) shifts of RT distributions, (b) stretching of slow tails of RT distributions, or (c) some combination. Importantly, effects in means can actually be obscured by opposing influences on the modal and tail portions of RT distributions. Such disparate patterns demand novel theoretical interpretations.},
  langid = {english}
}

@incollection{bogaczOptimalDecisionMaking2015,
  title = {Optimal {{Decision Making}} in the {{Cortico-Basal-Ganglia Circuit}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Bogacz, Rafal},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {291--302},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_14},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_14},
  urldate = {2019-03-20},
  abstract = {This chapter presents a model assuming that during decision making the cortico-basal-ganglia circuit computes probabilities that considered alternatives are correct, according to Bayes’ theorem. The model suggests how the equation of Bayes’ theorem is mapped onto the functional anatomy of a circuit involving the cortex, basal ganglia and thalamus. The chapter also describes the relationship of the model to other models of decision making and experimental data.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Action selection,Basal ganglia,Decision making}
}

@incollection{borstUsingACTRCognitive2015,
  title = {Using the {{ACT-R Cognitive Architecture}} in {{Combination With fMRI Data}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Borst, Jelmer P. and Anderson, John R.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {339--352},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_17},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_17},
  urldate = {2019-03-20},
  abstract = {In this chapter we discuss how the ACT-R cognitive architecture can be used in combination with fMRI data. ACT-R is a cognitive architecture that can provide a description of the processes from perception through to action for a wide range of cognitive tasks. It has a computational implementation that can be used to create models of specific tasks, which yield exact predictions in the form of response times and accuracy measures. In the last decade, researchers have extended the predictive capabilities of ACT-R to fMRI data. Since ACT-R provides a model of all the components in task performance it can address brain-wide activation patterns. fMRI data can now be used to inform and constrain the architecture, and, on the other hand, the architecture can be used to interpret fMRI data in a principled manner. In the following sections we first introduce cognitive architectures, and ACT-R in particular. Then, on the basis of an example dataset, we explain how ACT-R can be used to create fMRI predictions. In the third and fourth section of this chapter we discuss two ways in which these predictions can be used: region-of-interest and model-based fMRI analysis, and how the results can be used to inform the architecture and to interpret fMRI data.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {ACT-R,Cognitive Architecture,fMRI,Model-based fMRI,ROI analysis}
}

@article{burknerBrmsPackageBayesian2017,
  title = {Brms: {{An R Package}} for {{Bayesian Multilevel Models Using Stan}}},
  shorttitle = {Brms},
  author = {Bürkner, Paul-Christian},
  date = {2017-08-29},
  journaltitle = {Journal of Statistical Software},
  volume = {80},
  number = {1},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v080i01},
  urldate = {2019-01-28},
  langid = {english},
  keywords = {Bayesian inference,MCMC,multilevel model,ordinal data,R,Stan}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
  date = {2013-05},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  url = {http://www.nature.com/articles/nrn3475},
  urldate = {2022-05-10},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  langid = {english}
}

@article{chambersPoliciesKnowledgePriors2019,
  title = {Policies or Knowledge: Priors Differ between a Perceptual and Sensorimotor Task},
  shorttitle = {Policies or Knowledge},
  author = {Chambers, Claire and Fernandes, Hugo and Kording, Konrad Paul},
  date = {2019-06-01},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  volume = {121},
  number = {6},
  pages = {2267--2275},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00035.2018},
  url = {https://www.physiology.org/doi/10.1152/jn.00035.2018},
  urldate = {2022-02-17},
  abstract = {If the brain abstractly represents probability distributions as knowledge, then the modality of a decision, e.g., movement vs. perception, should not matter. If, on the other hand, learned representations are policies, they may be specific to the task where learning takes place. Here, we test this by asking whether a learned spatial prior generalizes from a sensorimotor estimation task to a two-alternative-forced choice (2-Afc) perceptual comparison task. A model and simulation-based analysis revealed that while participants learn prior distribution in the sensorimotor estimation task, measured priors are consistently broader than sensorimotor priors in the 2-Afc task. That the prior does not fully generalize suggests that sensorimotor priors are more like policies than knowledge. In disagreement with standard Bayesian thought, the modality of the decision has a strong influence on the implied prior distributions.             NEW \& NOTEWORTHY We do not know whether the brain represents abstract and generalizable knowledge or task-specific policies that map internal states to actions. We find that learning in a sensorimotor task does not generalize strongly to a perceptual task, suggesting that humans learned policies and did not truly acquire knowledge. Priors differ across tasks, thus casting doubt on the central tenet of many Bayesian models, that the brain’s representation of the world is built on generalizable knowledge.},
  langid = {english}
}

@article{chaterProbabilisticBiasesMeet2020,
  title = {Probabilistic {{Biases Meet}} the {{Bayesian Brain}}},
  author = {Chater, Nick and Zhu, Jian-Qiao and Spicer, Jake and Sundh, Joakim and León-Villagrá, Pablo and Sanborn, Adam},
  date = {2020-10-01},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {29},
  number = {5},
  pages = {506--512},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/0963721420954801},
  url = {https://doi.org/10.1177/0963721420954801},
  urldate = {2021-03-04},
  abstract = {In Bayesian cognitive science, the mind is seen as a spectacular probabilistic-inference machine. But judgment and decision-making (JDM) researchers have spent half a century uncovering how dramatically and systematically people depart from rational norms. In this article, we outline recent research that opens up the possibility of an unexpected reconciliation. The key hypothesis is that the brain neither represents nor calculates with probabilities but approximates probabilistic calculations by drawing samples from memory or mental simulation. Sampling models diverge from perfect probabilistic calculations in ways that capture many classic JDM findings, which offers the hope of an integrated explanation of classic heuristics and biases, including availability, representativeness, and anchoring and adjustment.},
  langid = {english},
  keywords = {Bayesian inference,heuristics and biases,judgment and decision-making,probability,sampling}
}

@article{colomboBayesBrainBayesian2012,
  title = {Bayes in the {{Brain}}—{{On Bayesian Modelling}} in {{Neuroscience}}},
  author = {Colombo, Matteo and Seriès, Peggy},
  date = {2012-09-01},
  journaltitle = {The British Journal for the Philosophy of Science},
  shortjournal = {The British Journal for the Philosophy of Science},
  volume = {63},
  number = {3},
  pages = {697--723},
  issn = {0007-0882, 1464-3537},
  doi = {10.1093/bjps/axr043},
  url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axr043},
  urldate = {2022-04-04},
  abstract = {According to a growing trend in theoretical neuroscience, the human perceptual system is akin to a Bayesian machine. The aim of this article is to clearly articulate the claims that perception can be considered Bayesian inference and that the brain can be considered a Bayesian machine, some of the epistemological challenges to these claims; and some of the implications of these claims. We address two questions: (i) How are Bayesian models used in theoretical neuroscience? (ii) From the use of Bayesian models in theoretical neuroscience, have we learned or can we hope to learn that perception is Bayesian inference or that the brain is a Bayesian machine? From actual practice in theoretical neuroscience, we argue for three claims. First, currently Bayesian models do not provide mechanistic explanations; instead they are useful devices for predicting and systematizing observational statements about people’s performances in a variety of perceptual tasks. That is, currently we should have an instrumentalist attitude towards Bayesian models in neuroscience. Second, the inference typically drawn from Bayesian behavioural performance in a variety of perceptual tasks to underlying Bayesian mechanisms should be understood within the three-level framework laid out by David Marr ([1982]). Third, we can hope to learn that perception is Bayesian inference or that the brain is a Bayesian machine to the extent that Bayesian models will prove successful in yielding secure and informative predictions of both subjects’ perceptual performance and features of the underlying neural mechanisms.},
  langid = {english}
}

@article{debruineUnderstandingMixedEffects2019a,
  title = {Understanding Mixed Effects Models through Data Simulation},
  author = {DeBruine, Lisa and Barr, Dale J.},
  date = {2019-06-01},
  publisher = {{OSF}},
  url = {https://osf.io/3cz2e/},
  urldate = {2021-03-22},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed effects models. However, much of this research is analyzed using ANOVA on aggregated responses because researchers are not confident specifying and interpreting mixed effects models. The tutorial will explain how to simulate data with random effects structure and analyse the data using linear mixed effects regression (with the lme4 R package). The focus will be on interpreting the LMER output in light of the simulated parameters, using this method for power calculations. Data simulation can not only enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs.      Hosted on the Open Science Framework},
  langid = {english}
}

@report{devezerCaseFormalMethodology2020,
  type = {preprint},
  title = {The Case for Formal Methodology in Scientific Reform},
  author = {Devezer, Berna and Navarro, Danielle J. and Vandekerckhove, Joachim and Buzbas, Erkan Ozge},
  date = {2020-04-28},
  institution = {{Scientific Communication and Education}},
  doi = {10.1101/2020.04.26.048306},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.04.26.048306},
  urldate = {2021-03-08},
  abstract = {Abstract           Current attempts at methodological reform in sciences come in response to an overall lack of rigor in methodological and scientific practices in experimental sciences. However, most methodological reform attempts suffer from similar mistakes and over-generalizations to the ones they aim to address. We argue that this can be attributed in part to lack of formalism and first principles. Considering the costs of allowing false claims to become canonized, we argue for formal statistical rigor and scientific nuance in methodological reform. To attain this rigor and nuance, we propose a five-step formal approach for solving methodological problems. To illustrate the use and benefits of such formalism, we present a formal statistical analysis of three popular claims in the metascientific literature: (a) that reproducibility is the cornerstone of science; (b) that data must not be used twice in any analysis; and (c) that exploratory projects imply poor statistical practice. We show how our formal approach can inform and shape debates about such methodological claims.},
  langid = {english}
}

@article{dijkstraImageryAddsStimulusspecific2022,
  title = {Imagery Adds Stimulus-Specific Sensory Evidence to Perceptual Detection},
  author = {Dijkstra, Nadine and Kok, Peter and Fleming, Stephen M.},
  date = {2022-02-17},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {22},
  number = {2},
  pages = {11},
  issn = {1534-7362},
  doi = {10.1167/jov.22.2.11},
  url = {https://doi.org/10.1167/jov.22.2.11},
  urldate = {2022-02-18},
  abstract = {Internally generated imagery and externally triggered perception rely on overlapping sensory processes. This overlap poses a challenge for perceptual reality monitoring: determining whether sensory signals reflect reality or imagination. In this study, we used psychophysics to investigate how imagery and perception interact to determine visual experience. Participants were instructed to detect oriented gratings that gradually appeared in noise while simultaneously either imagining the same grating, a grating perpendicular to the to-be-detected grating, or nothing. We found that, compared to both incongruent imagery and no imagery, congruent imagery caused a leftward shift of the psychometric function relating stimulus contrast to perceptual threshold. We discuss how this effect can best be explained by a model in which imagery adds sensory signal to the perceptual input, thereby increasing the visibility of perceived stimuli. These results suggest that, in contrast to changes in sensory signals caused by self-generated movement, the brain does not discount the influence of self-generated sensory signals on perception.}
}

@incollection{ditterichDistinguishingModelsPerceptual2015,
  title = {Distinguishing {{Between Models}} of {{Perceptual Decision Making}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Ditterich, Jochen},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {277--290},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_13},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_13},
  urldate = {2019-03-20},
  abstract = {Mathematical models are a useful tool for gaining insight into mechanisms of decision making. However, like other scientific methods, its application is not without pitfalls. This chapter demonstrates that it can be difficult to distinguish between alternative models and it illustrates that a model-based approach benefits from the availability of a rich dataset that provides sufficient constraints. Ideally, the dataset is not only comprised of behavioral data, but also contains neural data that provide information about the internal processing. The chapter focuses on two examples taken from perceptual decision making. In one case, information about response time distributions is used to reject a model that is otherwise consistent with accuracy data and mean response times. In the other case, only the availability of neural data allows a distinction between two alternative models that are both consistent with the behavioral data.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Choice,Feedback inhibition,Feedforward inhibition,Parietal cortex,Perceptual decision making,Response time,Stochastic integration,Time-variant}
}

@article{doornBayesFactorsMixed2021a,
  title = {Bayes {{Factors}} for {{Mixed Models}}},
  author = {family=Doorn, given=Johnny, prefix=van, useprefix=false and Aust, Frederik and Haaf, Julia M. and Stefan, Angelika and Wagenmakers, Eric-Jan},
  date = {2021-02-22T12:02:03},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/y65h8},
  url = {https://psyarxiv.com/y65h8/},
  urldate = {2021-02-24},
  abstract = {Although Bayesian mixed models are increasingly popular for data analysis in psychology and other fields, there remains considerable ambiguity on the most appropriate Bayes factor hypothesis test to quantify the degree to which the data support the presence or absence of an experimental effect. Specifically, different choices for both the null model and the alternative model are possible, and each choice constitutes a different definition of an effect resulting in a different test outcome. We outline the common approaches and focus on the impact of aggregation, the effect of measurement error, the choice of prior distribution, and the detection of interactions. For concreteness, three example scenarios showcase how seemingly innocuous choices can lead to dramatic differences in statistical evidence. We hope this work will facilitate a more explicit discussion about best practices in Bayes factor hypothesis testing in mixed models.},
  keywords = {Bayes factors,Mixed effects,Mixed models,Quantitative Methods,Random effects,Social and Behavioral Sciences,Statistical Methods}
}

@article{dutilhQualityResponseTime2019,
  title = {The {{Quality}} of {{Response Time Data Inference}}: {{A Blinded}}, {{Collaborative Assessment}} of the {{Validity}} of {{Cognitive Models}}},
  shorttitle = {The {{Quality}} of {{Response Time Data Inference}}},
  author = {Dutilh, Gilles and Annis, Jeffrey and Brown, Scott D. and Cassey, Peter and Evans, Nathan J. and Grasman, Raoul P. P. P. and Hawkins, Guy E. and Heathcote, Andrew and Holmes, William R. and Krypotos, Angelos-Miltiadis and Kupitz, Colin N. and Leite, Fábio P. and Lerche, Veronika and Lin, Yi-Shin and Logan, Gordon D. and Palmeri, Thomas J. and Starns, Jeffrey J. and Trueblood, Jennifer S. and family=Maanen, given=Leendert, prefix=van, useprefix=true and family=Ravenzwaaij, given=Don, prefix=van, useprefix=true and Vandekerckhove, Joachim and Visser, Ingmar and Voss, Andreas and White, Corey N. and Wiecki, Thomas V. and Rieskamp, Jörg and Donkin, Chris},
  date = {2019-08-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {26},
  number = {4},
  pages = {1051--1069},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1417-2},
  url = {https://doi.org/10.3758/s13423-017-1417-2},
  urldate = {2021-05-10},
  abstract = {Most data analyses rely on models. To complement statistical models, psychologists have developed cognitive models, which translate observed variables into psychologically interesting constructs. Response time models, in particular, assume that response time and accuracy are the observed expression of latent variables including 1) ease of processing, 2) response caution, 3) response bias, and 4) non-decision time. Inferences about these psychological factors, hinge upon the validity of the models’ parameters. Here, we use a blinded, collaborative approach to assess the validity of such model-based inferences. Seventeen teams of researchers analyzed the same 14 data sets. In each of these two-condition data sets, we manipulated properties of participants’ behavior in a two-alternative forced choice task. The contributing teams were blind to the manipulations, and had to infer what aspect of behavior was changed using their method of choice. The contributors chose to employ a variety of models, estimation methods, and inference procedures. Our results show that, although conclusions were similar across different methods, these "modeler’s degrees of freedom" did affect their inferences. Interestingly, many of the simpler approaches yielded as robust and accurate inferences as the more complex methods. We recommend that, in general, cognitive models become a typical analysis tool for response time data. In particular, we argue that the simpler models and procedures are sufficient for standard experimental designs. We finish by outlining situations in which more complicated models and methods may be necessary, and discuss potential pitfalls when interpreting the output from response time models.},
  langid = {english}
}

@article{etzHowBecomeBayesian2016,
  title = {How to Become a {{Bayesian}} in Eight Easy Steps: {{An}} Annotated Reading List},
  shorttitle = {How to Become a {{Bayesian}} in Eight Easy Steps},
  author = {Etz, Alexander and Gronau, Quentin Frederik and Dablander, Fabian and Edelsbrunner, Peter and Baribault, Beth},
  date = {2016-08-15T20:41:08},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/ph6sw},
  url = {https://psyarxiv.com/ph6sw/},
  urldate = {2021-03-01},
  abstract = {In this guide, we present a reading list to serve as a concise introduction to Bayesian data analysis. The introduction is geared toward reviewers, editors, and interested researchers who are new to Bayesian statistics. We provide commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and related sciences.},
  keywords = {Bayes Factor,Bayesian Inference,Bayesian Statistics,Posterior Probability,psyarxiv,Quantitative Methods,Social and Behavioral Sciences,Theory and Philosophy of Science}
}

@article{etzIntroductionBayesianInference2018,
  title = {Introduction to {{Bayesian Inference}} for {{Psychology}}},
  author = {Etz, Alexander and Vandekerckhove, Joachim},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  number = {1},
  pages = {5--34},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1262-3},
  url = {https://doi.org/10.3758/s13423-017-1262-3},
  urldate = {2021-03-01},
  abstract = {We introduce the fundamental tenets of Bayesian inference, which derive from two basic laws of probability theory. We cover the interpretation of probabilities, discrete and continuous versions of Bayes’ rule, parameter estimation, and model comparison. Using seven worked examples, we illustrate these principles and set up some of the technical background for the rest of this special issue of Psychonomic Bulletin \& Review. Supplemental material is available via https://osf.io/wskex/.},
  langid = {english}
}

@article{falconerBalancingMindVestibular2012,
  title = {Balancing the Mind: {{Vestibular}} Induced Facilitation of Egocentric Mental Transformations},
  shorttitle = {Balancing the Mind},
  author = {Falconer, Caroline J. and Mast, Fred W.},
  date = {2012},
  journaltitle = {Experimental Psychology},
  volume = {59},
  number = {6},
  pages = {332--339},
  publisher = {{Hogrefe Publishing}},
  location = {{Germany}},
  issn = {2190-5142(Electronic),1618-3169(Print)},
  doi = {10.1027/1618-3169/a000161},
  abstract = {The body schema is a key component in accomplishing egocentric mental transformations, which rely on bodily reference frames. These reference frames are based on a plurality of different cognitive and sensory cues among which the vestibular system plays a prominent role. We investigated whether a bottom-up influence of vestibular stimulation modulates the ability to perform egocentric mental transformations. Participants were significantly faster to make correct spatial judgments during vestibular stimulation as compared to sham stimulation. Interestingly, no such effects were found for mental transformation of hand stimuli or during mental transformations of letters, thus showing a selective influence of vestibular stimulation on the rotation of whole-body reference frames. Furthermore, we found an interaction with the angle of rotation and vestibular stimulation demonstrating an increase in facilitation during mental body rotations in a direction congruent with rightward vestibular afferents. We propose that facilitation reflects a convergence in shared brain areas that process bottom-up vestibular signals and top-down imagined whole-body rotations, including the precuneus and tempero-parietal junction. Ultimately, our results show that vestibular information can influence higher-order cognitive processes, such as the body schema and mental imagery. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Egocentrism,Mental Rotation,Schema,Somesthetic Stimulation,Spatial Ability}
}

@article{fardBayesianReformulationExtended2017,
  title = {A {{Bayesian Reformulation}} of the {{Extended Drift-Diffusion Model}} in {{Perceptual Decision Making}}},
  author = {Fard, Pouyan R. and Park, Hame and Warkentin, Andrej and Kiebel, Stefan J. and Bitzer, Sebastian},
  date = {2017},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {11},
  publisher = {{Frontiers}},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00029},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00029/full},
  urldate = {2021-03-31},
  abstract = {Perceptual decision making can be described as a process of accumulating evidence to a bound which has been formalized within drift-diffusion models. Recently, an equivalent Bayesian model has been proposed. In contrast to standard drift-diffusion models, this Bayesian model directly links information in the stimulus to the decision process. Here, we extend this Bayesian model further and allow inter-trial variability of two parameters following the extended version of the drift-diffusion model. We derive parameter distributions for the Bayesian model and show that they lead to predictions that are qualitatively equivalent to those made by the extended drift-diffusion model. Further, we demonstrate the usefulness of the extended Bayesian model for the analysis of concrete behavioral data. Specifically, using Bayesian model selection, we find evidence that including additional inter-trial parameter variability provides for a better model, when the model is constrained by trial-wise stimulus features. This result is remarkable because it was derived using just 200 trials per condition, which is typically thought to be insufficient for identifying variability parameters in drift-diffusion models. In sum, we present a Bayesian analysis, which provides for a novel and promising analysis of perceptual decision making experiments.},
  langid = {english},
  keywords = {Bayesian Models,drift-diffusion model,exact input modeling,Model Comparison,parameter fitting,perceptual decision making,single-trial models}
}

@incollection{farrellIntroductionCognitiveModeling2015,
  title = {An {{Introduction}} to {{Cognitive Modeling}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Farrell, Simon and Lewandowsky, Stephan},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {3--24},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_1},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_1},
  urldate = {2019-03-20},
  abstract = {We provide a tutorial on the basic attributes of computational cognitive models—models that are formulated as a set of mathematical equations or as a computer simulation. We first show how models can generate complex behavior and novel insights from very simple underlying assumptions about human cognition. We survey the different classes of models, from description to explanation, and present examples of each class. We then illustrate the reasons why computational models are preferable to purely verbal means of theorizing. For example, we show that computational models help theoreticians overcome the limitations of human cognition, thereby enabling us to create coherent and plausible accounts of how we think or remember and guard against subtle theoretical errors. Models can also measure latent constructs and link them to individual differences, which would escape detection if only the raw data were considered. We conclude by reviewing some open challenges.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Agent-based modelling,Computational models,Model comparison,Necessity,Parameter interpretation,Practice,Scientific reasoning}
}

@article{faulkenberryBayesianInferenceNumerical2020,
  title = {Bayesian {{Inference}} in {{Numerical Cognition}}: {{A Tutorial Using JASP}}},
  shorttitle = {Bayesian {{Inference}} in {{Numerical Cognition}}},
  author = {Faulkenberry, Thomas J. and Ly, Alexander and Wagenmakers, Eric-Jan},
  date = {2020-09-09},
  journaltitle = {Journal of Numerical Cognition},
  volume = {6},
  number = {2},
  pages = {231--259},
  issn = {2363-8761},
  doi = {10.5964/jnc.v6i2.288},
  url = {https://jnc.psychopen.eu/index.php/jnc/article/view/5903},
  urldate = {2022-05-10},
  langid = {american},
  keywords = {Bayes factors,Bayesian inference,JASP,numerical cognition,tutorial}
}

@article{feldmanNewSpinSpatial2020,
  title = {A {{New Spin}} on {{Spatial Cognition}} in {{ADHD}}: {{A Diffusion Model Decomposition}} of {{Mental Rotation}}},
  shorttitle = {A {{New Spin}} on {{Spatial Cognition}} in {{ADHD}}},
  author = {Feldman, Jason S. and Huang-Pollock, Cynthia},
  date = {2020},
  journaltitle = {Journal of the International Neuropsychological Society},
  pages = {1--12},
  publisher = {{Cambridge University Press}},
  issn = {1355-6177, 1469-7661},
  doi = {10.1017/S1355617720001198},
  url = {https://www.cambridge.org/core/journals/journal-of-the-international-neuropsychological-society/article/abs/new-spin-on-spatial-cognition-in-adhd-a-diffusion-model-decomposition-of-mental-rotation/DB35A44AF99DF3D2AB08EBA6E655B650},
  urldate = {2021-05-17},
  abstract = {Objectives: Multiple studies have found evidence of task non-specific slow drift rate in ADHD, and slow drift rate has rapidly become one of the most visible cognitive hallmarks of the disorder. In this study, we use the diffusion model to determine whether atypicalities in visuospatial cognitive processing exist independently of slow drift rate. Methods: Eight- to twelve-year-old children with (n = 207) and without ADHD (n = 99) completed a 144-trial mental rotation task. Results: Performance of children with ADHD was less accurate and more variable than non-ADHD controls, but there were no group differences in mean response time. Drift rate was slower, but nondecision time was faster for children with ADHD. A Rotation × ADHD interaction for boundary separation was also found in which children with ADHD did not strategically adjust their response thresholds to the same degree as non-ADHD controls. However, the Rotation × ADHD interaction was not significant for nondecision time, which would have been the primary indicator of a specific deficit in mental rotation per se. Conclusions: Poorer performance on the mental rotation task was due to slow rate of evidence accumulation, as well as relative inflexibility in adjusting boundary separation, but not to impaired visuospatial processing specifically. We discuss the implications of these findings for future cognitive research in ADHD.},
  langid = {english},
  keywords = {ADHD,Boundary separation,Children,Drift rate,Neuropsychology,Visuospatial reasoning}
}

@article{fenglerLikelihoodApproximationNetworks2020,
  title = {Likelihood {{Approximation Networks}} ({{LANs}}) for {{Fast Inference}} of {{Simulation Models}} in {{Cognitive Neuroscience}}},
  author = {Fengler, Alexander and Govindarajan, Lakshmi N. and Chen, Tony and Frank, Michael J.},
  date = {2020-12-02},
  journaltitle = {bioRxiv},
  pages = {2020.11.20.392274},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.11.20.392274},
  url = {https://www.biorxiv.org/content/10.1101/2020.11.20.392274v2},
  urldate = {2021-03-18},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}In cognitive neuroscience, computational modeling can formally adjudicate between theories and affords quantitative fits to behavioral/brain data. Pragmatically, however, the space of plausible generative models considered is dramatically limited by the set of models with known likelihood functions. For many models, the lack of a closed-form likelihood typically impedes Bayesian inference methods. As a result, standard models are evaluated for convenience, even when other models might be superior. Likelihood-free methods exist but are limited by their computational cost or their restriction to particular inference scenarios. Here, we propose neural networks that learn approximate likelihoods for arbitrary generative models, allowing fast posterior sampling with only a one-off cost for model simulations that is amortized for future inference. We show that these methods can accurately recover posterior parameter distributions for a variety of neurocognitive process models. We provide code allowing users to deploy these methods for arbitrary hierarchical model instantiations without further training.{$<$}/p{$>$}},
  langid = {english}
}

@article{feuerriegelPredictiveActivationSensory2021,
  title = {Predictive Activation of Sensory Representations as a Source of Evidence in Perceptual Decision-Making},
  author = {Feuerriegel, Daniel and Blom, Tessel and Hogendoorn, Hinze},
  date = {2021-03-01},
  journaltitle = {Cortex},
  shortjournal = {Cortex},
  volume = {136},
  pages = {140--146},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2020.12.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0010945220304494},
  urldate = {2022-02-23},
  abstract = {Our brains can represent expected future states of our sensory environment. Recent work has shown that, when we expect a specific stimulus to appear at a specific time, we can predictively generate neural representations of that stimulus even before it is physically presented. These observations raise two exciting questions: Are pre-activated sensory representations used for perceptual decision-making? And, do we transiently perceive an expected stimulus that does not actually appear? To address these questions, we propose that pre-activated neural representations provide sensory evidence that is used for perceptual decision-making. This can be understood within the framework of the Diffusion Decision Model as an early accumulation of decision evidence in favour of the expected percept. Our proposal makes novel predictions relating to expectation effects on neural markers of decision evidence accumulation, and also provides an explanation for why we sometimes perceive stimuli that are expected, but do not appear.},
  langid = {english},
  keywords = {Decision-making,Expectation,MVPA,Perception,Prediction}
}

@online{FirstLessonBayesian,
  title = {A {{First Lesson}} in {{Bayesian Inference}}},
  url = {http://lmpp10e-mucesm.srv.mwn.de:3838/felix/BayesLessons/BayesianLesson1.Rmd},
  urldate = {2021-03-01}
}

@incollection{forstmannIntroductionHumanBrain2015,
  title = {An {{Introduction}} to {{Human Brain Anatomy}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Forstmann, Birte U. and Keuken, Max C. and Alkemade, Anneke},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {71--89},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_4},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_4},
  urldate = {2019-03-20},
  abstract = {This tutorial chapter provides an overview of the human brain anatomy. Knowledge of brain anatomy is fundamental to our understanding of cognitive processes in health and disease; moreover, anatomical constraints are vital for neurocomputational models and can be important for psychological theorizing as well. The main challenge in understanding brain anatomy is to integrate the different levels of description ranging from molecules to macroscopic brain networks. This chapter contains three main sections. The first section provides a brief introduction to the neuroanatomical nomenclature. The second section provides an introduction to the different levels of brain anatomy and describes commonly used atlases for the visualization of functional imaging data. The third section provides a concrete example of how human brain structure relates to performance.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Connectional neuroanatomy,functional MRI,Neuroanatomical atlases,Sectional neuroanatomy,Structural MRI,Structure-function relationships,Ultra high resolution MRI}
}

@incollection{forstmannModelBasedCognitiveNeuroscience2015,
  title = {Model-{{Based Cognitive Neuroscience}}: {{A Conceptual Introduction}}},
  shorttitle = {Model-{{Based Cognitive Neuroscience}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {139--156},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_7},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_7},
  urldate = {2019-03-20},
  abstract = {This tutorial chapter shows how the separate fields of mathematical psychology and cognitive neuroscience can interact to their mutual benefit. Historically, the field of mathematical psychology is mostly concerned with formal theories of behavior, whereas cognitive neuroscience is mostly concerned with empirical measurements of brain activity. Despite these superficial differences in method, the ultimate goal of both disciplines is the same: to understand the workings of human cognition. In recognition of this common purpose, mathematical psychologists have recently started to apply their models in cognitive neuroscience, and cognitive neuroscientists have borrowed and extended key ideas that originated from mathematical psychology. This chapter consists of three main sections: the first describes the field of mathematical psychology, the second describes the field of cognitive neuroscience, and the third describes their recent combination: model-based cognitive neuroscience.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Blood Oxygenation Level Dependent,Blood Oxygenation Level Dependent Signal,Cognitive Neuroscience,Drift Rate,Mathematical Psychology}
}

@article{forstmannSequentialSamplingModels2016,
  title = {Sequential {{Sampling Models}} in {{Cognitive Neuroscience}}: {{Advantages}}, {{Applications}}, and {{Extensions}}},
  shorttitle = {Sequential {{Sampling Models}} in {{Cognitive Neuroscience}}},
  author = {Forstmann, B.U. and Ratcliff, R. and Wagenmakers, E.-J.},
  date = {2016},
  journaltitle = {Annual review of psychology},
  shortjournal = {Annu Rev Psychol},
  volume = {67},
  eprint = {26393872},
  eprinttype = {pmid},
  pages = {641--666},
  issn = {0066-4308},
  doi = {10.1146/annurev-psych-122414-033645},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5112760/},
  urldate = {2022-04-04},
  abstract = {Sequential sampling models assume that people make speeded decisions by gradually accumulating noisy information until a threshold of evidence is reached. In cognitive science, one such model—the diffusion decision model—is now regularly used to decompose task performance into underlying processes such as the quality of information processing, response caution, and a priori bias. In the cognitive neurosciences, the diffusion decision model has recently been adopted as a quantitative tool to study the neural basis of decision making under time pressure. We present a selective overview of several recent applications and extensions of the diffusion decision model in the cognitive neurosciences.},
  pmcid = {PMC5112760}
}

@article{forstmannSequentialSamplingModels2016a,
  title = {Sequential {{Sampling Models}} in {{Cognitive Neuroscience}}: {{Advantages}}, {{Applications}}, and {{Extensions}}},
  shorttitle = {Sequential {{Sampling Models}} in {{Cognitive Neuroscience}}},
  author = {Forstmann, B.U. and Ratcliff, R. and Wagenmakers, E.-J.},
  date = {2016-01-04},
  journaltitle = {Annual Review of Psychology},
  volume = {67},
  number = {1},
  pages = {641--666},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122414-033645},
  url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-122414-033645},
  urldate = {2020-04-29},
  abstract = {Sequential sampling models assume that people make speeded decisions by gradually accumulating noisy information until a threshold of evidence is reached. In cognitive science, one such model—the diffusion decision model—is now regularly used to decompose task performance into underlying processes such as the quality of information processing, response caution, and a priori bias. In the cognitive neurosciences, the diffusion decision model has recently been adopted as a quantitative tool to study the neural basis of decision making under time pressure. We present a selective overview of several recent applications and extensions of the diffusion decision model in the cognitive neurosciences.},
  langid = {english}
}

@article{frankeBayesianRegressionModeling2019,
  title = {Bayesian Regression Modeling (for Factorial Designs): {{A}} Tutorial},
  shorttitle = {Bayesian Regression Modeling (for Factorial Designs)},
  author = {Franke, Michael and Roettger, Timo B.},
  date = {2019-07-13T18:36:33},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdxv3},
  url = {https://psyarxiv.com/cdxv3/},
  urldate = {2022-02-14},
  abstract = {Generalized linear mixed models are handy tools for statistical inference, and Bayesian approaches to applying these become increasingly popular. This tutorial provides an accessible, non-technical introduction to the use and feel of Bayesian mixed effects regression models. The focus is on data from a factorial-design experiment.},
  langid = {american},
  keywords = {bayesian,factorial design,multilevel regression,parameter estimation,R,Social and Behavioral Sciences}
}

@article{frankeBayesianRegressionModeling2019a,
  title = {Bayesian Regression Modeling (for Factorial Designs): {{A}} Tutorial},
  shorttitle = {Bayesian Regression Modeling (for Factorial Designs)},
  author = {Franke, Michael and Roettger, Timo B.},
  date = {2019-07-13T18:36:33},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdxv3},
  url = {https://psyarxiv.com/cdxv3/},
  urldate = {2022-02-23},
  abstract = {Generalized linear mixed models are handy tools for statistical inference, and Bayesian approaches to applying these become increasingly popular. This tutorial provides an accessible, non-technical introduction to the use and feel of Bayesian mixed effects regression models. The focus is on data from a factorial-design experiment.},
  langid = {american},
  keywords = {bayesian,factorial design,multilevel regression,parameter estimation,R,Social and Behavioral Sciences}
}

@incollection{frankLinkingLevelsComputation2015,
  title = {Linking {{Across Levels}} of {{Computation}} in {{Model-Based Cognitive Neuroscience}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Frank, Michael J.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {159--177},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_8},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_8},
  urldate = {2019-03-20},
  abstract = {Computational approaches to cognitive neuroscience encompass multiple levels of analysis, from detailed biophysical models of neural activity to abstract algorithmic or normative models of cognition, with several levels in between. Despite often strong opinions on the ‘right’ level of modeling, there is no single panacea: attempts to link biological with higher level cognitive processes require a multitude of approaches. Here I argue that these disparate approaches should not be viewed as competitive, nor should they be accessible to only other researchers already endorsing the particular level of modeling. Rather, insights gained from one level of modeling should inform modeling endeavors at the level above and below it. One way to achieve this synergism is to link levels of modeling by quantitatively fitting the behavioral outputs of detailed mechanistic models with higher level descriptions. If the fits are reasonable (e.g., similar to those achieved when applying high level models to human behavior), one can then derive plausible links between mechanism and computation. Model-based cognitive neuroscience approaches can then be employed to manipulate or measure neural function motivated by the candidate mechanisms, and to test whether these are related to high level model parameters. I describe several examples of this approach in the domain of reward-based learning, cognitive control, and decision making and show how neural and algorithmic models have each informed or refined the other.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Algorithms,Basal ganglia,Computational models,Decision making,Dopamine,Neural networks,Prefrontal cortex,Reinforcement learning}
}

@article{ganisNewSetThreeDimensional2015,
  title = {A {{New Set}} of {{Three-Dimensional Shapes}} for {{Investigating Mental Rotation Processes}}: {{Validation Data}} and {{Stimulus Set}}},
  shorttitle = {A {{New Set}} of {{Three-Dimensional Shapes}} for {{Investigating Mental Rotation Processes}}},
  author = {Ganis, Giorgio and Kievit, Rogier},
  date = {2015-03-13},
  journaltitle = {Journal of Open Psychology Data},
  volume = {3},
  number = {1},
  pages = {e3},
  publisher = {{Ubiquity Press}},
  issn = {2050-9863},
  doi = {10.5334/jopd.ai},
  url = {http://openpsychologydata.metajnl.com/articles/10.5334/jopd.ai/},
  urldate = {2021-03-16},
  abstract = {Mental rotation is one of the most influential paradigms in the history of cognitive psychology. In this paper, we present a new set of validated mental rotation stimuli to be used freely by the scientific community. Three-dimensional visual rendering software was employed to generate a total of 384 realistic-looking mental rotation stimuli with shading and foreshortening depth cues. Each stimulus was composed of two pictures: a baseline object and a target object, placed side by side, which can be aligned by means of a rotation around the vertical axis in half of the stimuli but not in the other half. Behavioral data (N=54, freely available) based on these stimuli exhibited the typical linear increase in response times and error rates with angular disparity, validating the stimulus set. This set of stimuli is especially useful for studies where it is necessary to avoid stimulus repetition, such as training studies.},
  issue = {1},
  langid = {english},
  keywords = {Mental rotation,visual spatial skills; generalization}
}

@book{gelmanBayesianDataAnalysis2014,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew},
  date = {2014},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science},
  edition = {Third edition},
  publisher = {{CRC Press}},
  location = {{Boca Raton}},
  abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
  isbn = {978-1-4398-4095-5},
  pagetotal = {661},
  keywords = {Bayesian statistical decision theory,MATHEMATICS / Probability & Statistics / General}
}

@article{gelmanRsquaredBayesianRegression2019,
  title = {R-Squared for {{Bayesian Regression Models}}},
  author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari, Aki},
  date = {2019-07-03},
  journaltitle = {The American Statistician},
  volume = {73},
  number = {3},
  pages = {307--309},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1549100},
  url = {https://doi.org/10.1080/00031305.2018.1549100},
  urldate = {2021-05-28},
  abstract = {The usual definition of R2 (variance of the predicted values divided by the variance of the data) has a problem for Bayesian fits, as the numerator can be larger than the denominator. We propose an alternative definition similar to one that has appeared in the survival analysis literature: the variance of the predicted values divided by the variance of predicted values plus the expected variance of the errors.},
  keywords = {Bayesian methods,R-squared,Regression},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1549100}
}

@article{gigerenzerMindlessStatistics2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  date = {2004-11},
  journaltitle = {The Journal of Socio-Economics},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {10535357},
  doi = {10.1016/j.socec.2004.09.033},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053535704000927},
  urldate = {2019-02-11},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english}
}

@article{gigerenzerStatisticalRitualsReplication2018a,
  title = {Statistical {{Rituals}}: {{The Replication Delusion}} and {{How We Got There}}},
  shorttitle = {Statistical {{Rituals}}},
  author = {Gigerenzer, Gerd},
  date = {2018-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {198--218},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918771329},
  url = {https://doi.org/10.1177/2515245918771329},
  urldate = {2021-03-01},
  abstract = {The “replication crisis” has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers’ widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The “null ritual,” unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 – p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20\% of the faculty teaching statistics in psychology, 39\% of the professors and lecturers, and 66\% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 – p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56\%–97\%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as “significant” or “not significant.”},
  langid = {english},
  keywords = {illusion of certainty,null ritual,p value,p-hacking,replication}
}

@article{grabherrMentalTransformationAbilities2011,
  title = {Mental Transformation Abilities in Patients with Unilateral and Bilateral Vestibular Loss},
  author = {Grabherr, Luzia and Cuffel, Cyril and Guyot, Jean-Philippe and Mast, Fred W.},
  date = {2011-03},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {209},
  number = {2},
  eprint = {21287158},
  eprinttype = {pmid},
  pages = {205--214},
  issn = {1432-1106},
  doi = {10.1007/s00221-011-2535-0},
  abstract = {Vestibular information helps to establish a reliable gravitational frame of reference and contributes to the adequate perception of the location of one's own body in space. This information is likely to be required in spatial cognitive tasks. Indeed, previous studies suggest that the processing of vestibular information is involved in mental transformation tasks in healthy participants. In this study, we investigate whether patients with bilateral or unilateral vestibular loss show impaired ability to mentally transform images of bodies and body parts compared to a healthy, age-matched control group. An egocentric and an object-based mental transformation task were used. Moreover, spatial perception was assessed using a computerized version of the subjective visual vertical and the rod and frame test. Participants with bilateral vestibular loss showed impaired performance in mental transformation, especially in egocentric mental transformation, compared to participants with unilateral vestibular lesions and the control group. Performance of participants with unilateral vestibular lesions and the control group are comparable, and no differences were found between right- and left-sided labyrinthectomized patients. A control task showed no differences between the three groups. The findings from this study substantiate that central vestibular processes are involved in imagined spatial body transformations; but interestingly, only participants with bilateral vestibular loss are affected, whereas unilateral vestibular loss does not lead to a decline in spatial imagery.},
  langid = {english},
  keywords = {Adult,Aged,Analysis of Variance,Female,Humans,Imagination,Male,Middle Aged,Orientation,Psychomotor Performance,Reaction Time,Space Perception,Surveys and Questionnaires,Vestibular Diseases}
}

@article{gronauTutorialBridgeSampling2017a,
  title = {A Tutorial on Bridge Sampling},
  author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
  date = {2017-12-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {81},
  pages = {80--97},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2017.09.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249617300640},
  urldate = {2021-05-03},
  abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng \& Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model—a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
  langid = {english},
  keywords = {Bayes factor,Hierarchical model,Marginal likelihood,Normalizing constant,Predictive accuracy,Reinforcement learning}
}

@article{guestHowComputationalModeling2021,
  title = {How {{Computational Modeling Can Force Theory Building}} in {{Psychological Science}}},
  author = {Guest, Olivia and Martin, Andrea E.},
  date = {2021-01-22},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  pages = {1745691620970585},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620970585},
  url = {https://doi.org/10.1177/1745691620970585},
  urldate = {2021-02-22},
  abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined—what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
  langid = {english},
  keywords = {computational model,open science,scientific inference,theoretical psychology}
}

@article{guestHowComputationalModeling2021a,
  title = {How {{Computational Modeling Can Force Theory Building}} in {{Psychological Science}}},
  author = {Guest, Olivia and Martin, Andrea E.},
  date = {2021-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {16},
  number = {4},
  pages = {789--802},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620970585},
  url = {https://doi.org/10.1177/1745691620970585},
  urldate = {2022-02-14},
  abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined—what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
  langid = {english},
  keywords = {computational model,open science,scientific inference,theoretical psychology}
}

@article{hainesLearningReliabilityParadox2020a,
  title = {Learning from the {{Reliability Paradox}}: {{How Theoretically Informed Generative Models Can Advance}} the {{Social}}, {{Behavioral}}, and {{Brain Sciences}}},
  shorttitle = {Learning from the {{Reliability Paradox}}},
  author = {Haines, Nathaniel and Kvam, Peter D. and Irving, Louis H. and Smith, Colin and Beauchaine, Theodore P. and Pitt, Mark A. and Ahn, Woo-Young and Turner, Brandon},
  date = {2020-08-24T13:56:49},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/xr7y3},
  url = {https://psyarxiv.com/xr7y3/},
  urldate = {2021-03-08},
  abstract = {Behavioral tasks (e.g., Stroop task) that produce replicable group-level effects (e.g., Stroop effect) often fail to reliably capture individual differences between participants (e.g., low test-retest reliability). This “reliability paradox” has led many researchers to conclude that most behavioral tasks cannot be used to develop and advance theories of individual differences. However, these conclusions are derived from statistical models that provide only superficial summary descriptions of behavioral data, thereby ignoring theoretically-relevant data-generating mechanisms that underly individual-level behavior. More generally, such descriptive methods lack the flexibility to test and develop increasingly complex theories of individual differences. To resolve this theory-description gap, we present generative modeling approaches, which involve using background knowledge to specify how behavior is generated at the individual level, and in turn how the distributions of individual-level mechanisms are characterized at the group level—all in a single joint model. Generative modeling shifts our focus away from estimating descriptive statistical “effects” toward estimating psychologically meaningful parameters, while simultaneously accounting for measurement error that would otherwise attenuate individual difference correlations. Using simulations and empirical data from the Implicit Association Test and Stroop, Flanker, Posner Cueing, and Delay Discounting tasks, we demonstrate how generative models yield (1) higher test-retest reliability estimates, and (2) more theoretically informative parameter estimates relative to traditional statistical approaches. Our results reclaim optimism regarding the utility of behavioral paradigms for testing and advancing theories of individual differences, and emphasize the importance of formally specifying and checking model assumptions to reduce theory-description gaps and facilitate principled theory development.},
  keywords = {Bayesian analysis,Clinical Psychology,Cognitive Psychology,Generative modeling,Implicit attitudes,Impulsivity,Individual differences,Measurement error,Meta-science,Quantitative Methods,Reliability,Self-control,Social and Behavioral Sciences,Social and Personality Psychology,Theory and Philosophy of Science,Theory development}
}

@incollection{heathcoteIntroductionGoodPractices2015,
  title = {An {{Introduction}} to {{Good Practices}} in {{Cognitive Modeling}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Heathcote, Andrew and Brown, Scott D. and Wagenmakers, Eric-Jan},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {25--48},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_2},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_2},
  urldate = {2019-03-20},
  abstract = {Cognitive modeling can provide important insights into the underlying causes of behavior, but the validity of those insights rests on careful model development and checking. We provide guidelines on five important aspects of the practice of cognitive modeling: parameter recovery, testing selective influence of experimental manipulations on model parameters, quantifying uncertainty in parameter estimates, testing and displaying model fit, and selecting among different model parameterizations and types of models. Each aspect is illustrated with examples.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Cognition,Model,Model selection,Parameter estimation,Quantitative,Simulation study,Theory}
}

@article{heathcotePowerLawRepealed2000,
  title = {The Power Law Repealed: {{The}} Case for an Exponential Law of Practice},
  shorttitle = {The Power Law Repealed},
  author = {Heathcote, Andrew and Brown, Scott and Mewhort, D. J. K.},
  date = {2000-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {7},
  number = {2},
  pages = {185--207},
  issn = {1531-5320},
  doi = {10.3758/BF03212979},
  url = {https://doi.org/10.3758/BF03212979},
  urldate = {2022-05-11},
  abstract = {The power function is treated as the law relating response time to practice trials. However, the evidence for a power law is flawed, because it is based on averaged data. We report a survey that assessed the form of the practice function for individual learners and learning conditions in paradigms that have shaped theories of skill acquisition. We fit power and exponential functions to 40 sets of data representing 7,910 learning series from 475 subjects in 24 experiments. The exponential function fit better than the power function in all the unaveraged data sets. Averaging produced a bias in favor of the power function. A new practice function based on the exponential, the APEX function, fit better than a power function with an extra, preexperimental practice parameter. Clearly, the best candidate for the law of practice is the exponential or APEX function, not the generally accepted power function. The theoretical implications are discussed.},
  langid = {english},
  keywords = {Exponential Function,Journal ofExperimental Psychology,Learning Rate,Mental Rotation,Power Function}
}

@article{heitAreThereTwo2005,
  title = {Are {{There Two Kinds}} of {{Reasoning}}},
  author = {Heit, E. and Rotello, C.},
  date = {2005},
  journaltitle = {undefined},
  url = {https://www.semanticscholar.org/paper/Are-There-Two-Kinds-of-Reasoning-Heit-Rotello/44e5bc66e2d1e137eecfa64a3fc23f8e02d141f0},
  urldate = {2022-05-10},
  abstract = {Are There Two Kinds of Reasoning? Evan Heit (E.Heit@warwick.ac.uk) Department of Psychology, University of Warwick Coventry CV4 7AL, UK Caren M. Rotello (caren@psych.umass.edu) Department of Psychology, University of Massachusetts Amherst MA 01003-7710, USA people use a common set of reasoning processes for both deductive and inductive arguments. For example, Chater and Oaksford (2000) have applied an account of probabilistic reasoning, explicitly non-deductive in nature, to a range of deductive problems. Likewise, Harman (1999) has argued that people reason in an essentially non- deductive way, and bring these same reasoning processes to bear on both inductive and deductive reasoning problems. Taking a related approach, Johnson-Laird (1994) has extended the mental models account, more frequently applied to deductive problems, to a range of inductive problems as well. Finally, some researchers have proposed accounts that focus mainly on reasoning about inductive arguments, and have treated deductively correct arguments as special cases that would be covered by the same accounts (Heit, 2000; Osherson, Smith, Wilkie, Lopez, \&amp; Shafir, 1990; Sloman, 1993). In contrast, other researchers have emphasized a distinction between two kinds of reasoning (e.g., Evans \&amp; Over, 1996; Sloman, 1996; Stanovich, 1999). In these two- process accounts there is one system that is relatively fast but heavily influenced by context and associations, and another system that is more deliberative and analytic or rule-based. Although these two systems do not necessarily correspond directly to induction and deduction, it is plausible that induction would depend more on the first system whereas deduction would depend more on the second system. In addition there is some neuropsychological evidence, based on brain imaging, for two anatomically separate systems of reasoning (Goel, Gold, Kapur, \&amp; Houle, 1997; Parsons \&amp; Osherson, 2001). These one- and two-process proposals are mainly aimed at accounting for a range of phenomena rather than drawing a sharp line between deduction and induction. In contrast, the proposal by Rips (2001) does not aim for a detailed description of reasoning processes but instead focuses on a key commonality and a key difference between deduction and induction. In his account, there is a single scale for evaluating arguments. This account will be referred to as the criterion-shift account, and it is illustrated in Figure 1. Here, the unitary scale of argument strength is shown, with different points on the scale corresponding to arguments of different strengths. Criterion 1 indicates the dividing line between arguments that are inductively weak, or implausible, and arguments that are inductively strong, or Abstract Two experiments addressed the issue of how deductive reasoning and inductive reasoning are related. According to the criterion-shift account, these two kinds of reasoning assess arguments along a common scale of strength, however there is a stricter criterion for saying an argument is deductively correct as opposed to just inductively strong. The method, adapted from Rips (2001), was to give two groups of participants the same set of written arguments but with either deduction or induction instructions. Signal detection and receiver operating characteristic analyses showed that the difference between conditions could not be explained in terms of a criterion shift. Instead, the deduction condition showed greater sensitivity to argument strength than did the induction condition. Implications for two-process and one-process accounts of reasoning, and relations to memory research, are discussed. Keywords: reasoning; deduction; detection theory; memory; modeling. induction; signal Introduction How do convincing arguments differ from non-convincing arguments? Rips (2001) has referred to the intuitive case for a single psychological dimension of argument strength, in which arguments can range from utterly worthless to completely compelling. Hence, the convincingness of an argument could be judged by assessing its position on the scale, in a similar manner to how judgments of loudness or brightness would use a psychophysical scale. This intuition of a unitary scale needs to be reconciled with the notion that there are different kinds of reasoning. In particular there is the textbook distinction between deduction and induction, with deduction being concerned with drawing logically valid conclusions as opposed to induction which involves drawing plausible inferences. Strictly speaking, there are different kinds of arguments, such as deductively correct arguments, with respect to a well-defined logic, and inductively strong arguments (Skyrms, 2000). It is still an open question whether there are different kinds of reasoning, such as deductive reasoning and inductive reasoning. Some researchers have suggested that rather than having specialized cognitive processes for each kind of reasoning,},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{herveyReactionTimeDistribution2006,
  title = {Reaction {{Time Distribution Analysis}} of {{Neuropsychological Performance}} in an {{ADHD Sample}}},
  author = {Hervey, Aaron S. and Epstein, Jeffery N. and Curry, John F. and Tonev, Simon and Eugene Arnold, L. and Keith Conners, C. and Hinshaw, Stephen P. and Swanson, James M. and Hechtman, Lily},
  date = {2006-05},
  journaltitle = {Child Neuropsychology},
  shortjournal = {Child Neuropsychology},
  volume = {12},
  number = {2},
  pages = {125--140},
  issn = {0929-7049, 1744-4136},
  doi = {10.1080/09297040500499081},
  url = {http://www.tandfonline.com/doi/abs/10.1080/09297040500499081},
  urldate = {2022-04-05},
  abstract = {Differences in reaction time (RT) variability have been documented between children with and without Attention Deficit Hyperactivity Disorder (ADHD). Most previous research has utilized estimates of normal distributions to examine variability. Using a nontraditional approach, the present study evaluated RT distributions on the Conners’ Continuous Performance Test in children and adolescents from the Multimodal Treatment Study of ADHD sample compared to a matched sample of normal controls (n = 65 pairs). The ex-Gaussian curve was used to model RT and RT variability. Children with ADHD demonstrated faster RT associated with the normal portion of the curve and a greater proportion of abnormally slow responses associated with the exponential portion of the curve. These results contradict previous interpretation that children with ADHD have slower than normal responding and demonstrate why slower RT is found when estimates of variability assume normal Gaussian distributions. Further, results of this study suggest that the greater number of abnormally long RTs of children with ADHD reflect attentional lapses on some but not all trials.},
  langid = {english}
}

@article{hoekstraRobustMisinterpretationConfidence2014,
  title = {Robust Misinterpretation of Confidence Intervals},
  author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  date = {2014-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {21},
  number = {5},
  pages = {1157--1164},
  issn = {1531-5320},
  doi = {10.3758/s13423-013-0572-3},
  url = {https://doi.org/10.3758/s13423-013-0572-3},
  urldate = {2021-03-01},
  abstract = {Null hypothesis significance testing (NHST) is undoubtedly the most common inferential technique used to justify claims in the social sciences. However, even staunch defenders of NHST agree that its outcomes are often misinterpreted. Confidence intervals (CIs) have frequently been proposed as a more useful alternative to NHST, and their use is strongly encouraged in the APA Manual. Nevertheless, little is known about how researchers interpret CIs. In this study, 120 researchers and 442 students—all in the field of psychology—were asked to assess the truth value of six particular statements involving different interpretations of a CI. Although all six statements were false, both researchers and students endorsed, on average, more than three statements, indicating a gross misunderstanding of CIs. Self-declared experience with statistics was not related to researchers’ performance, and, even more surprisingly, researchers hardly outperformed the students, even though the students had not received any education on statistical inference whatsoever. Our findings suggest that many researchers do not know the correct interpretation of a CI. The misunderstandings surrounding p-values and CIs are particularly unfortunate because they constitute the main tools by which psychologists draw conclusions from data.},
  langid = {english}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  date = {2005-08-30},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
  urldate = {2021-03-18},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia}
}

@article{jangOptimalPolicyAttentionmodulated2021,
  title = {Optimal Policy for Attention-Modulated Decisions Explains Human Fixation Behavior},
  author = {Jang, Anthony Injoon and Sharma, Ravi and Drugowitsch, Jan},
  editor = {Tsetsos, Konstantinos},
  date = {2021-03-26},
  journaltitle = {eLife},
  volume = {10},
  pages = {e63436},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.63436},
  url = {https://doi.org/10.7554/eLife.63436},
  urldate = {2021-04-16},
  abstract = {Traditional accumulation-to-bound decision-making models assume that all choice options are processed with equal attention. In real life decisions, however, humans alternate their visual fixation between individual items to efficiently gather relevant information (Yang et al., 2016). These fixations also causally affect one's choices, biasing them toward the longer-fixated item (Krajbich et al., 2010). We derive a normative decision-making model in which attention enhances the reliability of information, consistent with neurophysiological findings (Cohen and Maunsell, 2009). Furthermore, our model actively controls fixation changes to optimize information gathering. We show that the optimal model reproduces fixation-related choice biases seen in humans and provides a Bayesian computational rationale for this phenomenon. This insight led to additional predictions that we could confirm in human data. Finally, by varying the relative cognitive advantage conferred by attention, we show that decision performance is benefited by a balanced spread of resources between the attended and unattended items.}
}

@article{jazayeriIntegrationSensoryEvidence2007,
  title = {Integration of Sensory Evidence in Motion Discrimination},
  author = {Jazayeri, Mehrdad and Movshon, J. Anthony},
  date = {2007-09-20},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {7},
  number = {12},
  pages = {7},
  issn = {1534-7362},
  doi = {10.1167/7.12.7},
  url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/7.12.7},
  urldate = {2022-05-18},
  abstract = {To make perceptual judgments, the brain must decode the responses of sensory cortical neurons. The direction of visual motion is represented by the activity of direction-selective neurons. Because these neurons are often broadly tuned and their responses are inherently variable, the brain must appropriately integrate their responses to infer the direction of motion reliably. The optimal integration strategy is task dependent. For coarse direction discriminations, neurons tuned to the directions of interest provide the most reliable information, but for fine discriminations, neurons with preferred directions displaced away from the target directions are more informative. We measured coarse and fine direction discriminations with random-dot stimuli. Unbeknownst to the observers, we added subthreshold motion signals of different directions to perturb the responses of different groups of direction-selective neurons. The pattern of biases induced by subthreshold signals of different directions indicates that subjects’ choice behavior relied on the activity of neurons with a wide range of preferred directions. For coarse discriminations, observers’ judgments were most strongly determined by neurons tuned to the target directions, but for fine discriminations, neurons with displaced preferred directions had the largest influence. We conclude that perceptual decisions rely on a population decoding strategy that takes the statistical reliability of sensory responses into account.},
  langid = {english}
}

@article{jazayeriOptimalRepresentationSensory2006,
  title = {Optimal Representation of Sensory Information by Neural Populations},
  author = {Jazayeri, Mehrdad and Movshon, J. Anthony},
  date = {2006-05},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {9},
  number = {5},
  pages = {690--696},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1691},
  url = {https://www.nature.com/articles/nn1691},
  urldate = {2022-05-18},
  abstract = {Sensory information is encoded by populations of neurons. The responses of individual neurons are inherently noisy, so the brain must interpret this information as reliably as possible. In most situations, the optimal strategy for decoding the population signal is to compute the likelihoods of the stimuli that are consistent with an observed neural response. But it has not been clear how the brain can directly compute likelihoods. Here we present a simple and biologically plausible model that can realize the likelihood function by computing a weighted sum of sensory neuron responses. The model provides the basis for an optimal decoding of sensory information. It explains a variety of psychophysical observations on detection, discrimination and identification, and it also directly predicts the relative contributions that different sensory neurons make to perceptual judgments.},
  issue = {5},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences}
}

@article{keysersUsingBayesFactor2020,
  title = {Using {{Bayes}} Factor Hypothesis Testing in Neuroscience to Establish Evidence of Absence},
  author = {Keysers, Christian and Gazzola, Valeria and Wagenmakers, Eric-Jan},
  date = {2020-07},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {23},
  number = {7},
  pages = {788--799},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-020-0660-4},
  url = {https://www.nature.com/articles/s41593-020-0660-4},
  urldate = {2022-04-04},
  abstract = {Most neuroscientists would agree that for brain research to progress, we have to know which experimental manipulations have no effect as much as we must identify those that do have an effect. The dominant statistical approaches used in neuroscience rely on P values and can establish the latter but not the former. This makes non-significant findings difficult to interpret: do they support the null hypothesis or are they simply not informative? Here we show how Bayesian hypothesis testing can be used in neuroscience studies to establish both whether there is evidence of absence and whether there is absence of evidence. Through simple tutorial-style examples of Bayesian t-tests and ANOVA using the open-source project JASP, this article aims to empower neuroscientists to use this approach to provide compelling and rigorous evidence for the absence of an effect.},
  issue = {7},
  langid = {english},
  keywords = {Empathy,Publishing}
}

@article{keysersUsingBayesFactor2020a,
  title = {Using {{Bayes}} Factor Hypothesis Testing in Neuroscience to Establish Evidence of Absence},
  author = {Keysers, Christian and Gazzola, Valeria and Wagenmakers, Eric-Jan},
  date = {2020-07},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {23},
  number = {7},
  pages = {788--799},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-020-0660-4},
  url = {http://www.nature.com/articles/s41593-020-0660-4},
  urldate = {2022-05-10},
  langid = {english}
}

@incollection{kokPredictiveCodingSensory2015,
  title = {Predictive {{Coding}} in {{Sensory Cortex}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Kok, Peter and family=Lange, given=Floris P., prefix=de, useprefix=true},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {221--244},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_11},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_11},
  urldate = {2019-03-20},
  abstract = {In recent years, predictive coding has become an increasingly influential model of how the brain processes sensory information. Predictive coding theories state that the brain is constantly trying to predict the inputs it receives, and each region in the cortical sensory hierarchy represents both these predictions and the mismatch between predictions and input (prediction error). In this chapter, we review the extant empirical evidence for this theory, as well as discuss recent theoretical advances. We find that predictive coding provides a good explanation for many phenomena observed in perception, and generates testable hypotheses. Furthermore, we suggest possible avenues for further empirical testing and for broadening the perspective of the role predictive coding may play in cognition.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Expectation,fMRI,Perception,Perceptual inference,Prediction,Predictive coding}
}

@article{kordingBayesianDecisionTheory2006,
  title = {Bayesian Decision Theory in Sensorimotor Control},
  author = {Körding, Konrad P. and Wolpert, Daniel M.},
  date = {2006-07},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {7},
  pages = {319--326},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.05.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661306001276},
  urldate = {2021-03-12},
  langid = {english}
}

@article{kordingBayesianIntegrationSensorimotor2004,
  title = {Bayesian Integration in Sensorimotor Learning},
  author = {Körding, Konrad P. and Wolpert, Daniel M.},
  date = {2004-01-15},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {427},
  number = {6971},
  eprint = {14724638},
  eprinttype = {pmid},
  pages = {244--247},
  issn = {1476-4687},
  doi = {10.1038/nature02169},
  abstract = {When we learn a new motor skill, such as playing an approaching tennis ball, both our sensors and the task possess variability. Our sensors provide imperfect information about the ball's velocity, so we can only estimate it. Combining information from multiple modalities can reduce the error in this estimate. On a longer time scale, not all velocities are a priori equally probable, and over the course of a match there will be a probability distribution of velocities. According to bayesian theory, an optimal estimate results from combining information about the distribution of velocities-the prior-with evidence from sensory feedback. As uncertainty increases, when playing in fog or at dusk, the system should increasingly rely on prior knowledge. To use a bayesian strategy, the brain would need to represent the prior distribution and the level of uncertainty in the sensory feedback. Here we control the statistical variations of a new sensorimotor task and manipulate the uncertainty of the sensory feedback. We show that subjects internally represent both the statistical distribution of the task and their sensory uncertainty, combining them in a manner consistent with a performance-optimizing bayesian process. The central nervous system therefore employs probabilistic models during sensorimotor learning.},
  langid = {english},
  keywords = {Bayes Theorem,Brain,Feedback,Female,Fingers,Humans,Learning,Male,Motor Skills,Movement,Normal Distribution,Photic Stimulation,Psychomotor Performance}
}

@article{kordingDecisionTheoryWhat2007,
  title = {Decision {{Theory}}: {{What}} "{{Should}}" the {{Nervous System Do}}?},
  shorttitle = {Decision {{Theory}}},
  author = {Körding, Konrad},
  date = {2007-10-26},
  journaltitle = {Science},
  volume = {318},
  number = {5850},
  eprint = {17962554},
  eprinttype = {pmid},
  pages = {606--610},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1142998},
  url = {https://science.sciencemag.org/content/318/5850/606},
  urldate = {2021-03-10},
  abstract = {The purpose of our nervous system is to allow us to successfully interact with our environment. This normative idea is formalized by decision theory that defines which choices would be most beneficial. We live in an uncertain world, and each decision may have many possible outcomes; choosing the best decision is thus complicated. Bayesian decision theory formalizes these problems in the presence of uncertainty and often provides compact models that predict observed behavior. With its elegant formalization of the problems faced by the nervous system, it promises to become a major inspiration for studies in neuroscience.},
  langid = {english}
}

@article{krakauerNeuroscienceNeedsBehavior2017,
  title = {Neuroscience {{Needs Behavior}}: {{Correcting}} a {{Reductionist Bias}}},
  shorttitle = {Neuroscience {{Needs Behavior}}},
  author = {Krakauer, John W. and Ghazanfar, Asif A. and Gomez-Marin, Alex and MacIver, Malcolm A. and Poeppel, David},
  date = {2017-02-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {93},
  number = {3},
  eprint = {28182904},
  eprinttype = {pmid},
  pages = {480--490},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.12.041},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(16)31040-6},
  urldate = {2021-06-23},
  langid = {english}
}

@article{kruschkeBayesianEstimationSupersedes2013,
  title = {Bayesian Estimation Supersedes the t Test.},
  author = {Kruschke, John K.},
  date = {2013},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {142},
  number = {2},
  pages = {573--603},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0029146},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029146},
  urldate = {2019-01-30},
  abstract = {Bayesian estimation for 2 groups provides complete distributions of credible values for the effect size, group means and their difference, standard deviations and their difference, and the normality of the data. The method handles outliers. The decision rule can accept the null value (unlike traditional t tests) when certainty in the estimate is high (unlike Bayesian model comparison using Bayes factors). The method also yields precise estimates of statistical power for various research goals. The software and programs are free and run on Macintosh, Windows, and Linux platforms.},
  langid = {english}
}

@book{kruschkeDoingBayesianData2015,
  title = {Doing Bayesian Data Analysis (Second Edition)},
  author = {Kruschke, John},
  date = {2015},
  publisher = {{Academic Press}},
  location = {{Boston}},
  added-at = {2016-12-29T09:25:25.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/277f97f6f84d077790b702e30ed86be5f/becker},
  interhash = {5e4043e24f7f58a8de076d7956ca08ea},
  intrahash = {77f97f6f84d077790b702e30ed86be5f},
  keywords = {diss imported inthesis mixedtrails},
  timestamp = {2017-06-19T10:12:05.000+0200}
}

@article{lamichhaneExploringBrainbehaviorRelationships2020,
  title = {Exploring Brain-Behavior Relationships in the {{N-back}} Task},
  author = {Lamichhane, Bidhan and Westbrook, Andrew and Cole, Michael W. and Braver, Todd S.},
  date = {2020-05-15},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {212},
  pages = {116683},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.116683},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811920301701},
  urldate = {2021-03-24},
  abstract = {Working memory (WM) function has traditionally been investigated in terms of two dimensions: within-individual effects of WM load, and between-individual differences in task performance. In human neuroimaging studies, the N-back task has frequently been used to study both. A reliable finding is that activation in frontoparietal regions exhibits an inverted-U pattern, such that activity tends to decrease at high load levels. Yet it is not known whether such U-shaped patterns are a key individual differences factor that can predict load-related changes in task performance. The current study investigated this question by manipulating load levels across a much wider range than explored previously (N~\hspace{0pt}=~\hspace{0pt}1–6), and providing a more comprehensive examination of brain-behavior relationships. In a sample of healthy young adults (n~\hspace{0pt}=~\hspace{0pt}57), the analysis focused on a distinct region of left lateral prefrontal cortex (LPFC) identified in prior work to show a unique relationship with task performance and WM function. In this region it was the linear slope of load-related activity, rather than the U-shaped pattern, that was positively associated with individual differences in target accuracy. Comprehensive supplemental analyses revealed the brain-wide selectivity of this pattern. Target accuracy was also independently predicted by the global resting-state connectivity of this LPFC region. These effects were robust, as demonstrated by cross-validation analyses and out-of-sample prediction, and also critically, were primarily driven by the high-load conditions. Together, the results highlight the utility of high-load conditions for investigating individual differences in WM function.},
  langid = {english},
  keywords = {Default mode network,Dorsolateral prefrontal cortex,Frontal-parietal network,N-back,Salience network,Working memory}
}

@article{lazarASAStatementPValues2016,
  title = {The {{ASA}}'s {{Statement}} on p-{{Values}}: {{Context}}, {{Process}}, and {{Purpose AU}}  - {{Wasserstein}}, {{Ronald L}}.},
  shorttitle = {The {{ASA}}'s {{Statement}} on p-{{Values}}},
  author = {Lazar, Nicole A.},
  date = {2016-04-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  url = {https://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108},
  urldate = {2019-01-30}
}

@book{leeBayesianCognitiveModeling2014a,
  title = {Bayesian {{Cognitive Modeling}}: {{A Practical Course}}},
  shorttitle = {Bayesian {{Cognitive Modeling}}},
  author = {Lee, Michael D. and Wagenmakers, Eric-Jan},
  date = {2014},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9781139087759},
  url = {https://www.cambridge.org/core/books/bayesian-cognitive-modeling/B477C799F1DB4EBB06F4EBAFBFD2C28B},
  urldate = {2021-03-01},
  abstract = {Bayesian inference has become a standard method of analysis in many fields of science. Students and researchers in experimental psychology and cognitive science, however, have failed to take full advantage of the new and exciting possibilities that the Bayesian approach affords. Ideal for teaching and self study, this book demonstrates how to do Bayesian modeling. Short, to-the-point chapters offer examples, exercises, and computer code (using WinBUGS or JAGS, and supported by Matlab and R), with additional support available online. No advance knowledge of statistics is required and, from the very start, readers are encouraged to apply and adjust Bayesian analyses by themselves. The book contains a series of chapters on parameter estimation and model selection, followed by detailed case studies from cognitive science. After working through this book, readers should be able to build their own Bayesian models, apply the models to their own data, and draw their own conclusions.},
  isbn = {978-1-107-01845-7}
}

@article{lindeDecisionsEquivalenceComparison2020,
  title = {Decisions {{About Equivalence}}: {{A Comparison}} of {{TOST}}, {{HDI-ROPE}}, and the {{Bayes Factor}}},
  shorttitle = {Decisions {{About Equivalence}}},
  author = {Linde, Maximilian and Tendeiro, Jorge and Selker, Ravi and Wagenmakers, Eric-Jan and family=Ravenzwaaij, given=Don, prefix=van, useprefix=false},
  date = {2020-11-10T16:07:30},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/bh8vu},
  url = {https://psyarxiv.com/bh8vu/},
  urldate = {2022-03-02},
  abstract = {Some important research questions require the ability to find evidence for two conditions being practically equivalent. This is impossible to accomplish within the traditional frequentist null hypothesis significance testing framework; hence, other methodologies must be utilized. We explain and illustrate three approaches for finding evidence for equivalence: The frequentist two one-sided tests procedure, the Bayesian highest density interval region of practical equivalence procedure, and the Bayes factor interval null procedure. We compare the classification performances of these three approaches for various plausible scenarios. The results indicate that the Bayes factor interval null approach compares favorably to the other two approaches in terms of statistical power. Critically, compared to the Bayes factor interval null procedure, the two one-sided tests and the highest density interval region of practical equivalence procedures have limited discrimination capabilities when the sample size is relatively small: specifically, in order to be practically useful, these two methods generally require over 250 cases within each condition when rather large equivalence margins of approximately 0.2 or 0.3 are used; for smaller equivalence margins even more cases are required. Because of these results, we recommend that researchers rely more on the Bayes factor interval null approach for quantifying evidence for equivalence, especially for studies that are constrained on sample size.},
  langid = {american},
  keywords = {equivalence testing,highest density interval,interval Bayes factor,Quantitative Methods,region of practical equivalence,Social and Behavioral Sciences,Statistical Methods,two one-sided tests}
}

@online{lindelovReactionTimeDistributions2021,
  title = {Reaction {{Time Distributions}}},
  author = {Lindeløv, Jonas Kristoffer},
  date = {2021-05-18},
  url = {https://lindeloev.shinyapps.io/shiny-rt/}
}

@incollection{loganInhibitoryControlMind2015,
  title = {Inhibitory {{Control}} in {{Mind}} and {{Brain}}: {{The Mathematics}} and {{Neurophysiology}} of the {{Underlying Computation}}},
  shorttitle = {Inhibitory {{Control}} in {{Mind}} and {{Brain}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Logan, Gordon D. and Schall, Jeffrey D. and Palmeri, Thomas J.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {303--320},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_15},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_15},
  urldate = {2019-03-20},
  abstract = {We develop desiderata for a computational theory of response inhibition that links mathematical psychology with neuroscience. The theory must be explicit mathematically and computationally, and grounded in behavior and neurophysiology. The theory must provide quantitative accounts of complexities of behavior in response inhibition tasks and must predict the neural activity that underlies performance. We evaluate three current theories of response inhibition in the stop signal paradigm using these desiderata, and we find that one theory fulfills the desiderata better than the others.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Computational theory of response inhibition,Mathematical psychology,Neuroscience,Response inhibition}
}

@article{mathotPupillometryPsychologyPhysiology2018,
  title = {Pupillometry: {{Psychology}}, {{Physiology}}, and {{Function}}},
  shorttitle = {Pupillometry},
  author = {Mathot, Sebastiaan},
  date = {2018-02-21},
  journaltitle = {Journal of Cognition},
  volume = {1},
  number = {1},
  pages = {16},
  publisher = {{Ubiquity Press}},
  issn = {2514-4820},
  doi = {10.5334/joc.18},
  url = {http://www.journalofcognition.org/articles/10.5334/joc.18/},
  urldate = {2021-05-03},
  abstract = {Article: Pupillometry: Psychology, Physiology, and Function},
  issue = {1},
  langid = {english}
}

@article{matzkePsychologicalInterpretationExGaussian2009,
  title = {Psychological Interpretation of the Ex-{{Gaussian}} and Shifted {{Wald}} Parameters: {{A}} Diffusion Model Analysis},
  shorttitle = {Psychological Interpretation of the Ex-{{Gaussian}} and Shifted {{Wald}} Parameters},
  author = {Matzke, Dora and Wagenmakers, Eric-Jan},
  date = {2009-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {5},
  pages = {798--817},
  issn = {1531-5320},
  doi = {10.3758/PBR.16.5.798},
  url = {https://doi.org/10.3758/PBR.16.5.798},
  urldate = {2022-04-05},
  abstract = {A growing number of researchers use descriptive distributions such as the ex-Gaussian and the shifted Wald to summarize response time data for speeded two-choice tasks. Some of these researchers also assume that the parameters of these distributions uniquely correspond to specific cognitive processes. We studied the validity of this cognitive interpretation by relating the parameters of the ex-Gaussian and shifted Wald distributions to those of the Ratcliff diffusion model, a successful model whose parameters have well-established cognitive interpretations. In a simulation study, we fitted the ex-Gaussian and shifted Wald distributions to data generated from the diffusion model by systematically varying its parameters across a wide range of plausible values. In an empirical study, the two descriptive distributions were fitted to published data that featured manipulations of task difficulty, response caution, and a priori bias. The results clearly demonstrate that the ex-Gaussian and shifted Wald parameters do not correspond uniquely to parameters of the diffusion model. We conclude that researchers should resist the temptation to interpret changes in the ex-Gaussian and shifted Wald parameters in terms of cognitive processes. Supporting materials may be downloaded from http://pbr.psychonomic-journals .org/content/supplemental.},
  langid = {english}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical Rethinking: {{A}} Bayesian Course with Examples in r and Stan},
  author = {McElreath, R.},
  date = {2020},
  series = {A Chapman \& Hall Book},
  publisher = {{CRC Press}},
  url = {https://books.google.ch/books?id=Ie2vxQEACAAJ},
  isbn = {978-0-367-13991-9},
  lccn = {2019957006}
}

@article{meuleReportingInterpretingWorking2017,
  title = {Reporting and {{Interpreting Working Memory Performance}} in N-Back {{Tasks}}},
  author = {Meule, Adrian},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00352},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00352/full},
  urldate = {2021-03-25},
  abstract = {Reporting and Interpreting Working Memory Performance in n-back Tasks},
  langid = {english},
  keywords = {accuracy,emotional stimuli,n-back task,reaction times,working memory}
}

@article{millerWarningMedianReaction1988,
  title = {A Warning about Median Reaction Time},
  author = {Miller, J.},
  date = {1988-08},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {14},
  number = {3},
  eprint = {2971778},
  eprinttype = {pmid},
  pages = {539--543},
  issn = {0096-1523},
  doi = {10.1037//0096-1523.14.3.539},
  abstract = {When used with positively skewed reaction time distributions, sample medians tend to over-estimate population medians. The extent of overestimation is related directly to the amount of skew in the reaction time distributions and inversely to the size of the sample over which the median is computed. Simulations indicate that overestimation could approach 50 ms with small samples and highly skewed distributions. An important practical consequence of the bias in median reaction time is that sample medians must not be used to compare reaction times across experimental conditions when there are unequal numbers of trials in the conditions. If medians are used with unequal sample sizes, then the bias may produce an artifactual difference in conditions or conceal a true difference. Some recent studies of cuing and stimulus probability effects provide examples of this potential artifact.},
  langid = {english},
  keywords = {Computer Simulation,Humans,Reaction Time,Statistics as Topic}
}

@article{moreyConfidenceIntervalsNormalized2008,
  title = {Confidence {{Intervals}} from {{Normalized Data}}: {{A}} Correction to {{Cousineau}} (2005)},
  shorttitle = {Confidence {{Intervals}} from {{Normalized Data}}},
  author = {Morey, Richard D.},
  date = {2008-09-01},
  journaltitle = {Tutorials in Quantitative Methods for Psychology},
  shortjournal = {TQMP},
  volume = {4},
  number = {2},
  pages = {61--64},
  issn = {1913-4126},
  doi = {10.20982/tqmp.04.2.p061},
  url = {http://www.tqmp.org/RegularArticles/vol04-2/p061},
  urldate = {2022-03-14},
  langid = {english}
}

@article{moscatelliModelingPsychophysicalData2012a,
  title = {Modeling Psychophysical Data at the Population-Level: {{The}} Generalized Linear Mixed Model},
  shorttitle = {Modeling Psychophysical Data at the Population-Level},
  author = {Moscatelli, A. and Mezzetti, M. and Lacquaniti, F.},
  date = {2012-10-25},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {12},
  number = {11},
  pages = {26--26},
  issn = {1534-7362},
  doi = {10.1167/12.11.26},
  url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/12.11.26},
  urldate = {2021-04-26},
  abstract = {In psychophysics, researchers usually apply a two-level model for the analysis of the behavior of the single subject and the population. This classical model has two main disadvantages. First, the second level of the analysis discards information on trial repetitions and subject-specific variability. Second, the model does not easily allow assessing the goodness of fit. As an alternative to this classical approach, here we propose the Generalized Linear Mixed Model (GLMM). The GLMM separately estimates the variability of fixed and random effects, it has a higher statistical power, and it allows an easier assessment of the goodness of fit compared with the classical two-level model. GLMMs have been frequently used in many disciplines since the 1990s; however, they have been rarely applied in psychophysics. Furthermore, to our knowledge, the issue of estimating the point-of-subjective-equivalence (PSE) within the GLMM framework has never been addressed. Therefore the article has two purposes: It provides a brief introduction to the usage of the GLMM in psychophysics, and it evaluates two different methods to estimate the PSE and its variability within the GLMM framework. We compare the performance of the GLMM and the classical two-level model on published experimental data and simulated data. We report that the estimated values of the parameters were similar between the two models and Type I errors were below the confidence level in both models. However, the GLMM has a higher statistical power than the two-level model. Moreover, one can easily compare the fit of different GLMMs according to different criteria. In conclusion, we argue that the GLMM can be a useful method in psychophysics.},
  langid = {english}
}

@article{motulskyCommonMisconceptionsData2014,
  title = {Common {{Misconceptions}} about {{Data Analysis}} and {{Statistics}}},
  author = {Motulsky, Harvey J.},
  date = {2014-10-01},
  journaltitle = {Journal of Pharmacology and Experimental Therapeutics},
  shortjournal = {J Pharmacol Exp Ther},
  volume = {351},
  number = {1},
  eprint = {25204545},
  eprinttype = {pmid},
  pages = {200--205},
  publisher = {{American Society for Pharmacology and Experimental Therapeutics}},
  issn = {0022-3565, 1521-0103},
  doi = {10.1124/jpet.114.219170},
  url = {https://jpet.aspetjournals.org/content/351/1/200},
  urldate = {2022-04-11},
  abstract = {Ideally, any experienced investigator with the right tools should be able to reproduce a finding published in a peer-reviewed biomedical science journal. In fact, however, the reproducibility of a large percentage of published findings has been questioned. Undoubtedly, there are many reasons for this, but one reason may be that investigators fool themselves due to a poor understanding of statistical concepts. In particular, investigators often make these mistakes: 1) P-hacking, which is when you reanalyze a data set in many different ways, or perhaps reanalyze with additional replicates, until you get the result you want; 2) overemphasis on P values rather than on the actual size of the observed effect; 3) overuse of statistical hypothesis testing, and being seduced by the word “significant”; and 4) over-reliance on standard errors, which are often misunderstood.},
  langid = {english}
}

@article{mulderBiasBrainDiffusion2012a,
  title = {Bias in the {{Brain}}: {{A Diffusion Model Analysis}} of {{Prior Probability}} and {{Potential Payoff}}},
  shorttitle = {Bias in the {{Brain}}},
  author = {Mulder, M. J. and Wagenmakers, E.-J. and Ratcliff, R. and Boekel, W. and Forstmann, B. U.},
  date = {2012-02-15},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {32},
  number = {7},
  pages = {2335--2343},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4156-11.2012},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4156-11.2012},
  urldate = {2021-05-31},
  langid = {english}
}

@article{navarroFastAccurateCalculations,
  title = {Fast and Accurate Calculations for First-Passage Times in {{Wiener}} Diffusion Models},
  author = {Navarro, Daniel J and Fuss, Ian G},
  pages = {20},
  abstract = {We propose a new method for quickly calculating the probability density function for first passage times in simple Wiener diffusion models, extending an earlier method used by Van Zandt, Colonius and Proctor (2000). The method relies on the observation that there are two distinct infinite series expansions of this probability density, one of which converges quickly for small time values, while the other converges quickly at large time values. By deriving error bounds associated with finite truncation of either expansion, we are able to determine analytically which of the two versions should be applied in any particular context. The bounds indicate that, even for extremely stringent error tolerances, no more than 8 terms are required to calculate the probability density. By making the calculation of this distribution tractable, the goal is to allow more complex extensions of Wiener diffusion models to be developed.},
  langid = {english},
  keywords = {❓ Multiple DOI}
}

@article{navarroIfMathematicalPsychology2020,
  title = {If Mathematical Psychology Did Not Exist We Might Need to Invent It: {{A}} Comment on Theory Building in Psychology},
  shorttitle = {If Mathematical Psychology Did Not Exist We Might Need to Invent It},
  author = {Navarro, Danielle},
  date = {2020-03-16T20:56:43},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/ygbjp},
  url = {https://psyarxiv.com/ygbjp/},
  urldate = {2021-02-19},
  abstract = {It is commonplace, when discussing the subject of psychological theory, to write articles from the assumption that psychology differs from physical sciences in that we have no theories that would support cumulative, incremental science. In this brief paper I discuss one counterexample, namely Shepard's (1987) law of generalization and the various Bayesian extensions that it inspired over the last three decades. Using Shepard's law as a running example I argue that psychological theory building is not a statistical problem; mathematical formalism is theoretically beneficial; measurement and theory have a complex relationship; rewriting old theory can yield new insights; and finally, that theoretical growth can drive empirical work. Though generally suggesting that the tools of mathematical psychology are valuable to the psychological theorist, the paper also comments on some limitations to this approach.},
  keywords = {Cognitive Psychology,Concepts and Categories,Meta-science,Reasoning,Social and Behavioral Sciences,Theory and Philosophy of Science}
}

@article{navarroPersonalEssayBayes2020,
  title = {A Personal Essay on {{Bayes}} Factors},
  author = {Navarro, Danielle},
  date = {2020-12-07T06:46:53},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/nujy6},
  url = {https://psyarxiv.com/nujy6/},
  urldate = {2021-03-11},
  abstract = {This is an archived version of a blog post on Bayes factors. It is a personal reflection on some of the practical issues that one encounters when attempting to apply Bayes factors to difficult inference problems. The main message of the piece is real world inference is hard and that being too prescriptive about how statistics must be done is generally a recipe for disaster.},
  keywords = {Bayes factors,Mathematical Psychology,Quantitative Methods,Social and Behavioral Sciences}
}

@article{nunezTutorialFittingJoint2022a,
  title = {A Tutorial on Fitting Joint Models of {{M}}/{{EEG}} and Behavior to Understand Cognition},
  author = {Nunez, Michael D. and Vandekerckhove, Joachim and Srinivasan, Ramesh},
  date = {2022-03-04T15:40:04},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/vf6t5},
  url = {https://psyarxiv.com/vf6t5/},
  urldate = {2022-03-10},
  abstract = {We present motivation and practical steps necessary to find parameter estimates of joint models of behavior and neural electrophysiological data. This tutorial is written for researchers wishing to build joint models of human behavior and scalp and intracranial electroencephalographic (EEG) or magnetoencephalographic (MEG) data, and more specifically those researchers who seek to understand human cognition. Although these techniques could easily be applied to animal models. Joint modeling of M/EEG and behavior requires some knowledge of existing computational and cognitive theories, M/EEG artifact correction, M/EEG analysis techniques, cognitive modeling, and programming for statistical modeling implementation. This paper seeks to give an introduction to these techniques as they apply to estimating parameters from neurocognitive models of M/EEG and human behavior, and to evaluate model results and compare models. Due to our research and knowledge on the subject matter, our examples in this paper will focus on testing specific hypotheses in human decision-making theory. However most of the motivation and discussion of this paper applies across many modeling procedures and applications.},
  langid = {american},
  keywords = {Cognitive modeling,Cognitive Neuroscience,Cognitive Psychology,Computational modeling,Computational Neuroscience,Electroencephalography (EEG),Magnetoencephalography (MEG),Neuroscience,Psychology,Social and Behavioral Sciences}
}

@article{oberauerWorkingMemoryCapacity2019,
  title = {Working {{Memory Capacity Limits Memory}} for {{Bindings}}},
  author = {Oberauer, Klaus},
  date = {2019-09-19},
  journaltitle = {Journal of Cognition},
  volume = {2},
  number = {1},
  pages = {40},
  publisher = {{Ubiquity Press}},
  issn = {2514-4820},
  doi = {10.5334/joc.86},
  url = {http://www.journalofcognition.org/article/10.5334/joc.86/},
  urldate = {2021-04-27},
  abstract = {Article: Working Memory Capacity Limits Memory for Bindings},
  issue = {1},
  langid = {english}
}

@incollection{oreillyBayesianModelsCognitive2015,
  title = {Bayesian {{Models}} in {{Cognitive Neuroscience}}: {{A Tutorial}}},
  shorttitle = {Bayesian {{Models}} in {{Cognitive Neuroscience}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {O’Reilly, Jill X. and Mars, Rogier B.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {179--197},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_9},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_9},
  urldate = {2019-03-20},
  abstract = {This chapter provides an introduction to Bayesian models and their application in cognitive neuroscience. The central feature of Bayesian models, as opposed to other classes of models, is that Bayesian models represent the beliefs of an observer as probability distributions, allowing them to integrate information while taking its uncertainty into account. In the chapter, we will consider how the probabilistic nature of Bayesian models makes them particularly useful in cognitive neuroscience. We will consider two types of tasks in which we believe a Bayesian approach is useful: optimal integration of evidence from different sources, and the development of beliefs about the environment given limited information (such as during learning). We will develop some detailed examples of Bayesian models to give the reader a taste of how the models are constructed and what insights they may be able to offer about participants’ behavior and brain activity.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Attention,Bayes,Bayesian modelling,Probability,Uncertainty}
}

@article{panichelloSharedMechanismsUnderlie2021,
  title = {Shared Mechanisms Underlie the Control of Working Memory and Attention},
  author = {Panichello, Matthew F. and Buschman, Timothy J.},
  date = {2021-03-31},
  journaltitle = {Nature},
  pages = {1--5},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03390-w},
  url = {https://www.nature.com/articles/s41586-021-03390-w},
  urldate = {2021-04-01},
  abstract = {Cognitive control guides behaviour by controlling what, when, and how information is represented in the brain1. For example, attention controls sensory processing; top-down signals from prefrontal and parietal cortex strengthen the representation of task-relevant stimuli2–4. A similar ‘selection’ mechanism is thought to control the representations held ‘in mind’—in working memory5–10. Here we show that shared neural mechanisms underlie the selection of items from working memory and attention to sensory stimuli. We trained rhesus monkeys to switch between two tasks, either selecting one item from a set of items held in working memory or attending to one stimulus from a set of visual stimuli. Neural recordings showed that similar representations in prefrontal cortex encoded the control of both selection and attention, suggesting that prefrontal cortex acts as a domain-general controller. By contrast, both attention and selection were represented independently in parietal and visual cortex. Both selection and attention facilitated behaviour by enhancing and transforming the representation of the selected memory or attended stimulus. Specifically, during the selection task, memory items were initially represented in independent subspaces of neural activity in prefrontal cortex. Selecting an item caused its representation to transform from its own subspace to a new subspace used to guide behaviour. A similar transformation occurred for attention. Our results suggest that prefrontal cortex controls cognition by dynamically transforming representations to control what and when cognitive computations are engaged.},
  langid = {english}
}

@book{peirceBuildingExperimentsPsychoPy2018,
  title = {Building {{Experiments}} in {{PsychoPy}}},
  author = {Peirce, Jonathan and MacAskill, Michael},
  date = {2018-05-23},
  eprint = {Pp1YDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{SAGE}},
  abstract = {PsychoPy is an open-source (free) software package for creating rich, dynamic experiments for psychology, neuroscience, and linguistics. It provides an intuitive graphical interface (the ‘Builder’) as well as the option to insert Python code. This combination makes it easy for teaching, but also flexible enough for all manner of behavioural experiments. Divided into three parts, this textbook is suitable for teaching practical undergraduate classes on research methods, or as a reference text for the professional scientist. The book is written by Jonathan Peirce, the original creator of PsychoPy and Michael MacAskill, and they utilise their breadth of experience in Python development to educate students and researchers in this intuitive, yet powerful, experiment generation package.},
  isbn = {978-1-5264-1816-6},
  langid = {english},
  pagetotal = {313},
  keywords = {Psychology / Experimental Psychology,Psychology / General,Psychology / Research & Methodology,Reference / Research}
}

@article{pratteExploringDifferencesDistributional2010,
  title = {Exploring the Differences in Distributional Properties between {{Stroop}} and {{Simon}} Effects Using Delta Plots},
  author = {Pratte, Michael S. and Rouder, Jeffrey N. and Morey, Richard D. and Feng, Chuning},
  date = {2010-10-01},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Attention, Perception, \& Psychophysics},
  volume = {72},
  number = {7},
  pages = {2013--2025},
  issn = {1943-393X},
  doi = {10.3758/APP.72.7.2013},
  url = {https://doi.org/10.3758/APP.72.7.2013},
  urldate = {2022-04-12},
  abstract = {Stroop and Simon tasks are logically similar and are often used to investigate cognitive control and inhibition processes. We compare the distributional properties of Stroop and Simon effects with delta plots and find different although stable patterns. Stroop effects across a variety of conditions are smallest for fast responses and increase as responses slow. Simon effects across a variety of conditions, however, are largest for fast responses but decrease, and even reverse, as responses slow. We show in three experiments that these diverging patterns hold within participants and even when the stimulus materials are identical across the tasks. These stable differences in time course serve as bedrock phenomena for building and testing theories of cognitive control and inhibition. The results of two additional experiments suggest that the determinant of time course is not simply whether the distracting information is location.},
  langid = {english}
}

@article{ratcliffDiffusionDecisionModel2008b,
  title = {The {{Diffusion Decision Model}}: {{Theory}} and {{Data}} for {{Two-Choice Decision Tasks}}},
  shorttitle = {The {{Diffusion Decision Model}}},
  author = {Ratcliff, Roger and McKoon, Gail},
  date = {2008-04},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {20},
  number = {4},
  pages = {873--922},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2008.12-06-420},
  url = {https://direct.mit.edu/neco/article/20/4/873-922/7299},
  urldate = {2021-05-25},
  abstract = {The diffusion decision model allows detailed explanations of behavior in two-choice discrimination tasks. In this article, the model is reviewed to show how it translates behavioral data—accuracy, mean response times, and response time distributions—into components of cognitive processing. Three experiments are used to illustrate experimental manipulations of three components: stimulus difficulty affects the quality of information on which a decision is based; instructions emphasizing either speed or accuracy affect the criterial amounts of information that a subject requires before initiating a response; and the relative proportions of the two stimuli affect biases in drift rate and starting point. The experiments also illustrate the strong constraints that ensure the model is empirically testable and potentially falsifiable. The broad range of applications of the model is also reviewed, including research in the domains of aging and neurophysiology.},
  langid = {english}
}

@article{ratcliffEstimatingParametersDiffusion2002,
  title = {Estimating Parameters of the Diffusion Model: {{Approaches}} to Dealing with Contaminant Reaction Times and Parameter Variability},
  shorttitle = {Estimating Parameters of the Diffusion Model},
  author = {Ratcliff, Roger and Tuerlinckx, Francis},
  date = {2002-09},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {9},
  number = {3},
  pages = {438--481},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03196302},
  url = {http://link.springer.com/10.3758/BF03196302},
  urldate = {2022-05-02},
  langid = {english}
}

@article{ravenzwaaijAdvantagesMasqueradingIssues2019,
  title = {Advantages {{Masquerading}} as ‘{{Issues}}’ in {{Bayesian Hypothesis Testing}}: {{A Commentary}} on {{Tendeiro}} and {{Kiers}} (2019)},
  shorttitle = {Advantages {{Masquerading}} as ‘{{Issues}}’ in {{Bayesian Hypothesis Testing}}},
  author = {family=Ravenzwaaij, given=Don, prefix=van, useprefix=false and Wagenmakers, Eric-Jan},
  date = {2019-09-03T12:42:56},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/nf7rp},
  url = {https://psyarxiv.com/nf7rp/},
  urldate = {2021-05-03},
  abstract = {Tendeiro and Kiers (2019) provide a detailed and scholarly critique of Null Hypothesis Bayesian Testing (NHBT) and its central component –the Bayes factor– that allows researchers to update knowledge and quantify statistical evidence. Tendeiro and Kiers conclude that NHBT constitutes an improvement over frequentist p-values, but primarily elaborate on a list of eleven ‘issues’ of NHBT. In this commentary, we provide context to each issue and conclude that many issues may in fact be conceived as pronounced advantages of NHBT.},
  keywords = {Bayes factors,Bayesian hypothesis testing,parameter estimation,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods}
}

@article{robertsDecisionMoveResponse2019,
  title = {The Decision to Move: Response Times, Neuronal Circuits and Sensory Memory in a Simple Vertebrate},
  shorttitle = {The Decision to Move},
  author = {Roberts, Alan and Borisyuk, Roman and Buhl, Edgar and Ferrario, Andrea and Koutsikou, Stella and Li, Wen-Chang and Soffe, Stephen R.},
  date = {2019-03-27},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {286},
  number = {1899},
  pages = {20190297},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2019.0297},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2019.0297},
  urldate = {2022-04-04},
  abstract = {All animals use sensory systems to monitor external events and have to decide whether to move. Response times are long and variable compared to reflexes, and fast escape movements. The complexity of adult vertebrate brains makes it difficult to trace the neuronal circuits underlying basic decisions to move. To simplify the problem, we investigate the nervous system and responses of hatchling frog tadpoles which swim when their skin is stimulated. Studying the neuron-by-neuron pathway from sensory to hindbrain neurons, where the decision to swim is made, has revealed two simple pathways generating excitation which sums to threshold in these neurons to initiate swimming. The direct pathway leads to short, and reliable delays like an escape response. The other includes a population of sensory processing neurons which extend firing to introduce noise and delay into responses. These neurons provide a brief, sensory memory of the stimulus, that allows tadpoles to integrate stimuli occurring within a second or so of each other. We relate these findings to other studies and conclude that sensory memory makes a fundamental contribution to simple decisions and is present in the brainstem of a basic vertebrate at a surprisingly early stage in development.},
  keywords = {decisions,locomotion,response times,sensory memory}
}

@article{rohrerThinkingClearlyCorrelations2018a,
  title = {Thinking {{Clearly About Correlations}} and {{Causation}}: {{Graphical Causal Models}} for {{Observational Data}}},
  shorttitle = {Thinking {{Clearly About Correlations}} and {{Causation}}},
  author = {Rohrer, Julia M.},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {27--42},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245917745629},
  url = {https://doi.org/10.1177/2515245917745629},
  urldate = {2021-03-16},
  abstract = {Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables—colliders and mediators—should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors.},
  langid = {english},
  keywords = {directed acyclic graphs}
}

@article{rouderBayesianTestsAccepting2009a,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  date = {2009-04-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1531-5320},
  doi = {10.3758/PBR.16.2.225},
  url = {https://doi.org/10.3758/PBR.16.2.225},
  urldate = {2019-04-22},
  abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
  langid = {english},
  keywords = {Akaike Information Criterion,Marginal Likelihood,Posterior Odds,Prior Standard Deviation,Subliminal Priming}
}

@article{rouderBayesianTestsAccepting2009b,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  date = {2009-04-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1531-5320},
  doi = {10.3758/PBR.16.2.225},
  url = {https://doi.org/10.3758/PBR.16.2.225},
  urldate = {2021-04-19},
  abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
  langid = {english}
}

@article{rousseletDifferencesMeansRobust2017a,
  title = {Beyond Differences in Means: Robust Graphical Methods to Compare Two Groups in Neuroscience},
  shorttitle = {Beyond Differences in Means},
  author = {Rousselet, Guillaume A. and Pernet, Cyril R. and Wilcox, Rand R.},
  date = {2017},
  journaltitle = {European Journal of Neuroscience},
  volume = {46},
  number = {2},
  pages = {1738--1748},
  issn = {1460-9568},
  doi = {10.1111/ejn.13610},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.13610},
  urldate = {2021-05-17},
  abstract = {If many changes are necessary to improve the quality of neuroscience research, one relatively simple step could have great pay-offs: to promote the adoption of detailed graphical methods, combined with robust inferential statistics. Here, we illustrate how such methods can lead to a much more detailed understanding of group differences than bar graphs and t-tests on means. To complement the neuroscientist's toolbox, we present two powerful tools that can help us understand how groups of observations differ: the shift function and the difference asymmetry function. These tools can be combined with detailed visualisations to provide complementary perspectives about the data. We provide implementations in R and MATLAB of the graphical tools, and all the examples in the article can be reproduced using R scripts.},
  langid = {english},
  keywords = {data visualisation,difference asymmetry function,quantile estimation,robust statistics,shift function},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.13610}
}

@article{rousseletFewSimpleSteps2016a,
  title = {A Few Simple Steps to Improve the Description of Group Results in Neuroscience},
  author = {Rousselet, Guillaume A. and Foxe, John J. and Bolam, J. Paul},
  date = {2016},
  journaltitle = {European Journal of Neuroscience},
  volume = {44},
  number = {9},
  pages = {2647--2651},
  issn = {1460-9568},
  doi = {10.1111/ejn.13400},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.13400},
  urldate = {2021-05-10},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.13400}
}

@article{rousseletPercentileBootstrapPrimer2019,
  title = {The Percentile Bootstrap: A Primer with Step-by-Step Instructions in {{R}}},
  shorttitle = {The Percentile Bootstrap},
  author = {Rousselet, Guillaume and Pernet, Dr Cyril and Wilcox, Rand R.},
  date = {2019-06-29T13:01:14},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/kxarf},
  url = {https://psyarxiv.com/kxarf/},
  urldate = {2022-04-11},
  abstract = {The percentile bootstrap is the Swiss Army Knife of statistics: it is a non-parametric method based on data-driven simulations. It can be applied to many statistical problems, as a substitute to standard parametric approaches, or in situations where parametric methods do not exist. In this tutorial, we cover R code to implement the percentile bootstrap in a few situations: one-sample estimation and the comparison of two independent groups for measures of central tendency (means and trimmed means) and spread. For each example, we explain how to derive a bootstrap distribution, and how to get a confidence interval and a p value from that distribution. We also demonstrate how to run a simulation to assess the behaviour of the bootstrap. In some situations, the bootstrap performs poorly, such as when making inferences about the mean. But for other purposes, it is the only known method that works well over a broad range of situations, such as when comparing medians and there are tied (duplicated) values. More broadly, combining the percentile bootstrap with robust estimators, i.e. estimators that are not overly sensitive to outliers, the bootstrap can help users gain a deeper understanding of their data, relative to conventional methods.},
  langid = {american},
  keywords = {bootstrap,confidence interval,correlation,group comparison,MAD,mean,Meta-science,P value,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods,trimmed mean}
}

@article{rousseletReactionTimesOther2019a,
  title = {Reaction Times and Other Skewed Distributions: Problems with the Mean and the Median},
  shorttitle = {Reaction Times and Other Skewed Distributions},
  author = {Rousselet, Guillaume and Wilcox, Rand R.},
  date = {2019-01-17T11:18:00},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/3y54r},
  url = {https://psyarxiv.com/3y54r/},
  urldate = {2022-04-05},
  abstract = {To summarise skewed (asymmetric) distributions, such as reaction times, typically the mean or the median are used as measures of central tendency. Using the mean might seem surprising, given that it provides a poor measure of central tendency for skewed distributions, whereas the median provides a better indication of the location of the bulk of the observations. However, the sample median is biased: with small sample sizes, it tends to overestimate the population median. This is not the case for the mean. Based on this observation, Miller (1988) concluded that ”sample medians must not be used to compare reaction times across experimental conditions when there are unequal numbers of trials in the conditions.” Here we replicate and extend Miller (1988), and demonstrate that his conclusion was ill-advised for several reasons. First, the median’s bias can be corrected using a percentile bootstrap bias correction. Second, a careful examination of the sampling distributions reveals that the sample median is median unbiased, whereas the mean is median biased when dealing with skewed distributions. That is, on average the sample mean estimates the population mean, but typically this is not the case. In addition, simulations of false and true positives in various situations show that no method dominates. Crucially, neither the mean nor the median are sufficient or even necessary to compare skewed distributions. Different questions require different methods and it would be unwise to use the mean or the median in all situations. Better tools are available to get a deeper understanding of how distributions differ: we illustrate the hierarchical shift function, a powerful alternative that relies on quantile estimation. All the code and data to reproduce the figures and analyses in the article are available online.},
  langid = {american},
  keywords = {bias,bootstrap,estimation,mean,median,Meta-science,quantile,Quantitative Methods,sampling,skewness,Social and Behavioral Sciences,Statistical Methods,trimmed mean}
}

@unpublished{schadWorkflowTechniquesRobust2021,
  title = {Workflow {{Techniques}} for the {{Robust Use}} of {{Bayes Factors}}},
  author = {Schad, Daniel J. and Nicenboim, Bruno and Bürkner, Paul-Christian and Betancourt, Michael and Vasishth, Shravan},
  date = {2021-03-15},
  eprint = {2103.08744},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/2103.08744},
  urldate = {2021-03-20},
  abstract = {Inferences about hypotheses are ubiquitous in the cognitive sciences. Bayes factors provide one general way to compare different hypotheses by their compatibility with the observed data. Those quantifications can then also be used to choose between hypotheses. While Bayes factors provide an immediate approach to hypothesis testing, they are highly sensitive to details of the data/model assumptions. Moreover it's not clear how straightforwardly this approach can be implemented in practice, and in particular how sensitive it is to the details of the computational implementation. Here, we investigate these questions for Bayes factor analyses in the cognitive sciences. We explain the statistics underlying Bayes factors as a tool for Bayesian inferences and discuss that utility functions are needed for principled decisions on hypotheses. Next, we study how Bayes factors misbehave under different conditions. This includes a study of errors in the estimation of Bayes factors. Importantly, it is unknown whether Bayes factor estimates based on bridge sampling are unbiased for complex analyses. We are the first to use simulation-based calibration as a tool to test the accuracy of Bayes factor estimates. Moreover, we study how stable Bayes factors are against different MCMC draws. We moreover study how Bayes factors depend on variation in the data. We also look at variability of decisions based on Bayes factors and how to optimize decisions using a utility function. We outline a Bayes factor workflow that researchers can use to study whether Bayes factors are robust for their individual analysis, and we illustrate this workflow using an example from the cognitive sciences. We hope that this study will provide a workflow to test the strengths and limitations of Bayes factors as a way to quantify evidence in support of scientific hypotheses. Reproducible code is available from https://osf.io/y354c/.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology}
}

@article{schallAccumulatorsNeuronsResponse2019,
  title = {Accumulators, {{Neurons}}, and {{Response Time}}},
  author = {Schall, Jeffrey D.},
  date = {2019-12},
  journaltitle = {Trends in neurosciences},
  shortjournal = {Trends Neurosci},
  volume = {42},
  number = {12},
  eprint = {31704180},
  eprinttype = {pmid},
  pages = {848--860},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2019.10.001},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6981279/},
  urldate = {2022-04-04},
  abstract = {The marriage of cognitive neurophysiology and mathematical psychology to understand decision-making has been exceptionally productive. This interdisciplinary area is based on the proposition that particular neurons or circuits instantiate the accumulation of evidence specified by mathematical models of sequential sampling and stochastic accumulation. This linking proposition has earned widespread endorsement. Here, a brief survey of the history of the proposition precedes a review of multiple conundrums and paradoxes concerning the accuracy, precision, and transparency of that linking proposition. Correctly establishing how abstract models of decision-making are instantiated by particular neural circuits would represent a remarkable accomplishment in mapping mind to brain. Failing would reveal challenging limits for cognitive neuroscience. This is such a vigorous area of research because so much is at stake.},
  pmcid = {PMC6981279}
}

@article{schootGentleIntroductionBayesian2014,
  title = {A {{Gentle Introduction}} to {{Bayesian Analysis}}: {{Applications}} to {{Developmental Research}}},
  shorttitle = {A {{Gentle Introduction}} to {{Bayesian Analysis}}},
  author = {family=Schoot, given=Rens, prefix=van de, useprefix=false and Kaplan, David and Denissen, Jaap and Asendorpf, Jens B. and Neyer, Franz J. and family=Aken, given=Marcel A. G., prefix=van, useprefix=false},
  date = {2014},
  journaltitle = {Child Development},
  volume = {85},
  number = {3},
  pages = {842--860},
  issn = {1467-8624},
  doi = {10.1111/cdev.12169},
  url = {https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/cdev.12169},
  urldate = {2021-03-01},
  abstract = {Bayesian statistical methods are becoming ever more popular in applied and fundamental research. In this study a gentle introduction to Bayesian analysis is provided. It is shown under what circumstances it is attractive to use Bayesian estimation, and how to interpret properly the results. First, the ingredients underlying Bayesian methods are introduced using a simplified example. Thereafter, the advantages and pitfalls of the specification of prior knowledge are discussed. To illustrate Bayesian methods explained in this study, in a second example a series of studies that examine the theoretical framework of dynamic interactionism are considered. In the Discussion the advantages and disadvantages of using Bayesian statistics are reviewed, and guidelines on how to report on Bayesian statistics are provided.},
  langid = {english},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.12169}
}

@article{shadlenDecisionMakingWindow2013,
  title = {Decision {{Making}} as a {{Window}} on {{Cognition}}},
  author = {Shadlen, Michael N. and Kiani, Roozbeh},
  date = {2013-10-30},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {80},
  number = {3},
  pages = {791--806},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.10.047},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627313009999},
  urldate = {2022-03-28},
  abstract = {A decision is a commitment to a proposition or plan of action based on information and values associated with the possible outcomes. The process operates in a flexible timeframe that is free from the immediacy of evidence acquisition and the real time demands of action itself. Thus, it involves deliberation, planning, and strategizing. This Perspective focuses on perceptual decision making in nonhuman primates and the discovery of neural mechanisms that support accuracy, speed, and confidence in a decision. We suggest that these mechanisms expose principles of cognitive function in general, and we speculate about the challenges and directions before the field.},
  langid = {english}
}

@article{shadlenNeuralBasisPerceptual2001,
  title = {Neural {{Basis}} of a {{Perceptual Decision}} in the {{Parietal Cortex}} ({{Area LIP}}) of the {{Rhesus Monkey}}},
  author = {Shadlen, Michael N. and Newsome, William T.},
  date = {2001-10-01},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  volume = {86},
  number = {4},
  pages = {1916--1936},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.2001.86.4.1916},
  url = {https://www.physiology.org/doi/10.1152/jn.2001.86.4.1916},
  urldate = {2022-03-28},
  abstract = {We recorded the activity of single neurons in the posterior parietal cortex (area LIP) of two rhesus monkeys while they discriminated the direction of motion in random-dot visual stimuli. The visual task was similar to a motion discrimination task that has been used in previous investigations of motion-sensitive regions of the extrastriate cortex. The monkeys were trained to decide whether the direction of motion was toward one of two choice targets that appeared on either side of the random-dot stimulus. At the end of the trial, the monkeys reported their direction judgment by making an eye movement to the appropriate target. We studied neurons in LIP that exhibited spatially selective persistent activity during delayed saccadic eye movement tasks. These neurons are thought to carry high-level signals appropriate for identifying salient visual targets and for guiding saccadic eye movements. We arranged the motion discrimination task so that one of the choice targets was in the LIP neuron's response field (RF) while the other target was positioned well away from the RF. During motion viewing, neurons in LIP altered their firing rate in a manner that predicted the saccadic eye movement that the monkey would make at the end of the trial. The activity thus predicted the monkey's judgment of motion direction. This predictive activity began early in the motion-viewing period and became increasingly reliable as the monkey viewed the random-dot motion. The neural activity predicted the monkey's direction judgment on both easy and difficult trials (strong and weak motion), whether or not the judgment was correct. In addition, the timing and magnitude of the response was affected by the strength of the motion signal in the stimulus. When the direction of motion was toward the RF, stronger motion led to larger neural responses earlier in the motion-viewing period. When motion was away from the RF, stronger motion led to greater suppression of ongoing activity. Thus the activity of single neurons in area LIP reflects both the direction of an impending gaze shift and the quality of the sensory information that instructs such a response. The time course of the neural response suggests that LIP accumulates sensory signals relevant to the selection of a target for an eye movement.},
  langid = {english}
}

@article{shepardMentalRotationThreeDimensional1971a,
  title = {Mental {{Rotation}} of {{Three-Dimensional Objects}}},
  author = {Shepard, Roger N. and Metzler, Jacqueline},
  date = {1971-02-19},
  journaltitle = {Science},
  volume = {171},
  number = {3972},
  eprint = {5540314},
  eprinttype = {pmid},
  pages = {701--703},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.171.3972.701},
  url = {https://science.sciencemag.org/content/171/3972/701},
  urldate = {2021-05-17},
  abstract = {The time required to recognize that two perspective drawings portray objects of the same three-dimensional shape is found to be (i) a linearly increasing function of the angular difference in the portrayed orientations of the two objects and (ii) no shorter for differences corresponding simply to a rigid rotation of one of the two-dimensional drawings in its own picture plane than for differences corresponding to a rotation of the three-dimensional object in depth.},
  langid = {english}
}

@incollection{singmannIntroductionMixedModels2019,
  title = {An {{Introduction}} to {{Mixed Models}} for {{Experimental Psychology}}},
  booktitle = {New {{Methods}} in {{Cognitive Psychology}}},
  author = {Singmann, Henrik and Kellen, David},
  editor = {Spieler, Daniel and Schumacher, Eric},
  date = {2019-10-28},
  edition = {1},
  pages = {4--31},
  publisher = {{Routledge}},
  doi = {10.4324/9780429318405-2},
  url = {https://www.taylorfrancis.com/books/9781000617467/chapters/10.4324/9780429318405-2},
  urldate = {2021-02-18},
  isbn = {978-0-429-31840-5},
  langid = {english}
}

@article{smithIntactImpairedCognitivecontrol2011,
  title = {Intact and Impaired Cognitive-Control Processes in Schizophrenia},
  author = {Smith, Edward E. and Eich, Teal S. and Cebenoyan, Deniz and Malapani, Chariklia},
  date = {2011-03-01},
  journaltitle = {Schizophrenia Research},
  shortjournal = {Schizophrenia Research},
  volume = {126},
  number = {1},
  pages = {132--137},
  issn = {0920-9964},
  doi = {10.1016/j.schres.2010.11.022},
  url = {https://www.sciencedirect.com/science/article/pii/S092099641001666X},
  urldate = {2021-04-12},
  abstract = {Deficits of cognitive-control in schizophrenia have been assumed to result from a single impairment that leads to widespread consequences. Contrary to this view, we hypothesized that different control processes operate at different stages of processing, and that only some of these processes may be impaired. We employed two selection tasks to test the hypothesis that patients with schizophrenia have deficits in selecting information in working memory (WM), but not in selecting perceptual information. In the “Ignore” task, which fosters perceptual selection, participants saw a cue to remember either red or blue words, followed by a memory-set (2 red, 2 blue), a brief delay, and then a probe. The “Suppress” task was similar, except the memory-set came before the instruction-cue, and hence selection had to occur in WM. We recorded reaction time and percentage errors for positive probes (“Valid”), and two kinds of negative probes, those that were supposed to have been dropped from WM (“Lures”) and those that had not appeared in the memory-set (“Controls”). Compared to healthy controls, patients were impaired in the Suppress but not the Ignore task. This dissociation implies that there are two different selection mechanisms.},
  langid = {english},
  keywords = {Cognitive-control,Inhibition,Selection,Suppression,Working-memory}
}

@incollection{smithIntroductionDiffusionModel2015,
  title = {An {{Introduction}} to the {{Diffusion Model}} of {{Decision Making}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Smith, Philip L. and Ratcliff, Roger},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {49--70},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_3},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_3},
  urldate = {2019-03-20},
  abstract = {The diffusion model assumes that two-choice decisions are made by accumulating successive samples of noisy evidence to a response criterion. The model has a pair of criteria that represent the amounts of evidence needed to make each response. The time taken to reach criterion determines the decision time and the criterion that is reached first determines the response. The model predicts choice probabilities and the distributions of response times for correct responses and errors as a function of experimental conditions such as stimulus discriminability, speed-accuracy instructions, and manipulations of relative stimulus frequency, which affect response bias. This chapter describes the main features of the model, including mathematical methods for obtaining response time predictions, methods for fitting it to experimental data, including alternative fitting criteria, and ways to represent the fit to multiple experimental conditions graphically in a compact way. The chapter concludes with a discussion of recent work in psychology that links evidence accumulation to processes of perception, attention, and memory, and in neuroscience, to neural firing rates in the oculomotor control system in monkeys performing saccade-to-target decision tasks.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Choice probability,Decision-making,Diffusion process,Random walk,Response time}
}

@article{smithStochasticDynamicModels2000a,
  title = {Stochastic {{Dynamic Models}} of {{Response Time}} and {{Accuracy}}: {{A Foundational Primer}}},
  shorttitle = {Stochastic {{Dynamic Models}} of {{Response Time}} and {{Accuracy}}},
  author = {Smith, Philip L.},
  date = {2000-09-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {44},
  number = {3},
  pages = {408--463},
  issn = {0022-2496},
  doi = {10.1006/jmps.1999.1260},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249699912609},
  urldate = {2022-05-03},
  abstract = {A large class of statistical decision models for performance in simple information processing tasks can be described by linear, first-order, stochastic differential equations (SDEs), whose solutions are diffusion processes. In such models, the first passage time for the diffusion process through a response criterion determines the time at which an observer makes a decision about the identity of a stimulus. Because the assumptions of many cognitive models lead to SDEs that are time inhomogeneous, classical methods for solving such first passage time problems are usually inapplicable. In contrast, recent integral equation methods often yield solutions to both the one-sided and the two-sided first passage time problems, even in the presence of time inhomogeneity. These methods, which are of particular relevance to the cognitive modeler, are described in detail, together with illustrative applications.},
  langid = {english}
}

@report{sohnMetamersBayesianComputation2020,
  type = {preprint},
  title = {Metamers of {{Bayesian}} Computation},
  author = {Sohn, Hansem and Jazayeri, Mehrdad},
  date = {2020-08-12},
  institution = {{Neuroscience}},
  doi = {10.1101/2020.08.11.246355},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.08.11.246355},
  urldate = {2021-03-22},
  abstract = {There are two sharply debated views on how humans make decisions under uncertainty. Bayesian decision theory posits that humans optimize their behavior by establishing and integrating internal models of past sensory experiences (priors) and decision outcomes (cost functions). An alternative model-free hypothesis posits that decisions are optimized through trial and error without explicit internal models for priors and cost functions. To distinguish between these possibilities, we introduce a novel paradigm that probes sensitivity of humans to transitions between prior-cost pairs that demand the same optimal policy (metamers) but distinct internal models. We demonstrate the utility of our approach in two experiments that were classically explained by model-based Bayesian theory. Our approach validates the modelbased strategy in an interval timing task but not in a visuomotor rotation task. More generally, our work provides a domain-general approach for testing the circumstances under which humans implement model-based Bayesian computations.},
  langid = {english}
}

@article{speckmanDeltaPlotsCoherent2008a,
  title = {Delta {{Plots}} and {{Coherent Distribution Ordering}}},
  author = {Speckman, Paul L and Rouder, Jeffrey N and Morey, Richard D and Pratte, Michael S},
  date = {2008-08},
  journaltitle = {The American Statistician},
  volume = {62},
  number = {3},
  pages = {262--266},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/000313008X333493},
  url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X333493},
  urldate = {2020-11-05},
  langid = {english}
}

@article{speckmanDeltaPlotsCoherent2008b,
  title = {Delta {{Plots}} and {{Coherent Distribution Ordering}}},
  author = {Speckman, Paul L and Rouder, Jeffrey N and Morey, Richard D and Pratte, Michael S},
  date = {2008-08},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {62},
  number = {3},
  pages = {262--266},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/000313008X333493},
  url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X333493},
  urldate = {2021-05-10},
  langid = {english}
}

@incollection{spragueUsingHumanNeuroimaging2015,
  title = {Using {{Human Neuroimaging}} to {{Examine Top-down Modulation}} of {{Visual Perception}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Sprague, Thomas C. and Serences, John T.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {245--274},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_12},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_12},
  urldate = {2019-03-20},
  abstract = {Both univariate and multivariate analysis methods largely have focused on characterizing how measurements from neural firing rates, EEG electrodes, or fMRI voxels change as a function of stimulus parameters or task demands –they focus on characterizing changes in neural signals. However, in cognitive neuroscience we are often interested in how these changes in neural signals collectively modify representations of information. We compare methods whereby activation patterns across entire brain regions can be used to reconstruct representations of information to more traditional univariate and multivariate analysis approaches. We highlight findings using these methods, focusing on how a representation-based analysis approach yields novel insights into how information is encoded, maintained and manipulated under various task demands.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Analysis,Attention,Decoding,EEG,Encoding,fMRI,Neuroimaging,Reconstruction,Vision,Working memory}
}

@incollection{stuphornIntroductionNeuroscientificMethods2015,
  title = {An {{Introduction}} to {{Neuroscientific Methods}}: {{Single-cell Recordings}}},
  shorttitle = {An {{Introduction}} to {{Neuroscientific Methods}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Stuphorn, Veit and Chen, Xiaomo},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {113--137},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_6},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_6},
  urldate = {2019-03-20},
  abstract = {This chapter describes the role of single-cell recordings in understanding the mechanisms underlying human cognition. Cognition is a function of the brain, a complex computational network, whose most elementary nodes are made up out of individual neurons. These neurons encode information and influence each other through a dynamically changing pattern of action potentials. For this reason, the activity of neurons in the awake, behaving brain constitutes the most fundamental form of neural data for cognitive neuroscience. This chapter discusses a number of technical issues and challenges of single-cell neurophysiology using a recent project of the authors as an example. We discuss issues such as the choice of an appropriate animal model, the role of psychophysics, technical challenges surrounding the simultaneous recording of multiple neurons, and various methods for perturbation experiments. The chapter closes with a consideration of the challenge that the brain’s complexity poses for fully understanding any realistic nervous circuit, and of the importance of conceptual insights and mathematical models in the interpretation of single-cell recordings.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Action potentials,Animal models,Behavior,Decision making,Electrophysiological recording,Frontal cortex,Nervous circuit,Perturbation experiment,Primate}
}

@article{tammReactionTimeVariability2012,
  title = {Reaction {{Time Variability}} in {{ADHD}}: {{A Review}}},
  shorttitle = {Reaction {{Time Variability}} in {{ADHD}}},
  author = {Tamm, Leanne and Narad, Megan E. and Antonini, Tanya N. and O’Brien, Kathleen M. and Hawk, Larry W. and Epstein, Jeffery N.},
  date = {2012-07-01},
  journaltitle = {Neurotherapeutics},
  shortjournal = {Neurotherapeutics},
  volume = {9},
  number = {3},
  pages = {500--508},
  issn = {1878-7479},
  doi = {10.1007/s13311-012-0138-5},
  url = {https://doi.org/10.1007/s13311-012-0138-5},
  urldate = {2022-04-11},
  abstract = {For the past decade, intra-individual variability in reaction times on computerized tasks has become a central focus of cognitive research on Attention-Deficit/Hyperactivity Disorder (ADHD). Numerous studies document increased reaction time variability among children and adults with ADHD, relative to typically developing controls. However, direct comparisons with other disorders with heightened reaction time variability are virtually nonexistent, despite their potential to inform our understanding of the phenomenon. A growing literature examines the sensitivity of reaction time variability to theoretically and clinically relevant manipulations. There is strong evidence that stimulus treatment reduces reaction time variability during a range of cognitive tasks, but the literature is mixed regarding the impact of motivational incentives and variation in stimulus event rate. Most studies of reaction time variability implicitly assume that heightened reaction time variability reflects occasional lapses in attention, and the dominant neurophysiological interpretation suggests this variability is linked to intrusions of task-negative brain network activity during task performance. Work examining the behavioral and neurophysiological correlates of reaction time variability provides some support for these hypotheses, but considerably more work is needed in this area. Finally, because conclusions from each of domains reviewed are limited by the wide range of measures used to measure reaction time variability, this review highlights the need for increased attention to the cognitive and motivational context in which variability is assessed and recommends that future work always supplement macro-level variability indices with metrics that isolate particular components of reaction time variability.},
  langid = {english}
}

@article{tomasinoEffectsStimulusType2016,
  title = {Effects of {{Stimulus Type}} and {{Strategy}} on {{Mental Rotation Network}}: {{An Activation Likelihood Estimation Meta-Analysis}}},
  shorttitle = {Effects of {{Stimulus Type}} and {{Strategy}} on {{Mental Rotation Network}}},
  author = {Tomasino, Barbara and Gremese, Michele},
  date = {2016},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {9},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2015.00693},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2015.00693/full},
  urldate = {2021-05-17},
  abstract = {We could predict how an object would look like if we were to see it from different viewpoints. The brain network governing mental rotation (MR) has been studied using a variety of stimuli and tasks instructions. By using activation likelihood estimation (ALE) meta-analysis we tested whether different MR networks can be modulated by the type of stimulus (body vs. non body parts) or by the type of tasks instructions (motor imagery-based vs. non-motor imagery-based MR instructions). Testing for the bodily and non-bodily stimulus axis revealed a bilateral sensorimotor activation for bodily-related as compared to non bodily-related stimuli and a posterior right lateralized activation for non bodily-related as compared to bodily-related stimuli. A top-down modulation of the network was exerted by the MR tasks instructions frame with a bilateral (preferentially sensorimotor left) network for motor imagery- vs. non-motor imagery-based MR instructions and the latter activating a preferentially posterior right occipito-temporal-parietal network. The present quantitative meta-analysis summarizes and amends previous descriptions of the brain network related to MR and shows how it is modulated by top-down and bottom-up experimental factors.},
  langid = {english},
  keywords = {ALE meta-analysis,Cognitive Strategies,fMRI,Mental Imagery,mental rotation}
}

@incollection{turnerConstrainingCognitiveAbstractions2015,
  title = {Constraining {{Cognitive Abstractions Through Bayesian Modeling}}},
  booktitle = {An {{Introduction}} to {{Model-Based Cognitive Neuroscience}}},
  author = {Turner, Brandon M.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {199--220},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_10},
  url = {https://doi.org/10.1007/978-1-4939-2236-9_10},
  urldate = {2019-03-20},
  abstract = {There are many ways to combine neural and behavioral measures to study cognition. Some ways are theoretical, and other ways are statistical. The predominant statistical approach treats both sources of data as independent and the relationship between the two measures is inferred by way of a (post hoc) regression analysis. In this chapter, we review an alternative approach that allows for flexible modeling of both measures simultaneously. We then explore and elaborate on several of the most important benefits of this modeling approach, and close with a model comparison of the Linear Ballistic Accumulator model and a drift diffusion model on neural and behavioral data.},
  isbn = {978-1-4939-2236-9},
  langid = {english},
  keywords = {Bayesian,cognitive modeling,hierarchical,joint modeling framework}
}

@article{vanceRightParietalDysfunction2007,
  title = {Right Parietal Dysfunction in Children with Attention Deficit Hyperactivity Disorder, Combined Type: A Functional {{MRI}} Study},
  shorttitle = {Right Parietal Dysfunction in Children with Attention Deficit Hyperactivity Disorder, Combined Type},
  author = {Vance, A and Silk, T J and Casey, M and Rinehart, N J and Bradshaw, J L and Bellgrove, M A and Cunnington, R},
  date = {2007-09},
  journaltitle = {Molecular Psychiatry},
  shortjournal = {Mol Psychiatry},
  volume = {12},
  number = {9},
  pages = {826--832},
  issn = {1359-4184, 1476-5578},
  doi = {10.1038/sj.mp.4001999},
  url = {http://www.nature.com/articles/4001999},
  urldate = {2021-03-16},
  langid = {english}
}

@report{vandenberghTutorialConductingInterpreting2019,
  type = {preprint},
  title = {A {{Tutorial}} on {{Conducting}} and {{Interpreting}} a {{Bayesian ANOVA}} in {{JASP}}},
  author = {family=Bergh, given=Don, prefix=van den, useprefix=true and family=Doorn, given=Johnny, prefix=van, useprefix=true and Marsman, Maarten and Draws, Tim and family=Kesteren, given=Erik-Jan, prefix=van, useprefix=true and Derks, Koen and Dablander, Fabian and Gronau, Quentin Frederik and Kucharský, Šimon and Raj, Akash and Sarafoglou, Alexandra and Voelkel, Jan G. and Stefan, Angelika and Ly, Alexander and Hinne, Max and Matzke, Dora and Wagenmakers, Eric-Jan},
  date = {2019-11-11},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/spreb},
  url = {https://osf.io/spreb},
  urldate = {2020-05-29},
  abstract = {Analysis of variance (ANOVA) is the standard procedure for statistical inference in factorial designs. Typically, ANOVAs are executed using frequentist statistics, where p-values determine statistical significance in an all-or-none fashion. In recent years, the Bayesian approach to statistics is increasingly viewed as a legitimate alternative to the p-value. However, the broad adoption of Bayesian statistics –and Bayesian ANOVA in particular– is frustrated by the fact that Bayesian concepts are rarely taught in applied statistics courses. Consequently, practitioners may be unsure how to conduct a Bayesian ANOVA and interpret the results. Herewe provide a guide for executing and interpreting a  Bayesian ANOVA with JASP, an open-source statistical software program with a graphical user interface. We explain the key concepts of the Bayesian ANOVA using twoempirical examples.}
}

@article{vandeschootBayesianStatisticsModelling2021,
  title = {Bayesian Statistics and Modelling},
  author = {family=Schoot, given=Rens, prefix=van de, useprefix=true and Depaoli, Sarah and King, Ruth and Kramer, Bianca and Märtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher},
  date = {2021-12},
  journaltitle = {Nature Reviews Methods Primers},
  shortjournal = {Nat Rev Methods Primers},
  volume = {1},
  number = {1},
  pages = {1},
  issn = {2662-8449},
  doi = {10.1038/s43586-020-00001-2},
  url = {http://www.nature.com/articles/s43586-020-00001-2},
  urldate = {2021-04-16},
  abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.},
  langid = {english}
}

@article{vanrooijFormalizingVerbalTheories2020,
  title = {Formalizing {{Verbal Theories}}},
  author = {family=Rooij, given=Iris, prefix=van, useprefix=true and Blokpoel, Mark},
  date = {2020-09-01},
  journaltitle = {Social Psychology},
  volume = {51},
  number = {5},
  pages = {285--298},
  publisher = {{Hogrefe Publishing}},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000428},
  url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000428},
  urldate = {2020-11-01},
  abstract = {. We present a tutorial for formalizing verbal theories of psychological phenomena           – social or otherwise. The approach builds on concepts and tools from the mathematics of computation.           We use intuitive examples and illustrate the intrinsic dialectical nature of the formalization process by           presenting dialogues between two fictive characters, called Verbal and             Formal. These characters’ conversations and thought experiments serve to highlight           important lessons in theoretical modeling.}
}

@article{verstynenOrganizationDynamicsCorticostriatal2014a,
  title = {The Organization and Dynamics of Corticostriatal Pathways Link the Medial Orbitofrontal Cortex to Future Behavioral Responses},
  author = {Verstynen, Timothy D.},
  date = {2014-11-15},
  journaltitle = {Journal of Neurophysiology},
  volume = {112},
  number = {10},
  pages = {2457--2469},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00221.2014},
  url = {https://journals.physiology.org/doi/full/10.1152/jn.00221.2014},
  urldate = {2022-04-13},
  abstract = {Accurately making a decision in the face of incongruent options increases the efficiency of making similar congruency decisions in the future. Contextual factors like reward can modulate this adaptive process, suggesting that networks associated with monitoring previous success and failure outcomes might contribute to this form of behavioral updating. To evaluate this possibility, a group of healthy adults (n = 30) were tested with functional MRI (fMRI) while they performed a color-word Stroop task. In a conflict-related region of the medial orbitofrontal cortex (mOFC), stronger BOLD responses predicted faster response times (RTs) on the next trial. More importantly, the degree of behavioral adaptation of RTs was correlated with the magnitude of mOFC-RT associations on the previous trial, but only after accounting for network-level interactions with prefrontal and striatal regions. This suggests that congruency sequencing effects may rely on interactions between distributed corticostriatal circuits. This possibility was evaluated by measuring the convergence of white matter projections from frontal areas into the striatum with diffusion-weighted imaging. In these pathways, greater convergence of corticostriatal projections correlated with stronger functional mOFC-RT associations that, in turn, provided an indirect pathway linking anatomical structure to behavior. Thus distributed corticostriatal processing may mediate the orbitofrontal cortex's influence on behavioral updating, even in the absence of explicit rewards.},
  keywords = {adaptation,congruency,corticostriatal processing,diffusion-weighted imaging,fMRI}
}

@article{vilaresDifferentialRepresentationsPrior2012,
  title = {Differential {{Representations}} of {{Prior}} and {{Likelihood Uncertainty}} in the {{Human Brain}}},
  author = {Vilares, Iris and Howard, James D. and Fernandes, Hugo L. and Gottfried, Jay A. and Kording, Konrad P.},
  date = {2012-09-25},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {22},
  number = {18},
  eprint = {22840519},
  eprinttype = {pmid},
  pages = {1641--1648},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2012.07.010},
  url = {https://www.cell.com/current-biology/abstract/S0960-9822(12)00801-9},
  urldate = {2022-02-17},
  langid = {english}
}

@article{vincentTutorialBayesianModels2015a,
  title = {A Tutorial on {{Bayesian}} Models of Perception},
  author = {Vincent, Benjamin T.},
  date = {2015-06-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {66},
  pages = {103--114},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2015.02.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249615000061},
  urldate = {2022-04-27},
  abstract = {The notion that perception involves Bayesian inference is an increasingly popular position taken by many researchers. Bayesian models have provided insights into many perceptual phenomena, but their description and practical implementation does not always convey their theoretical appeal or conceptual elegance. This tutorial provides an introduction to core concepts in Bayesian modelling and should help a wide variety of readers to more deeply understand, or to generate their own Bayesian models of perception. Core theoretical and implementational issues are covered, using the 2 alternative-forced-choice task as a case study. Supplementary code is available to help bridge the gap between model description and practical implementation (see Appendix B).},
  langid = {english},
  keywords = {Alternative forced choice,Bayesian inference,Bayesian network,Ideal observer,MCMC,Probabilistic generative model,Psychometric function}
}

@article{vindingVolitionProspectiveMemory2018,
  title = {Volition in {{Prospective Memory}}: Evidence against Differences in Recalling Free and Fixed Delayed Intentions},
  shorttitle = {Volition in {{Prospective Memory}}},
  author = {Vinding, Mikkel C. and Lindeløv, Jonas Kristoffer and Xiao, Yahui and Chan, Raymond C. K. and Sørensen, Thomas Alrik},
  date = {2018-12-11T14:30:44},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/hsrbt},
  url = {https://psyarxiv.com/hsrbt/},
  urldate = {2021-05-25},
  abstract = {Human volition can be defined as the extent to which actions are generated by internal states or as a response to externally dictated instructions. Whether actions are voluntary or not influence the cognitive process of action generation and perception of action. The influence of volition on deciding actions at a later point in time is a less explored dimension. A voluntary decision on future action requires that the intention must be stored in the prospective memory until the intended action is performed. It is unknown if the distinction between freely chosen actions and externally dictated actions has a cognitive relevance for delayed intentions. In the present study, we compare the difference between voluntarily formed intentions and intentions fixed by external instructions in a prospective memory task. In the task, participants either freely chose a cue or were given a fixed cue by the task instructions which they had to store in memory and recalled when the cue was encountered during an ongoing filler task. We examined if there would be a difference between the free and fixed delayed intention on retrieval of the delayed intention by modelling the task performance and reaction time using a Bayesian hierarchical drift-diffusion model. We then compared if there were differences in diffusion rate, decision threshold, bias, and non-decision time between free and fixed intentions in the prospective memory task, which would signify that free and fixed delayed intentions differentially engage prospective memory. Comparison of the estimated model parameters for the free and fixed intentions showed evidence against differences between free and fixed conditions in the prospective memory task. The results suggest that once the intention is encoded in memory, it no longer makes a cognitive difference at retrieval if it was initially formed freely or was fixed.},
  keywords = {Cognitive Psychology,Consciousness,Drift-diffusion model,Intention,Judgment and Decision Making,Memory,Prospective memory,Social and Behavioral Sciences,Volition}
}

@article{vossDiffusionModelsExperimental2013,
  title = {Diffusion {{Models}} in {{Experimental Psychology}}: {{A Practical Introduction}}},
  shorttitle = {Diffusion {{Models}} in {{Experimental Psychology}}},
  author = {Voss, Andreas and Nagler, Markus and Lerche, Veronika},
  date = {2013-01-01},
  journaltitle = {Experimental Psychology},
  volume = {60},
  number = {6},
  pages = {385--402},
  issn = {1618-3169, 2190-5142},
  doi = {10.1027/1618-3169/a000218},
  url = {https://econtent.hogrefe.com/doi/10.1027/1618-3169/a000218},
  urldate = {2020-11-13},
  abstract = {Stochastic diffusion models (Ratcliff, 1978) can be used to analyze response time data from binary decision tasks. They provide detailed information about cognitive processes underlying the performance in such tasks. Most importantly, different parameters are estimated from the response time distributions of correct responses and errors that map (1) the speed of information uptake, (2) the amount of information used to make a decision, (3) possible decision biases, and (4) the duration of nondecisional processes. Although this kind of model can be applied to many experimental paradigms and provides much more insight than the analysis of mean response times can, it is still rarely used in cognitive psychology. In the present paper, we provide comprehensive information on the theory of the diffusion model, as well as on practical issues that have to be considered for implementing the model.},
  langid = {english}
}

@article{vossInterpretingParametersDiffusion2004b,
  title = {Interpreting the Parameters of the Diffusion Model: {{An}} Empirical Validation},
  shorttitle = {Interpreting the Parameters of the Diffusion Model},
  author = {Voss, Andreas and Rothermund, Klaus and Voss, Jochen},
  date = {2004-10},
  journaltitle = {Memory \& Cognition},
  shortjournal = {Memory \& Cognition},
  volume = {32},
  number = {7},
  pages = {1206--1220},
  issn = {0090-502X, 1532-5946},
  doi = {10.3758/BF03196893},
  url = {http://link.springer.com/10.3758/BF03196893},
  urldate = {2022-05-02},
  langid = {english}
}

@article{wagenmakersBayesianBenefitsPragmatic2016,
  title = {Bayesian {{Benefits}} for the {{Pragmatic Researcher}}},
  author = {Wagenmakers, Eric-Jan and Morey, Richard D. and Lee, Michael D.},
  date = {2016-06},
  journaltitle = {Current Directions in Psychological Science},
  volume = {25},
  number = {3},
  pages = {169--176},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/0963721416643289},
  url = {http://journals.sagepub.com/doi/10.1177/0963721416643289},
  urldate = {2019-04-23},
  abstract = {The practical advantages of Bayesian inference are demonstrated here through two concrete examples. In the first example, we wish to learn about a criminal’s IQ: a problem of parameter estimation. In the second example, we wish to quantify and track support in favor of the null hypothesis that Adam Sandler movies are profitable regardless of their quality: a problem of hypothesis testing. The Bayesian approach unifies both problems within a coherent predictive framework, in which parameters and models that predict the data successfully receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more informative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology.},
  langid = {english}
}

@article{wagenmakersBayesianHypothesisTesting2010a,
  title = {Bayesian Hypothesis Testing for Psychologists: {{A}} Tutorial on the {{Savage}}–{{Dickey}} Method},
  shorttitle = {Bayesian Hypothesis Testing for Psychologists},
  author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
  date = {2010-05-01},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {60},
  number = {3},
  pages = {158--189},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2009.12.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0010028509000826},
  urldate = {2021-03-29},
  abstract = {In the field of cognitive psychology, the p-value hypothesis test has established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage–Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method’s validity, generality, and flexibility.},
  langid = {english},
  keywords = {Bayes factor,Hierarchical modeling,Model selection,Order-restrictions,Random effects,Statistical evidence}
}

@article{wagenmakersBayesianInferencePsychology2018b,
  title = {Bayesian Inference for Psychology. {{Part II}}: {{Example}} Applications with {{JASP}}},
  shorttitle = {Bayesian Inference for Psychology. {{Part II}}},
  author = {Wagenmakers, Eric-Jan and Love, Jonathon and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Selker, Ravi and Gronau, Quentin F. and Dropmann, Damian and Boutin, Bruno and Meerhoff, Frans and Knight, Patrick and Raj, Akash and family=Kesteren, given=Erik-Jan, prefix=van, useprefix=true and family=Doorn, given=Johnny, prefix=van, useprefix=true and Šmíra, Martin and Epskamp, Sacha and Etz, Alexander and Matzke, Dora and family=Jong, given=Tim, prefix=de, useprefix=true and family=Bergh, given=Don, prefix=van den, useprefix=true and Sarafoglou, Alexandra and Steingroever, Helen and Derks, Koen and Rouder, Jeffrey N. and Morey, Richard D.},
  date = {2018-02},
  journaltitle = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {58--76},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1323-7},
  url = {http://link.springer.com/10.3758/s13423-017-1323-7},
  urldate = {2019-04-23},
  langid = {english}
}

@article{wagenmakersLinearRelationMean2007,
  title = {On the Linear Relation between the Mean and the Standard Deviation of a Response Time Distribution.},
  author = {Wagenmakers, Eric-Jan and Brown, Scott},
  date = {2007},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {114},
  number = {3},
  pages = {830--841},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.114.3.830},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.114.3.830},
  urldate = {2021-05-10},
  abstract = {Although it is generally accepted that the spread of a response time (RT) distribution increases with the mean, the precise nature of this relation remains relatively unexplored. The authors show that in several descriptive RT distributions, the standard deviation increases linearly with the mean. Results from a wide range of tasks from different experimental paradigms support a linear relation between RT mean and RT standard deviation. Both R. Ratcliff’s (1978) diffusion model and G. D. Logan’s (1988) instance theory of automatization provide explanations for this linear relation. The authors identify and discuss 3 specific boundary conditions for the linear law to hold. The law constrains RT models and supports the use of the coefficient of variation to (a) compare variability while controlling for differences in baseline speed of processing and (b) assess whether changes in performance with practice are due to quantitative speedup or qualitative reorganization.},
  langid = {english}
}

@article{wagenmakersPracticalSolutionPervasive2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Wagenmakers, Eric-Jan},
  date = {2007-10},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03194105},
  url = {http://link.springer.com/10.3758/BF03194105},
  urldate = {2021-02-24},
  langid = {english}
}

@article{wahnPupilSizesScale2016,
  title = {Pupil {{Sizes Scale}} with {{Attentional Load}} and {{Task Experience}} in a {{Multiple Object Tracking Task}}},
  author = {Wahn, Basil and Ferris, Daniel P. and Hairston, W. David and König, Peter},
  date = {2016-12-15},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {11},
  number = {12},
  pages = {e0168087},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0168087},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168087},
  urldate = {2021-05-03},
  abstract = {Previous studies have related changes in attentional load to pupil size modulations. However, studies relating changes in attentional load and task experience on a finer scale to pupil size modulations are scarce. Here, we investigated how these changes affect pupil sizes. To manipulate attentional load, participants covertly tracked between zero and five objects among several randomly moving objects on a computer screen. To investigate effects of task experience, the experiment was conducted on three consecutive days. We found that pupil sizes increased with each increment in attentional load. Across days, we found systematic pupil size reductions. We compared the model fit for predicting pupil size modulations using attentional load, task experience, and task performance as predictors. We found that a model which included attentional load and task experience as predictors had the best model fit while adding performance as a predictor to this model reduced the overall model fit. Overall, results suggest that pupillometry provides a viable metric for precisely assessing attentional load and task experience in visuospatial tasks.},
  langid = {english},
  keywords = {Animal performance,Attention,Eyes,Locus coeruleus,Pupil,Reflexes,Target detection,Vision}
}

@article{wassersteinASAStatementPValues2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  date = {2016-04-02},
  journaltitle = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  url = {https://doi.org/10.1080/00031305.2016.1154108},
  urldate = {2021-03-01},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2016.1154108}
}

@article{whelanEffectiveAnalysisReaction2008,
  title = {Effective {{Analysis}} of {{Reaction Time Data}}},
  author = {Whelan, Robert},
  date = {2008-07},
  journaltitle = {The Psychological Record},
  shortjournal = {Psychol Rec},
  volume = {58},
  number = {3},
  pages = {475--482},
  issn = {0033-2933, 2163-3452},
  doi = {10.1007/BF03395630},
  url = {http://link.springer.com/10.1007/BF03395630},
  urldate = {2022-04-04},
  langid = {english}
}

@article{wilsonTenSimpleRules2019,
  title = {Ten Simple Rules for the Computational Modeling of Behavioral Data},
  author = {Wilson, Robert C and Collins, Anne GE},
  editor = {Behrens, Timothy E},
  date = {2019-11-26},
  journaltitle = {eLife},
  volume = {8},
  pages = {e49547},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.49547},
  url = {https://doi.org/10.7554/eLife.49547},
  urldate = {2021-02-04},
  abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
  keywords = {computational modeling,model fitting,reproducibility,validation}
}

@article{wilsonTenSimpleRules2019a,
  title = {Ten Simple Rules for the Computational Modeling of Behavioral Data},
  author = {Wilson, Robert C and Collins, Anne GE},
  editor = {Behrens, Timothy E},
  date = {2019-11-26},
  journaltitle = {eLife},
  volume = {8},
  pages = {e49547},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.49547},
  url = {https://doi.org/10.7554/eLife.49547},
  urldate = {2021-02-23},
  abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
  keywords = {computational modeling,model fitting,reproducibility,validation}
}

@article{wittSignalDetectionMeasures2015,
  title = {Signal {{Detection Measures Cannot Distinguish Perceptual Biases}} from {{Response Biases}}},
  author = {Witt, Jessica K and Taylor, J Eric T and Sugovic, Mila and Wixted, John T},
  date = {2015-03-01},
  journaltitle = {Perception},
  shortjournal = {Perception},
  volume = {44},
  number = {3},
  pages = {289--300},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0301-0066},
  doi = {10.1068/p7908},
  url = {https://doi.org/10.1068/p7908},
  urldate = {2022-03-28},
  abstract = {A common conceptualization of signal detection theory (SDT) holds that if the effect of an experimental manipulation is truly perceptual, then it will necessarily be reflected in a change in d' rather than a change in the measure of response bias. Thus, if an experimental manipulation affects the measure of bias, but not d', then it is safe to conclude that the manipulation in question did not affect perception but instead affected the placement of the internal decision criterion. However, the opposite may be true: an effect on perception may affect measured bias while having no effect on d'. To illustrate this point, we expound how signal detection measures are calculated and show how all biases—including perceptual biases—can exert their effects on the criterion measure rather than on d'. While d' can provide evidence for a perceptual effect, an effect solely on the criterion measure can also arise from a perceptual effect. We further support this conclusion using simulations to demonstrate that the Müller-Lyer illusion, which is a classic visual illusion that creates a powerful perceptual effect on the apparent length of a line, influences the criterion measure without influencing d'. For discrimination experiments, SDT is effective at discriminating between sensitivity and bias but cannot by itself determine the underlying source of the bias, be it perceptual or response based.},
  langid = {english},
  keywords = {Müller-Lyer illusion,perceptual biases,response biases,signal detection theory}
}

@article{wylieUsingSignalDetection2020,
  title = {Using {{Signal Detection Theory}} to {{Better Understand Cognitive Fatigue}}},
  author = {Wylie, Glenn R. and Yao, Bing and Sandry, Joshua and DeLuca, John},
  date = {2020},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {11},
  eprint = {33519595},
  eprinttype = {pmid},
  pages = {579188},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.579188},
  abstract = {When we are fatigued, we feel that our performance is worse than when we are fresh. Yet, for over 100 years, researchers have been unable to identify an objective, behavioral measure that covaries with the subjective experience of fatigue. Previous work suggests that the metrics of signal detection theory (SDT)-response bias (criterion) and perceptual certainty (d')-may change as a function of fatigue, but no work has yet been done to examine whether these metrics covary with fatigue. Here, we investigated cognitive fatigue using SDT. We induced fatigue through repetitive performance of the n-back working memory task, while functional magnetic resonance imaging (fMRI) data was acquired. We also assessed cognitive fatigue at intervals throughout. This enabled us to assess not only whether criterion and d' covary with cognitive fatigue but also whether similar patterns of brain activation underlie cognitive fatigue and SDT measures. Our results show that both criterion and d' were correlated with changes in cognitive fatigue: as fatigue increased, subjects became more conservative in their response bias and their perceptual certainty declined. Furthermore, activation in the striatum of the basal ganglia was also related to cognitive fatigue, criterion, and d'. These results suggest that SDT measures represent an objective measure of cognitive fatigue. Additionally, the overlap and difference in the fMRI results between cognitive fatigue and SDT measures indicate that these measures are related while also separate. In sum, we show the relevance of SDT measures in the understanding of fatigue, thus providing researchers with a new set of tools with which to better understand the nature and consequences of cognitive fatigue.},
  langid = {english},
  pmcid = {PMC7844088},
  keywords = {cognitive fatigue,fMRI,signal detection theory,striatum,working memory}
}

@article{wylieUsingSignalDetection2021,
  title = {Using {{Signal Detection Theory}} to {{Better Understand Cognitive Fatigue}}},
  author = {Wylie, Glenn R. and Yao, Bing and Sandry, Joshua and DeLuca, John},
  date = {2021},
  journaltitle = {Frontiers in Psychology},
  volume = {11},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2020.579188},
  urldate = {2022-02-15},
  abstract = {When we are fatigued, we feel that our performance is worse than when we are fresh. Yet, for over 100 years, researchers have been unable to identify an objective, behavioral measure that covaries with the subjective experience of fatigue. Previous work suggests that the metrics of signal detection theory (SDT)—response bias (criterion) and perceptual certainty (d’)—may change as a function of fatigue, but no work has yet been done to examine whether these metrics covary with fatigue. Here, we investigated cognitive fatigue using SDT. We induced fatigue through repetitive performance of the n-back working memory task, while functional magnetic resonance imaging (fMRI) data was acquired. We also assessed cognitive fatigue at intervals throughout. This enabled us to assess not only whether criterion and d’ covary with cognitive fatigue but also whether similar patterns of brain activation underlie cognitive fatigue and SDT measures. Our results show that both criterion and d’ were correlated with changes in cognitive fatigue: as fatigue increased, subjects became more conservative in their response bias and their perceptual certainty declined. Furthermore, activation in the striatum of the basal ganglia was also related to cognitive fatigue, criterion, and d’. These results suggest that SDT measures represent an objective measure of cognitive fatigue. Additionally, the overlap and difference in the fMRI results between cognitive fatigue and SDT measures indicate that these measures are related while also separate. In sum, we show the relevance of SDT measures in the understanding of fatigue, thus providing researchers with a new set of tools with which to better understand the nature and consequences of cognitive fatigue.}
}


