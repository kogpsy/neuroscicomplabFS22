<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.301">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Andrew Ellis">
<meta name="dcterms.date" content="2022-05-10">
<meta name="description" content="Combining informatin from multiple participants | Intro to Bayesian data anlysis.">
<title>Neurowissenschaft Computerlab - Data analysis: I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>
<script src="../../site_libs/quarto-nav/quarto-nav.js"></script><script src="../../site_libs/quarto-nav/headroom.min.js"></script><script src="../../site_libs/clipboard/clipboard.min.js"></script><meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script><script src="../../site_libs/quarto-search/fuse.min.js"></script><script src="../../site_libs/quarto-search/quarto-search.js"></script><link href="../..//assets/logo_landing.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script><script src="../../site_libs/quarto-html/popper.min.js"></script><script src="../../site_libs/quarto-html/tippy.umd.min.js"></script><script src="../../site_libs/quarto-html/anchor.min.js"></script><link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link class="quarto-color-scheme" id="quarto-text-highlighting-styles" href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<link class="quarto-color-scheme quarto-color-alternate" rel="prefetch" id="quarto-text-highlighting-styles" href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script><link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link class="quarto-color-scheme" href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<link class="quarto-color-scheme quarto-color-alternate" rel="prefetch" href="../../site_libs/bootstrap/bootstrap-dark.min.css">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="../../styles/global.css">
</head>
<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg navbar-dark "><div class="navbar-container container-fluid">
      <a class="navbar-brand" href="../../index.html">
    <img src="../../assets/logo.png" alt=""><span class="navbar-title">Neurowissenschaft Computerlab</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
<li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-admin" role="button" data-bs-toggle="dropdown" aria-expanded="false">Admin</a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-admin">
<li>
    <a class="dropdown-item" href="../../pages/admin/01_overview.html">
 <span class="dropdown-text">Übersicht</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/admin/02_assessments.html">
 <span class="dropdown-text">Leistungskontrollen</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/admin/03_zulip_forum.html">
 <span class="dropdown-text">Zulip Forum</span></a>
  </li>  
    </ul>
</li>
</ul>
<ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-kapitel" role="button" data-bs-toggle="dropdown" aria-expanded="false">Kapitel</a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-kapitel">
<li>
    <a class="dropdown-item" href="../../pages/chapters/01_psychopy_experiments.html">
 <span class="dropdown-text">Verhaltensexperiment mit PsychoPy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/02_importing_data.html">
 <span class="dropdown-text">Daten importieren</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/03_data_cleaning.html">
 <span class="dropdown-text">Data cleaning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/04_summarizing_data.html">
 <span class="dropdown-text">Daten bearbeiten und zusammenfassen</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/05_signal_detection_i.html">
 <span class="dropdown-text">Signal Detection Theory: I</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/06_signal_detection_ii.html">
 <span class="dropdown-text">Signal Detection Theory: II</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/07_response_times_i.html">
 <span class="dropdown-text">Reaktionszeiten: I</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/08_response_times_ii.html">
 <span class="dropdown-text">Reaktionszeiten: II</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/09_evidence_accumulation_1.html">
 <span class="dropdown-text">Evidence Accumulation Models: I</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/10_evidence_accumulation_2.html">
 <span class="dropdown-text">Evidence Accumulation Models: II</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/chapters/11_data-analysis-1.html">
 <span class="dropdown-text">Data analysis: I</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-slides" role="button" data-bs-toggle="dropdown" aria-expanded="false">Slides</a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-slides">
<li>
    <a class="dropdown-item" href="../../slides/01_introduction.html">
 <span class="dropdown-text">1. Sitzung (22.02.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/02_psychopy.html">
 <span class="dropdown-text">2. Sitzung (01.03.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/03_import_and_process_data.html">
 <span class="dropdown-text">3. Sitzung (08.03.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/04_process_data.html">
 <span class="dropdown-text">4. Sitzung (15.03.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/05_signal_detection.html">
 <span class="dropdown-text">5. Sitzung (22.03.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/06_signal_detection_examples.html">
 <span class="dropdown-text">6. Sitzung (29.03.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/07_response_times.html">
 <span class="dropdown-text">7. Sitzung (05.04.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/08_response_times_ii.html">
 <span class="dropdown-text">8. Sitzung (12.04.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/09_evidence_accumulation.html">
 <span class="dropdown-text">9. Sitzung (26.04.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/10_fitting_ddm.html">
 <span class="dropdown-text">10. Sitzung (03.05.2022)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../slides/11_data-analysis-1.html">
 <span class="dropdown-text">11. Sitzung (10.05.2022)</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bungen" role="button" data-bs-toggle="dropdown" aria-expanded="false">Übungen</a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bungen">
<li>
    <a class="dropdown-item" href="../../pages/exercises/exercise_01.html">
 <span class="dropdown-text">Übung 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/exercises/exercise_02.html">
 <span class="dropdown-text">Übung 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/exercises/exercise_03.html">
 <span class="dropdown-text">Übung 3</span></a>
  </li>  
        <li class="dropdown-header">pages/exercises/exercise_04.qmd</li>
        <li>
    <a class="dropdown-item" href="../../pages/exercises/exercise_05.html">
 <span class="dropdown-text">Übung 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/exercises/exercise_06.html">
 <span class="dropdown-text">Übung 6</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-lösungen" role="button" data-bs-toggle="dropdown" aria-expanded="false">Lösungen</a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-lösungen">
<li>
    <a class="dropdown-item" href="../../pages/solutions/solution_03.html">
 <span class="dropdown-text">Übung 3: Lösung</span></a>
  </li>  
    </ul>
</li>
</ul>
<div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li><a href="#data-analysis" id="toc-data-analysis" class="nav-link active" data-scroll-target="#data-analysis">Data analysis</a></li>
  <li>
<a href="#modelling-and-summarizing-data-from-multiple-participants" id="toc-modelling-and-summarizing-data-from-multiple-participants" class="nav-link" data-scroll-target="#modelling-and-summarizing-data-from-multiple-participants">Modelling and summarizing data from multiple participants</a>
  <ul class="collapse">
<li><a href="#complete-pooling" id="toc-complete-pooling" class="nav-link" data-scroll-target="#complete-pooling">Complete pooling</a></li>
  </ul>
</li>
  <li><a href="#no-pooling" id="toc-no-pooling" class="nav-link" data-scroll-target="#no-pooling">No pooling</a></li>
  <li><a href="#partial-pooling" id="toc-partial-pooling" class="nav-link" data-scroll-target="#partial-pooling">Partial pooling</a></li>
  <li>
<a href="#frequentist-statistics" id="toc-frequentist-statistics" class="nav-link" data-scroll-target="#frequentist-statistics">Frequentist statistics</a>
  <ul class="collapse">
<li><a href="#what-can-we-discover-with-nhst" id="toc-what-can-we-discover-with-nhst" class="nav-link" data-scroll-target="#what-can-we-discover-with-nhst">What can we discover with NHST?</a></li>
  </ul>
</li>
  <li><a href="#problems-with-nhst" id="toc-problems-with-nhst" class="nav-link" data-scroll-target="#problems-with-nhst">Problems with NHST</a></li>
  <li>
<a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies">Case studies</a>
  <ul class="collapse">
<li><a href="#sec-fancyhat" id="toc-sec-fancyhat" class="nav-link" data-scroll-target="#sec-fancyhat">T-test</a></li>
  <li><a href="#card-game" id="toc-card-game" class="nav-link" data-scroll-target="#card-game">Card game</a></li>
  </ul>
</li>
  <li><a href="#an-introduction-to-bayesian-inference" id="toc-an-introduction-to-bayesian-inference" class="nav-link" data-scroll-target="#an-introduction-to-bayesian-inference">An introduction to Bayesian inference</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap">Recap</a></li>
  <li><a href="#bayesian-inference-in-the-card-game-a-numerical-example" id="toc-bayesian-inference-in-the-card-game-a-numerical-example" class="nav-link" data-scroll-target="#bayesian-inference-in-the-card-game-a-numerical-example">Bayesian inference in the card game: a numerical example</a></li>
  <li><a href="#summarizing-the-posterior" id="toc-summarizing-the-posterior" class="nav-link" data-scroll-target="#summarizing-the-posterior">Summarizing the posterior</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Data analysis: I</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>

<div>
  <div class="description">
    <p>Combining informatin from multiple participants | Intro to Bayesian data anlysis.</p>
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">Andrew Ellis <a href="https://orcid.org/0000-0002-2788-936X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
</div>

    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.kog.psy.unibe.ch">
            Kognitive Psychologie, Wahrnehmung und Methodenlehre, Universität Bern
            </a>
          </p>
      </div>
    </div>



<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2022-05-10</p>
    </div>
  </div>
    
  </div>
  

</header><!-- :::{.callout-note} --><!-- 👉 [R Code für dieses Kapitel downloaden](../../downloadable_files/evidence-accumulation-1.R) --><!-- ::: --><!-- t-test --><!--     - Lee --><!-- Signal detection --><!--     - Lee --><!--     - Farrell --><div class="cell">

</div>
<section id="data-analysis" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="data-analysis">Data analysis</h2>
<p>In this chapter, we will tackle two important topics:</p>
<ol type="1">
<li><p>What methods should we use when modelling and summarizing data from multiple participants, and</p></li>
<li><p>Which alternative approaches are available when methods for null hypothesis significance testing (NHST) are not suitable.</p></li>
</ol>
<p>The latter is often the case in neuroscience, where low power due to small sample sizes is a well-known problem <span class="citation" data-cites="buttonPowerFailureWhy2013">(<a href="#ref-buttonPowerFailureWhy2013" role="doc-biblioref">Button et al. 2013</a>)</span>. The alternative approach we will discuss here is Bayesian data analysis, which also has the benefit is allowing us to provide evidence <em>for</em> or <em>against</em> hypotheses, something which is not possible using NHST.</p>
<div class="no-row-height column-margin column-container"><div id="ref-buttonPowerFailureWhy2013" class="csl-entry" role="doc-biblioentry">
Button, Katherine S., John P. A. Ioannidis, Claire Mokrysz, Brian A. Nosek, Jonathan Flint, Emma S. J. Robinson, and Marcus R. Munafò. 2013. <span>“Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience.”</span> <em>Nature Reviews Neuroscience</em> 14 (5): 365–76. <a href="https://doi.org/10.1038/nrn3475">https://doi.org/10.1038/nrn3475</a>.
</div></div></section><section id="modelling-and-summarizing-data-from-multiple-participants" class="level2"><h2 class="anchored" data-anchor-id="modelling-and-summarizing-data-from-multiple-participants">Modelling and summarizing data from multiple participants</h2>
<p>In most neuroscience experiments, we are interested in effects of manipulations at the individual level, but we also need to be sure that theses effects hold at the group level, and are not particular to certain individuals. However, reporting effects at the group level is not straightforward.In short, there are three possiblities when combining informatin from multiple participants:</p>
<section id="complete-pooling" class="level3"><h3 class="anchored" data-anchor-id="complete-pooling">Complete pooling</h3>
<p>We can pretend that all our data come from 1 participant, by averaging over participants. In statistics, this is know as <em>complete pooling</em>, meaning that all data are pooled and we are no longer interested in individual participants. The pooled data are then used to estimate parameters, and to perfom hypothesis testing. While there are cases in which it is possible to analyze averaged data, in general this approach can lead to errors, and needs to be treated with care. A classic example is shown in <a href="#fig-power-law">Figure&nbsp;1</a>. The thin solid lines represent individual learning curves. Each indivdial appears to learn very rapidly (and linearly), but there is a huge variation in the onset of learning. The filled circles represent a naive attempt at averaging over all individuals. The result suggests that learning is gradual and starts from the outset; however the average learning curve looks nothing like the individual learning curves. This type of analysis can lead to erroneous conclusions.</p>
<div id="fig-power-law" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="../../assets/images/aggregating-power-law.png" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 1: Learning curves</figcaption><p></p>
</figure>
</div>
</section></section><section id="no-pooling" class="level2"><h2 class="anchored" data-anchor-id="no-pooling">No pooling</h2>
<p>This means that data we estimate parameters of our models for each participant separately. While this appears to be a more sensible approach, there is the danger of over-fitting. This means that we give up the ability to generalize because our models are too sensitive to noise inherent in our data. Estimating parameters at the individual level is usually followed by a group-level statistical analysis. We might estimate the parameters of a signal detection or diffusion decision model for each participant individually, and then perform hypothesis tests on those parameters at the group level. For example, in a within-person design, we estimate individual bias parameters for each person in two conditions, and then perform a t-test at the group level to discover whether there is a difference between conditions. A problem that is associated with this approach is that we have to treat the estimated parameters as “data” at the group level, thus ignoring any uncertainty associated with those parameters; this can lead to over-confidence in our results. Nevertheless, this two-stage analysis is by far the most common approach in neuroscience.</p>
</section><section id="partial-pooling" class="level2"><h2 class="anchored" data-anchor-id="partial-pooling">Partial pooling</h2>
<p>A principled solution to the problems associated with individual and group level analyses is to use multilevel modelling. This allows us to simultaneously estimate group level parameters (fixed effects) and individual parameters (random effects) as deviations from the group average. This leads to the phenomenon known as <em>shrinkage</em>, which means that individual estimates are “pulled” towards the group average, and are less susceptible to noise in the data. An intuitive way of thinking about this is that we are assuming that participants are all individuals, but they do share some common characteristics, and we can use information obtained from other participants to inform our estimates. This is what <em>partial pooling</em> refers to.</p>
<p>While multilevel models are outside the scope of this course, we will look some basic models in our final session on Baysian data analysis. THe basic idea is that each individual participant’s parameters are treated as random draws from a group-level distribution, and the average effect is then simply the mean of that distribution.</p>
<div class="aside">
<p>Multilevel models do not have to be Bayesian - the can just as well be estimated using maximum likelihood estimation.</p>
</div>
</section><section id="frequentist-statistics" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="frequentist-statistics">Frequentist statistics</h2>
<p>The traditional approach to statistics is know as frequentist statistics. We will first go through the basic principles, then discuss some of the limitations associated with this approach, and then contrast this with the Bayesian approach.</p>
<p>In a traditional approach to statistics, students are usually taught a variety of different approaches (far too many to discuss here - there are enough enough different models to fill entire text books). In a nutshell, most of these approachs perform the following steps (I will use an equal-variance independent samples t-test as a running example, as well as the problem of estimating the probability of sucess parameter in a model of Bernoulli trials):</p>
<ol type="1">
<li>A statistical model is assumed. In the case of the t-test all observations within both groups are assumed to be normally distributed, with the groups differing in the means of the distributions (assuming that the standard deviations of both groups are equal to <span class="math inline">\(\sigma\)</span>).</li>
</ol>
<div class="page-columns page-full"><p><span class="math display">\[
y_{ij} \sim \mathcal{N}(\mu_j, \sigma^2)
\]</span> <span class="math inline">\(y_{ij}\)</span> refers to the <span class="math inline">\(i_{th}\)</span> observation in group <span class="math inline">\(j \in \{1, 2\}\)</span>. This observations are assumed to be independent random variates from a normal distribution (i.i.d)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, with the distribution’s mean <span class="math inline">\(\mu_j\)</span> depending on the group. In a t-test, we are interested in the difference between the group means, <span class="math inline">\(\mu_2 - \mu_1\)</span>.</p><div class="no-row-height column-margin column-container"><li id="fn1" role="doc-endnote"><p><sup>1</sup>&nbsp;independent and identically distributed</p></li></div></div>
<ol start="2" type="1">
<li><p>The group means are estimated. This can be done, e.g.&nbsp;using maximum likelihood estimation. You will be more familiar with the technique of “simply” using the sample means to estimate <span class="math inline">\(\mu_j\)</span>, which actually corresponds to the maximum likelihood estimate. The third parameter that has to be estimated is <span class="math inline">\(\sigma\)</span>. This is the <strong>parameter estimation</strong> step. <span class="math inline">\(\sigma\)</span> can be estimated using maximum likelihood, or simply “calculated” as the pooled standard deviation <span class="math inline">\(s_p\)</span>. In either case, the parameter is estimated.</p></li>
<li><p>A test statistic is calculated. In this case, the difference <span class="math inline">\(\mu_1 - \mu_2\)</span> is of interest, and using this difference, a test statistic is computed. The test statistic can be computed as</p></li>
</ol>
<p><span class="math display">\[
t = \frac{\bar{x_1} - \bar{x_2}}{s_p \sqrt{2/n}}
\]</span></p>
<p>where <span class="math inline">\(s_p \sqrt{2/n}\)</span> is the standard error of the difference. <span class="math inline">\(t\)</span> is assumed to follow a Student-t distribution (under the null hypothesis), and we can compute the probability of observing a test statistic (estimated from the data) that is at least as extreme (depending on the hypothesis) as the observed one (or rather the t value estimated from the actually observed data), under the assumption that the null hypothesis is true. This tail probability is known as a <em>p-value</em>.</p>
<section id="what-can-we-discover-with-nhst" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="what-can-we-discover-with-nhst">What can we discover with NHST?</h3>
<p>It is important to think about what exactly this approach can tell us. For instance, can we discover how probable the null hypothesis is? Can we ask how probable it is that the null hypothesis is false, and that therefore the alternative is true?</p>
<p>Unfortunately, the answer to both of these questions is <strong>no</strong>, we cannot.</p>
<p>This approach cannot tell us anything about probabilities of hypotheses - all that we can discover is how likely the data (as summarized by the parameters) are to have occured under the assumption that the null is true. In this case, how likely is the t value, given that there is no difference between the groups, or mathematically, <span class="math inline">\(\mu_1 - \mu_2 = 0\)</span>. A small p-value can tell us that the data are unlikely, but that is not really the question we are asking.</p>
<p>What we would like to know (intuitively) is how likely are our hypotheses <span class="citation" data-cites="wagenmakersPracticalSolutionPervasive2007">(<a href="#ref-wagenmakersPracticalSolutionPervasive2007" role="doc-biblioref">Wagenmakers 2007</a>)</span>. But this is something that frequentist statistics cannot tell us, by design. In the frequentist interpretation, a probability is the relative frequency of occurence of an event. Under this interpretation, a parameter does not have an associated probability distribution, and the question of how probable certain parameter values are is meaningless. In contrast, in Bayesian statistics, a probability is assumed to represent the degree of belief (either subjective or objective) that a certain parameter value is true. As an example, for a frequentist, talking about the probability that it will rain tomorrow is meaningless, because this is not an experiment that can be repeated infinitely many times. For a Bayesian, on the other hand, the probability of rain is merely an expression of belief, a summary of our state of knowledge.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wagenmakersPracticalSolutionPervasive2007" class="csl-entry" role="doc-biblioentry">
Wagenmakers, Eric-Jan. 2007. <span>“A Practical Solution to the Pervasive Problems of p Values.”</span> <em>Psychonomic Bulletin &amp; Review</em> 14 (5): 779–804. <a href="https://doi.org/10.3758/BF03194105">https://doi.org/10.3758/BF03194105</a>.
</div></div></section></section><section id="problems-with-nhst" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="problems-with-nhst">Problems with NHST</h2>
<p>Even scientists routinely mistake p-values for probabilities of hypotheses. This is actually one of the most common misconceptions, and lead <span class="citation" data-cites="wassersteinASAStatementPValues2016">Wasserstein and Lazar (<a href="#ref-wassersteinASAStatementPValues2016" role="doc-biblioref">2016</a>)</span> to write a <em>Statement on Statistical Significance and P-Values</em> in the American Statistical Association. They clarify:</p>
<div class="no-row-height column-margin column-container"><div id="ref-wassersteinASAStatementPValues2016" class="csl-entry" role="doc-biblioentry">
Wasserstein, Ronald L., and Nicole A. Lazar. 2016. <span>“The <span>ASA Statement</span> on p-<span>Values</span>: <span>Context</span>, <span>Process</span>, and <span>Purpose</span>.”</span> <em>The American Statistician</em> 70 (2): 129–33. <a href="https://doi.org/10.1080/00031305.2016.1154108">https://doi.org/10.1080/00031305.2016.1154108</a>.
</div></div><ul>
<li>P-values can indicate how incompatible the data are with a specified statistical model.</li>
<li>P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.</li>
<li>Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.</li>
</ul>
<p>A further complication is that the commonly used threshold <span class="math inline">\(p = 0.05\)</span> is completely arbitrary, and based on a combination of ideas by Fisher and Pearson/Neyman. The modern usage actually reflects neither approaches, and is often considered to be a bit of a mess <span class="citation" data-cites="amrheinScientistsRiseStatistical2019">(<a href="#ref-amrheinScientistsRiseStatistical2019" role="doc-biblioref">Amrhein, Greenland, and McShane 2019</a>)</span>. <span class="citation" data-cites="gigerenzerMindlessStatistics2004">Gigerenzer (<a href="#ref-gigerenzerMindlessStatistics2004" role="doc-biblioref">2004</a>)</span>;<span class="citation" data-cites="gigerenzerStatisticalRitualsReplication2018a">Gigerenzer (<a href="#ref-gigerenzerStatisticalRitualsReplication2018a" role="doc-biblioref">2018</a>)</span> provides an interesting comment on modern usage of hypothesis testing.</p>
<div class="no-row-height column-margin column-container"><div id="ref-amrheinScientistsRiseStatistical2019" class="csl-entry" role="doc-biblioentry">
Amrhein, Valentin, Sander Greenland, and Blake McShane. 2019. <span>“Scientists Rise up Against Statistical Significance.”</span> <em>Nature</em> 567 (7748, 7748): 305–7. <a href="https://doi.org/10.1038/d41586-019-00857-9">https://doi.org/10.1038/d41586-019-00857-9</a>.
</div><div id="ref-gigerenzerMindlessStatistics2004" class="csl-entry" role="doc-biblioentry">
Gigerenzer, Gerd. 2004. <span>“Mindless Statistics.”</span> <em>The Journal of Socio-Economics</em> 33 (5): 587–606. <a href="https://doi.org/10.1016/j.socec.2004.09.033">https://doi.org/10.1016/j.socec.2004.09.033</a>.
</div><div id="ref-gigerenzerStatisticalRitualsReplication2018a" class="csl-entry" role="doc-biblioentry">
———. 2018. <span>“Statistical <span>Rituals</span>: <span>The Replication Delusion</span> and <span>How We Got There</span>.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (2): 198–218. <a href="https://doi.org/10.1177/2515245918771329">https://doi.org/10.1177/2515245918771329</a>.
</div></div><p>Apart from not being particularly intuitive and not allowing us to adress the questions we would like to answer, NHST also has the problem that incentive structures in scientific publishing can often lead to misapplications of NHST procedures. Various questionable research practices are associated with this, including <em>p-hacking</em>. This refers to the practice of performing multiple significance tests, and selecting only those that yield significant results for reporting. In other words - NHST, if used correclty, can be a very useful tool. A big problem is that it os often not used correctly, and this can lead to misleading results.</p>
</section><section id="case-studies" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="case-studies">Case studies</h2>
<p>We will briefly explore the frequentist approach to data analysis using two examples:</p>
<ol type="1">
<li>an independent samples t-test, and</li>
<li>the card game from the previous chapter, and</li>
</ol>
<section id="sec-fancyhat" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-fancyhat">T-test</h3>
<p>This example uses simulated data, but is based on an actual study. In the code below, we simulate creativity scores from two groups. One group was instructed to wear a fancy hat, the other, woth no head gear, serves as a control group. We would like to know whether the fancy hat group and the control group differ in their mean creativity.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">12</span><span class="op">)</span>

<span class="co"># Number of people wearing fancy hats</span>
<span class="va">N_fancyhats</span> <span class="op">&lt;-</span> <span class="fl">50</span> 

<span class="co"># Number of people not wearing fancy hats</span>
<span class="va">N_nofancyhats</span> <span class="op">&lt;-</span> <span class="fl">50</span>

<span class="co"># Population mean of creativity for people wearing fancy hats</span>
<span class="va">mu_fancyhats</span> <span class="op">&lt;-</span> <span class="fl">103</span> 

<span class="co"># Population mean of creativity for people wearing no fancy hats</span>
<span class="va">mu_nofancyhats</span> <span class="op">&lt;-</span> <span class="fl">98</span> 

<span class="co"># Average population standard deviation of both groups</span>
<span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">15</span> 

<span class="co"># Generate data</span>
<span class="va">fancyhats</span> <span class="op">=</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>Creativity <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N_fancyhats</span>, <span class="va">mu_fancyhats</span>, <span class="va">sigma</span><span class="op">)</span>,
               Group <span class="op">=</span> <span class="st">"Fancy Hat"</span><span class="op">)</span>

<span class="va">nofancyhats</span> <span class="op">=</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>Creativity <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N_nofancyhats</span>, <span class="va">mu_nofancyhats</span>, <span class="va">sigma</span><span class="op">)</span>,
                 Group <span class="op">=</span> <span class="st">"No Fancy Hat"</span><span class="op">)</span>


<span class="va">FancyHat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_rows</a></span><span class="op">(</span><span class="va">fancyhats</span>, <span class="va">nofancyhats</span><span class="op">)</span>  |&gt;
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Group <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_relevel.html">fct_relevel</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">Group</span><span class="op">)</span>, <span class="st">"No Fancy Hat"</span><span class="op">)</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We have created a dataframe called <code>FancyHat</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">FancyHat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 100 × 2
   Creativity Group    
        &lt;dbl&gt; &lt;fct&gt;    
 1       80.8 Fancy Hat
 2      127.  Fancy Hat
 3       88.6 Fancy Hat
 4       89.2 Fancy Hat
 5       73.0 Fancy Hat
 6       98.9 Fancy Hat
 7       98.3 Fancy Hat
 8       93.6 Fancy Hat
 9      101.  Fancy Hat
10      109.  Fancy Hat
# … with 90 more rows</code></pre>
</div>
</div>
<p>We can now pretend that we don’t know how the data were generated, and treat them as data from an experiment.</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">FancyHat</span> |&gt; 
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Creativity</span>, x <span class="op">=</span> <span class="va">Group</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title<span class="op">=</span> <span class="st">"Box Plot of Creativity Values"</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Assuming that both groups are normally distributed with the same variance, we can use the t-test to determine whether the mean of the two groups is different.</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">fancyhat_ttest</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">Creativity</span> <span class="op">~</span> <span class="va">Group</span>,
       var.equal <span class="op">=</span> <span class="cn">TRUE</span>,
       data <span class="op">=</span> <span class="va">FancyHat</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">fancyhat_ttest_tab</span> <span class="op">&lt;-</span> <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="va">fancyhat_ttest</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">fancyhat_ttest_tab</span> |&gt;
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">estimate</span>, <span class="va">estimate1</span>, <span class="va">estimate2</span>, <span class="va">statistic</span>, <span class="va">p.value</span>, <span class="va">conf.low</span>, <span class="va">conf.high</span><span class="op">)</span> |&gt;
    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 7
  estimate estimate1 estimate2 statistic p.value conf.low conf.high
     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    -1.65      99.2      101.    -0.637   0.526    -6.78      3.49</code></pre>
</div>
</div>
<p>It might not be obvious at first glance what we have done here:</p>
<ol type="1">
<li>We assumed that the data are conditionally normally distributed, given the means, with the same variance.</li>
</ol>
<p><span class="math display">\[
y_{ij} \sim \mathcal{N}(\mu_j, \sigma^2)
\]</span></p>
<ol start="2" type="1">
<li><p>we estimated three parameters: <span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\mu_2\)</span> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, and <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>We computed the difference between groups as <span class="math inline">\(\mu_1 - \mu_2\)</span>. This gives us an estimate of the difference between the means <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p></li>
<li><p>We compute a test statistic (empirical t value)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p></li>
<li><p>We computed the probability of observing the observed difference divided by the standard error, under the null hypothesis that the means are the same (<span class="math inline">\(\mu_1 = \mu_2\)</span>)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. This is a two-sided test, i.e.&nbsp;we have no hypothesis as to which group has the larger mean.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn2" role="doc-endnote"><p><sup>2</sup>&nbsp;<code>estimate1</code> und <code>estimate2</code></p></li><li id="fn3" role="doc-endnote"><p><sup>3</sup>&nbsp;<code>estimate</code></p></li><li id="fn4" role="doc-endnote"><p><sup>4</sup>&nbsp;<code>statistic</code></p></li><li id="fn5" role="doc-endnote"><p><sup>5</sup>&nbsp;<code>p.value</code></p></li></div><div class="callout-tip callout callout-style-default callout-captioned page-columns page-full">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body page-columns page-full">
<div class="page-columns page-full"><p>What does the p-value tell us? Does it tell you what you want to know? Do you know what the confidence interval tells you <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>?</p><div class="no-row-height column-margin column-container"><li id="fn6" role="doc-endnote"><p><sup>6</sup>&nbsp;<code>conf.low</code> and <code>conf.high</code></p></li></div></div>
<p>The p-value is 0.53 This is larger that the conventional threshold <span class="math inline">\(\alpha=0.05\)</span>. What does this mean?</p>
</div>
</div>
</section><section id="card-game" class="level3"><h3 class="anchored" data-anchor-id="card-game">Card game</h3>
<p>Two players are playing a game of cards. You observe that they play 9 games, and that player A wins 6 of those. Now you would like to estimate the probability that player A will win the next game. Another way of putting it that you would to estimate the ability of player A to beat player B at this particular game.</p>
<p>You know that the probability of success must lie in the range <span class="math inline">\([0, 1]\)</span>. What you might not be aware of is that you are assuming a certain probability model, and the probability of success is a parameter of that model. Let’s take a closer look:</p>
<p>We know that the number of <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> games follows a binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>. To make things simpler, we also know that each individual game is independent of the others, and the probability of success <span class="math inline">\(\theta\)</span> is the same for each game. Each success therefore follows a Bernoulli distribution with parameter <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[
y_i \sim \mathcal{Bernoulli}(\theta)
\]</span></p>
<div class="{callout-tip}">
<p>I will generally use the notation <span class="math inline">\(y\)</span> for a variable that is observed, i.e.&nbsp;the data.</p>
</div>
<p><span class="math inline">\(y_i\)</span> is the <span class="math inline">\(i\)</span> th observation in the data, meaning that it tells us whether player A won the game or not on trial <span class="math inline">\(i\)</span>. <span class="math inline">\(\theta\)</span> is the probability of success for each individual game; this is a parameter of our model (<span class="math inline">\(\mathcal{M}\)</span>).</p>
<p>Previously, we used maximum likelihood to estimate <span class="math inline">\(\theta\)</span> - we will repeat this briefly here.</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">wins</span> <span class="op">&lt;-</span> <span class="fl">6</span>
<span class="va">games</span> <span class="op">&lt;-</span> <span class="fl">9</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The goal is to figure out the “best” value of <span class="math inline">\(\theta\)</span>, i.e.&nbsp;the value that maximizes the likelihood of observing the data. To do this, we need to consider a range of possible values of <span class="math inline">\(\theta\)</span> (we already know that this range is <span class="math inline">\([0, 1]\)</span>, so that part is easy). We will consider 100 values of <span class="math inline">\(\theta\)</span> between 0 and 1, and compute the likelihood of observing the data for each value of <span class="math inline">\(\theta\)</span>.</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">n_points</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">theta_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span> from<span class="op">=</span><span class="fl">0</span> , to<span class="op">=</span><span class="fl">1</span> , length.out <span class="op">=</span> <span class="va">n_points</span> <span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Assuming that both players have an equal chance of winning, the parameter should be <span class="math inline">\(\theta = 0.5\)</span>. The probability of the data given <span class="math inline">\(\theta = 0.5\)</span> is:</p>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">wins</span>, size <span class="op">=</span> <span class="va">games</span>, prob <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1640625</code></pre>
</div>
</div>
<p>The probability of winning 6 out of 9 games, given that both players are equally likely to win, is 0.1640625.</p>
<p>We can also compute the probability of A winning 6, 7, 8 or 9 games, using the cumulative distribution function of the binomial distribution.</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">pbinom</a></span><span class="op">(</span>q <span class="op">=</span> <span class="fl">5</span>, size <span class="op">=</span> <span class="va">games</span>, prob <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2539063</code></pre>
</div>
</div>
<p>or</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">pbinom</a></span><span class="op">(</span>q <span class="op">=</span> <span class="fl">5</span>, size <span class="op">=</span> <span class="va">games</span>, prob <span class="op">=</span> <span class="fl">0.5</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2539063</code></pre>
</div>
</div>
<div class="aside">
<p><code><a href="https://rdrr.io/r/stats/Binomial.html">pbinom()</a></code> gives us the lower tail probability, which is the probability that the number of successes is less than or equal to the given value, by default.</p>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
p-value
</div>
</div>
<div class="callout-body-container callout-body">
<p>Does this seem familiar?</p>
<p>If we want to quantify our <em>null</em> hypothsis that both players are equally likely to win, we would assume that <span class="math inline">\(\theta=0.5\)</span>. Computing the probability of the data under the null is exactly what we have just done. We then plug in the actual data, i.e.&nbsp;6 out of 9, and the upper tail probability is the p-value. In this case, the p-value is approximately <span class="math inline">\(0.25\)</span>. Using cut-off of 0.05, we would not reject the null hypothesis, and conclude that there is not enough evidence that player A is better than player B (this is a one-sided test).</p>
</div>
</div>
<p>Now we compute the probability of the data under all parameter values under consideration. In R, this is very simple, since all functions are vectorized.</p>
<div class="cell">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">wins</span> , size <span class="op">=</span> <span class="va">games</span> , prob <span class="op">=</span> <span class="va">theta_grid</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">likelihood</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can see in the above figure that the probability of observing the data is small for a lot of values of <span class="math inline">\(\theta\)</span>. The probability of observing the data, or the likelihood, is maximal for the value 0.6666667:</p>
<div class="cell">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">theta_grid</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="va">likelihood</span><span class="op">)</span><span class="op">]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6666667</code></pre>
</div>
</div>
<p>What we have done so far highlights the distinction between parameter estimation and hypothesis testing. Computing the tail probability under the null (<span class="math inline">\(\theta=0.5\)</span>) is a hypothesis test, and estimating <span class="math inline">\(\theta\)</span> is parameter estimation.</p>
</section></section><section id="an-introduction-to-bayesian-inference" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="an-introduction-to-bayesian-inference">An introduction to Bayesian inference</h2>
<div class="page-columns page-full"><p>So far, we haven’t considered any prior knowledge we might have had about which parameters are the most likely a priori. In fact, as we will see a bit further down, we have implicitly assumed that all parameters are equally likely. We will now introduce a new concept: a prior distribution for the parameter(s) we are trying to estimate<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn7" role="doc-endnote"><p><sup>7</sup>&nbsp;In frequentist statistics, the concept is meaningless - parameters cannot have distribution. In Bayesian statistics, a prior distribution should reflect everything we know about the parameter, before we consider the data. The prior reflects our belief, which can be subjective, or objective.</p></li></div></div>
<p>We will then use that <strong>prior</strong> belief in order to obtain a <strong>posterior</strong> belief over the possible parameter values. To do this, we need to multiply the prior probability of each parameter value by the likelihood of the data, i.e.&nbsp;by the probability of observing the data given that parameter value. This is an application of Bayes theorem:</p>
<p><span class="math display">\[
p(\theta|y) = \frac{ p(y|\theta) * p(\theta) } {p(y)}
\]</span></p>
<p>This states that the posterior probability of <span class="math inline">\(\theta\)</span> given the observed data <span class="math inline">\(y\)</span> is equal to the probability of the data, multiplied by how probable each value of <span class="math inline">\(\theta\)</span> is a priori. You can think of it like this: each parameter value is weighted according to how well it predicts the data. The product <span class="math inline">\(p(y|\theta) * p(\theta)\)</span> is then divided by the probability of the data, which in this case is summed over all possible parameter values. This step serves to normalize the posterior, so that it sums to <span class="math inline">\(1\)</span>. This essentially turns the unnormalized posterior into a proper probability distribution.</p>
<p><span class="math display">\[
p(y) = \sum_{\theta}p(y|\theta) * p(\theta)
\]</span></p>
<p>When we are interested in estimating the parameters of a given model, we can often neglect the (constant for a model) normalizing term <span class="math inline">\(p(y)\)</span>. This term, often called the evidence, reflects the prbability of the data, averaged over all parameter values. Written without the normalizing constant, Bayes rule is often written as:</p>
<p><span class="math display">\[
p(\theta|y) \propto  p(y|\theta) * p(\theta)
\]</span></p>
</section><section id="recap" class="level2"><h2 class="anchored" data-anchor-id="recap">Recap</h2>
<p>Bayesian inference, in a nutshell, consists of:</p>
<ol type="1">
<li><p>Represent your prior belief by a probability distribution over the possible parameter values. This is a principled way of dealing with uncertainty.</p></li>
<li><p>Use the likelihood to weight the prior belief.</p></li>
<li><p>Obtain a posterior belief over the possible parameter values.</p></li>
</ol>
<div class="aside">
<p>The term <em>belief</em> is used synonymously with <em>probability distribution</em>.</p>
</div>
</section><section id="bayesian-inference-in-the-card-game-a-numerical-example" class="level2"><h2 class="anchored" data-anchor-id="bayesian-inference-in-the-card-game-a-numerical-example">Bayesian inference in the card game: a numerical example</h2>
<p>Recall that we defined a sequence of 100 points between 0 and 1, which represented the possible <span class="math inline">\(\theta\)</span> values.</p>
<div class="cell">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">n_points</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">theta_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span> from<span class="op">=</span><span class="fl">0</span> , to<span class="op">=</span><span class="fl">1</span> , length.out <span class="op">=</span> <span class="va">n_points</span> <span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For each of these, we computed the likelihood, that is the probability of observing the (fixed) data, given the parameter. Now, we can make our knowledge about the probability of each parameter value explicit. At first, we will assume that all parameters are equally likely. We will assign the probability of 1 to each parameter value. This is our prior distribution.</p>
<div class="cell">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">theta_grid</span><span class="op">)</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span>, <span class="va">prior_1</span>, <span class="st">"type"</span> <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We could also express the belief that player is at least as good as player B, i.e.&nbsp;they are equally good or A is better than B. One way of doing this is to assign a probability of <span class="math inline">\(1\)</span> to parameter values greater than or equal to <span class="math inline">\(0.5\)</span>, and the value <span class="math inline">\(0\)</span> to parameter values less than <span class="math inline">\(0.5\)</span>.</p>
<div class="cell">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">theta_grid</span> <span class="op">&lt;</span> <span class="fl">0.5</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span>, <span class="va">prior_2</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>A more systematic way of doing this is to use a parameterized probability distribution that expresses our beliefs about the parameter.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A family of probability distributions that are suitable for parameters that lie in the interval <span class="math inline">\([0,1]\)</span> is the <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>. This distribution has <span class="math inline">\(2\)</span> parameters <span class="math inline">\(\alpha\)</span> und <span class="math inline">\(\beta\)</span>, which can be interpreted as the prior number of successes and the number of failures, respectively. The number of trials is therefore <span class="math inline">\(\alpha + \beta\)</span>. <span class="citation" data-cites="ref">(<a href="#ref-ref" role="doc-biblioref"><strong>ref?</strong></a>)</span>(fig:betadists) shows a number of possible Beta distributions for various settings of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<div class="cell">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>

<span class="va">length</span> <span class="op">&lt;-</span> <span class="fl">1e4</span>
<span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/expand.html">crossing</a></span><span class="op">(</span>shape1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span>,
           shape2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span><span class="op">)</span> |&gt;
  <span class="fu">tidyr</span><span class="fu">::</span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/expand.html">expand</a></span><span class="op">(</span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/expand.html">nesting</a></span><span class="op">(</span><span class="va">shape1</span>, <span class="va">shape2</span><span class="op">)</span>,
         x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">1</span>, length.out <span class="op">=</span> <span class="va">length</span><span class="op">)</span><span class="op">)</span> |&gt; 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>a <span class="op">=</span> <span class="fu"><a href="https://stringr.tidyverse.org/reference/str_c.html">str_c</a></span><span class="op">(</span><span class="st">"a = "</span>, <span class="va">shape1</span><span class="op">)</span>,
         b <span class="op">=</span> <span class="fu"><a href="https://stringr.tidyverse.org/reference/str_c.html">str_c</a></span><span class="op">(</span><span class="st">"b = "</span>, <span class="va">shape2</span><span class="op">)</span>,
         group <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">length</span>, each <span class="op">=</span> <span class="fl">25</span><span class="op">)</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">d</span> |&gt; 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, group <span class="op">=</span> <span class="va">group</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">x</span>, shape1 <span class="op">=</span> <span class="va">shape1</span>, shape2 <span class="op">=</span> <span class="va">shape2</span><span class="op">)</span><span class="op">)</span>,
            color <span class="op">=</span> <span class="st">"steelblue4"</span>, size <span class="op">=</span> <span class="fl">1.1</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_continuous</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">.5</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_cartesian.html">coord_cartesian</a></span><span class="op">(</span>ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Beta distributions"</span>,
       y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">p</span><span class="op">(</span><span class="va">theta</span><span class="op">*</span><span class="st">"|"</span><span class="op">*</span><span class="va">a</span><span class="op">*</span><span class="st">", "</span><span class="op">*</span><span class="va">b</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>panel.grid <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_blank</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_grid.html">facet_grid</a></span><span class="op">(</span><span class="va">b</span><span class="op">~</span><span class="va">a</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
<p>If we want to use a Beta distribution to express the belief that all values of <span class="math inline">\(\theta\)</span> are equally likely (uniform prior), we can a Beta distribution with <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\beta = 1\)</span>.</p>
<div class="cell">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta_grid</span>, shape1 <span class="op">=</span> <span class="fl">1</span>, shape2 <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span>, <span class="va">prior_3</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Finally, we could express the following prior information as a Beta distribution: imagine you had previously observed <span class="math inline">\(100\)</span> games between A and B, and each won half of the games. You can set <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to the number of games won by A, and the number of games won by B, respectively. A won <span class="math inline">\(50\)</span>, and B won <span class="math inline">\(50\)</span>:</p>
<div class="cell">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta_grid</span>, shape1 <span class="op">=</span> <span class="fl">50</span>, shape2 <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span>, <span class="va">prior</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Now, we can combine the prior and the likelihood by multiplying them element-wise, i.e.&nbsp;we need to multiply each parameter value with the probability of the data given that parameter.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recall Bayes theorem:</p>
<p><span class="math display">\[
p(\theta|y) = \frac{ p(y|\theta) * p(\theta) } {p(y)}
\]</span></p>
</div>
</div>
<p>In R, this is simply:</p>
<div class="cell">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">wins</span> <span class="op">&lt;-</span> <span class="fl">6</span>
<span class="va">games</span> <span class="op">&lt;-</span> <span class="fl">9</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta_grid</span>, shape1 <span class="op">=</span> <span class="fl">4</span>, shape2 <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>
<span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">wins</span> , size <span class="op">=</span> <span class="va">games</span> , prob <span class="op">=</span> <span class="va">theta_grid</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">unstandardized_posterior</span> <span class="op">&lt;-</span> <span class="va">likelihood</span> <span class="op">*</span> <span class="va">prior</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This gives us the unnormalized posterior:</p>
<p><span class="math display">\[
p(\theta|y) \propto  p(y|\theta) * p(\theta)
\]</span></p>
<p>We can then normalize the posterior distribution by dividing it by <span class="math inline">\(p(y) = \sum_{\theta}p(y|\theta) * p(\theta)\)</span>. In R we can use the <code><a href="https://rdrr.io/r/base/sum.html">sum()</a></code> function: <code>sum(unstandardized_posterior)</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">posterior</span> <span class="op">&lt;-</span> <span class="va">unstandardized_posterior</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">unstandardized_posterior</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now plot the resulting normalized posterior distribution.</p>
<div class="cell">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span>, <span class="va">posterior</span>, type <span class="op">=</span> <span class="st">"l"</span>, yaxt <span class="op">=</span> <span class="st">'n'</span>, ylab <span class="op">=</span> <span class="st">'Probability'</span>, 
        main <span class="op">=</span> <span class="st">"Posterior"</span>, cex.lab <span class="op">=</span> <span class="fl">1.5</span>, cex.main <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To make this repeatable, we will write two functions. The first, <code>compute_posterior()</code>, will compute the posterior, given the prior and likelihood, and return a dataframe containg prior, likelihood and posterior. The second, <code>plot_posterior()</code>, will plot all three side-by-side. You can also pass in the maximum likelihood estimate, e.g.&nbsp;<code>6/9</code>, and this will be plotted as well.</p>
<div class="cell">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">compute_posterior</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">likelihood</span>, <span class="va">prior</span><span class="op">)</span><span class="op">{</span>
  <span class="co"># compute product of likelihood and prior</span>
  <span class="va">unstandardized_posterior</span> <span class="op">&lt;-</span> <span class="va">likelihood</span> <span class="op">*</span> <span class="va">prior</span>
  
  <span class="co"># standardize the posterior, so it sums to 1</span>
  <span class="va">posterior</span> <span class="op">&lt;-</span> <span class="va">unstandardized_posterior</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">unstandardized_posterior</span><span class="op">)</span>
  
  <span class="va">out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span><span class="va">prior</span>, <span class="va">likelihood</span>, <span class="va">posterior</span><span class="op">)</span>
  <span class="va">out</span>
<span class="op">}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">plot_posterior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span>, <span class="va">mle</span> <span class="op">=</span> <span class="fl">6</span><span class="op">/</span><span class="fl">9</span><span class="op">)</span><span class="op">{</span>
<span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">df</span>, <span class="op">{</span>
    <span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>
    <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span> , <span class="va">prior</span>, type<span class="op">=</span><span class="st">"l"</span>, main<span class="op">=</span><span class="st">"Prior"</span>, col <span class="op">=</span> <span class="st">"dodgerblue3"</span>, 
            lwd <span class="op">=</span> <span class="fl">4</span>, yaxt <span class="op">=</span> <span class="st">'n'</span>, ylab <span class="op">=</span> <span class="st">'Probability'</span>, cex.lab <span class="op">=</span> <span class="fl">1.5</span>, cex.main <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
    <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span> , <span class="va">likelihood</span>, type <span class="op">=</span> <span class="st">"l"</span>, main <span class="op">=</span> <span class="st">"Likelihood"</span>, col <span class="op">=</span> <span class="st">"firebrick3"</span>, 
            lwd <span class="op">=</span> <span class="fl">4</span>, yaxt <span class="op">=</span> <span class="st">'n'</span>, ylab <span class="op">=</span> <span class="st">''</span>, cex.lab <span class="op">=</span> <span class="fl">1.5</span>, cex.main <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
    <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta_grid</span> , <span class="va">posterior</span> , type <span class="op">=</span> <span class="st">"l"</span>, main <span class="op">=</span> <span class="st">"Posterior"</span>, col <span class="op">=</span> <span class="st">"darkorchid3"</span>, 
            lwd <span class="op">=</span> <span class="fl">4</span>, yaxt <span class="op">=</span> <span class="st">'n'</span>, ylab <span class="op">=</span> <span class="st">''</span>, cex.lab <span class="op">=</span> <span class="fl">1.5</span>, cex.main <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
    <span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="va">mle</span>, col <span class="op">=</span> <span class="fl">4</span>, lty <span class="op">=</span> <span class="fl">2</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
  <span class="op">}</span> <span class="op">)</span>
<span class="op">}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can now try out various prior distributions, and observe the effect on the posterior.</p>
<p>Let’s try out a uniform prior first:</p>
<div class="cell">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta_grid</span>, shape1 <span class="op">=</span> <span class="fl">1</span>, shape2 <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>
<span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">wins</span> , size <span class="op">=</span> <span class="va">games</span> , prob <span class="op">=</span> <span class="va">theta_grid</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">compute_posterior</span><span class="op">(</span><span class="va">likelihood</span>, <span class="va">prior</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu">plot_posterior</span><span class="op">(</span><span class="va">df</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this case, the maximum likelihood estimate coincides with the value that maximizes the posterior probability. This is the case because maximum likelihood estimate uses only the likelihood and does not consider prior knowledge. If we are are using a uniform prior, that amounts to saying that all values of <span class="math inline">\(\theta\)</span> are equally likely. In other words, we have no information as which values are more probable.z</p>
</div>
</div>
<p>Now let’s try out our prior expressing the belief that player A cannot be worse than player B:</p>
<div class="cell">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">theta_grid</span> <span class="op">&lt;</span> <span class="fl">0.5</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>
<span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">wins</span> , size <span class="op">=</span> <span class="va">games</span> , prob <span class="op">=</span> <span class="va">theta_grid</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">compute_posterior</span><span class="op">(</span><span class="va">likelihood</span>, <span class="va">prior</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu">plot_posterior</span><span class="op">(</span><span class="va">df</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-39-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The resulting posterior distribution is zero for all values of <span class="math inline">\(\theta\)</span> that are less than 0.5. This is because our prior allocates zero probability those values.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Try out various priors. How do they affect the posterior?</p>
<ul>
<li>A prior exspressing the belief that B is better than A.</li>
<li>A prior expressing that belief that either A is a lot better, or B is a lot better.</li>
<li>A prior expressing the belief that the players are probably as good as each other, and that it is unlikely that either is much better than the other.</li>
</ul>
</div>
</div>
<p>The posterior represents our belief about the the possible parameter values, after having observed the data. You can therefore think of Bayesian inference as a method for updating your beliefs, conditional on observed data. You are re-allocating probabilities over parameter values, depending on how well those parameter values predicted the data.</p>
</section><section id="summarizing-the-posterior" class="level2"><h2 class="anchored" data-anchor-id="summarizing-the-posterior">Summarizing the posterior</h2>
<p>We now need one last step: we need to summarize the posterior distribution. We can do this, for example by computing the mean and standard deviation of the posterior distribution. However, the method we have looked at so far is very simple, and only works for single parameters with a well-defined range. For real-world inference problems, we will numerical methods to approximate the posterior distribution. These methods are know collectively as <em>Monte Carlo sampling</em>, or <em>Markov Chain Monte Carlo</em> (MCMC). Using MCMC, we will not obtain an analytical description of posterior distributions, but instead a collection of random numbers (samples), that are drawn from the posterior distribution. We will use these samples to represent the posterior distribution.</p>
<p>To show how this works, we can draw a thousand samples from the posterior distribution. First, we’ll create a posterior.</p>
<div class="cell">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta_grid</span>, shape1 <span class="op">=</span> <span class="fl">1</span>, shape2 <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>
<span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">wins</span>, size <span class="op">=</span> <span class="va">games</span> , prob <span class="op">=</span> <span class="va">theta_grid</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">compute_posterior</span><span class="op">(</span><span class="va">likelihood</span>, <span class="va">prior</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu">plot_posterior</span><span class="op">(</span><span class="va">df</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="11_data-analysis-1_files/figure-html/unnamed-chunk-42-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Now we’ll draw 1000 random numbers from the posterior.</p>
<div class="cell">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">n_samples</span> <span class="op">&lt;-</span> <span class="fl">1e3</span>

<span class="va">samples</span> <span class="op">&lt;-</span> <span class="va">theta_grid</span> |&gt; <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span>size <span class="op">=</span> <span class="va">n_samples</span>, replace <span class="op">=</span> <span class="cn">TRUE</span>, prob <span class="op">=</span> <span class="va">df</span><span class="op">$</span><span class="va">posterior</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">samples</span>, <span class="fl">10</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.3333333 0.6262626 0.5555556 0.8383838 0.3434343 0.7878788 0.7171717
 [8] 0.6565657 0.7676768 0.7171717</code></pre>
</div>
</div>
<p>Now we can summarize the samples, e.g.&nbsp;by computing the mean or quantiles.</p>
<div class="cell">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">samples</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.634697</code></pre>
</div>
</div>
<p>The following will give us the median, and a 50% <strong>credible interval</strong>, i.e.&nbsp;an interval that contains 50% of the mass of the distribution.</p>
<div class="cell">
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">samples</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span><span class="op">)</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      25%       50%       75% 
0.5454545 0.6464646 0.7373737 </code></pre>
</div>
</div>
<p>We can also use this approach to compute a 95% credible interval.</p>
<div class="cell">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span>

<span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">samples</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">alpha</span><span class="op">/</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">-</span><span class="va">alpha</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%     97.5% 
0.3535354 0.8787879 </code></pre>
</div>
</div>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This should not be confused with the 95% confidence interval. Can you remember how a confidence interval is defined? What is the difference between a confidence interval and a credible interval?</p>
</div>
</div>
<p>What we have done so far is to look at Bayesian inference for parameter estimation (in a very simple model). Let’s call this model <span class="math inline">\(\mathcal{M}\)</span>. We have only considered one model at a time, but in the next session we will consider two or models $, which differ in the prior distributions they use. We will then look at methods for comparing models, in order to perform hypothesis testing in a Bayesian framework.</p>
<p>In particular, we will look at how Bayesian model comparison can help, in the case that we have no significant results (see <a href="#sec-fancyhat">Section&nbsp;7.1</a>).</p>


<!-- -->


</section><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ellis2022,
  author = {Andrew Ellis},
  title = {Data Analysis: {I}},
  date = {2022-05-10},
  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/11_data-analysis-1.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ellis2022" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Andrew Ellis. 2022. <span>“Data Analysis: I.”</span> May 10, 2022. <a href="https://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/11_data-analysis-1.html">https://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/11_data-analysis-1.html</a>.
</div></div></section></div></main><!-- /main --><script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
    } else {
      disableStylesheet(alternateStylesheets);
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb57" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Data analysis: I"</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> | </span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Combining informatin from multiple participants | Intro to Bayesian data anlysis.</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2022-05-10"</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Andrew Ellis</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://github.com/awellis</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, Universität Bern </span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://www.kog.psy.unibe.ch</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-2788-936X</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="an">license:</span><span class="co"> CC BY</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span><span class="co"> true</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> ../../bibliography.bib</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="an">reference-location:</span><span class="co"> margin</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="an">citation-location:</span><span class="co"> margin</span></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a><span class="co">    html:</span></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a><span class="co">        toc: true</span></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a><span class="co">        code-link: true</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a><span class="co">        code-fold: false</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a><span class="co">        code-tools: true</span></span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- :::{.callout-note} --&gt;</span></span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- 👉 [R Code für dieses Kapitel downloaden](../../downloadable_files/evidence-accumulation-1.R) --&gt;</span></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: --&gt;</span></span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- t-test --&gt;</span></span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     - Lee --&gt;</span></span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Signal detection --&gt;</span></span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     - Lee --&gt;</span></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--     - Farrell --&gt;</span></span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load-packages</span></span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridis)</span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-47"><a href="#cb57-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-48"><a href="#cb57-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-49"><a href="#cb57-49" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data analysis</span></span>
<span id="cb57-50"><a href="#cb57-50" aria-hidden="true" tabindex="-1"></a>In this chapter, we will tackle two important topics: </span>
<span id="cb57-51"><a href="#cb57-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-52"><a href="#cb57-52" aria-hidden="true" tabindex="-1"></a>1) What methods should we use when modelling and summarizing data from multiple participants, and </span>
<span id="cb57-53"><a href="#cb57-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-54"><a href="#cb57-54" aria-hidden="true" tabindex="-1"></a>2) Which alternative approaches are available when methods for null hypothesis significance testing (NHST) are not suitable. </span>
<span id="cb57-55"><a href="#cb57-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-56"><a href="#cb57-56" aria-hidden="true" tabindex="-1"></a>The latter is often the case in neuroscience, where low power due to small sample sizes is a well-known problem <span class="co">[</span><span class="ot">@buttonPowerFailureWhy2013</span><span class="co">]</span>. The alternative approach we will discuss here is Bayesian data analysis, which also has the benefit is allowing us to provide evidence _for_ or _against_ hypotheses, something which is not possible using NHST. </span>
<span id="cb57-57"><a href="#cb57-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-58"><a href="#cb57-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-59"><a href="#cb57-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-60"><a href="#cb57-60" aria-hidden="true" tabindex="-1"></a><span class="fu">## Modelling and summarizing data from multiple participants</span></span>
<span id="cb57-61"><a href="#cb57-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-62"><a href="#cb57-62" aria-hidden="true" tabindex="-1"></a>In most neuroscience experiments, we are interested in effects of manipulations at the individual level, but we also need to be sure that theses effects hold at the group level, and are not particular to certain individuals. However, reporting effects at the group level is not straightforward.In short, there are three possiblities </span>
<span id="cb57-63"><a href="#cb57-63" aria-hidden="true" tabindex="-1"></a>when combining informatin from multiple participants:</span>
<span id="cb57-64"><a href="#cb57-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-65"><a href="#cb57-65" aria-hidden="true" tabindex="-1"></a><span class="fu">### Complete pooling</span></span>
<span id="cb57-66"><a href="#cb57-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-67"><a href="#cb57-67" aria-hidden="true" tabindex="-1"></a>We can pretend that all our data come from 1 participant, by averaging over participants. In statistics, this is know as  _complete pooling_, meaning that all data are pooled and we are no longer interested in individual participants. The pooled data are then used to estimate parameters, and to perfom hypothesis testing. While there are cases in which it is possible to analyze averaged data, in general this approach can lead to errors, and needs to be treated with care. A classic example is shown in @fig-power-law. The thin solid lines represent individual learning curves. Each indivdial appears to learn very rapidly (and linearly), but there is a huge variation in the onset of learning. The filled circles represent a naive attempt at averaging over all individuals. The result suggests that learning is gradual and starts from the outset; however the average learning curve looks nothing like the individual learning curves. This type of analysis can lead to erroneous conclusions.</span>
<span id="cb57-68"><a href="#cb57-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-69"><a href="#cb57-69" aria-hidden="true" tabindex="-1"></a><span class="al">![Learning curves](../../assets/images/aggregating-power-law.png)</span>{#fig-power-law}</span>
<span id="cb57-70"><a href="#cb57-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-71"><a href="#cb57-71" aria-hidden="true" tabindex="-1"></a><span class="fu">## No pooling</span></span>
<span id="cb57-72"><a href="#cb57-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-73"><a href="#cb57-73" aria-hidden="true" tabindex="-1"></a>This means that data we estimate parameters of our models for each participant separately. While this appears to be a more sensible approach, there is the danger of over-fitting. This means that we give up the ability to generalize because our models are too sensitive to noise inherent in our data. Estimating parameters at the individual level is usually followed by a group-level statistical analysis. We might estimate the parameters of a signal detection or diffusion decision model for each participant individually, and then perform hypothesis tests on those parameters at the group level. For example, in a within-person design, we estimate individual bias parameters for each person in two conditions, and then perform a t-test at the group level to discover whether there is a difference between conditions. A problem that is associated with this approach is that we have to treat the estimated parameters as "data" at the group level, thus ignoring any uncertainty associated with those parameters; this can lead to over-confidence in our results. Nevertheless, this two-stage analysis is by far the most common approach in neuroscience.</span>
<span id="cb57-74"><a href="#cb57-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-75"><a href="#cb57-75" aria-hidden="true" tabindex="-1"></a><span class="fu">## Partial pooling</span></span>
<span id="cb57-76"><a href="#cb57-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-77"><a href="#cb57-77" aria-hidden="true" tabindex="-1"></a>A principled solution to the problems associated with individual and group level analyses is to use multilevel modelling. This allows us to simultaneously estimate group level parameters (fixed effects) and individual parameters (random effects) as deviations from the group average. This leads to the phenomenon known as _shrinkage_, which means that individual estimates are "pulled" towards the group average, and are less susceptible to noise in the data. An intuitive way of thinking about this is that we are assuming that participants are all individuals, but they do share some common characteristics, and we can use information obtained from other participants to inform our estimates. This is what _partial pooling_ refers to.</span>
<span id="cb57-78"><a href="#cb57-78" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb57-79"><a href="#cb57-79" aria-hidden="true" tabindex="-1"></a>While multilevel models are outside the scope of this course, we will look some basic models in our final session on Baysian data analysis. THe basic idea is that each individual participant's parameters are treated as random draws from a group-level distribution, and the average effect is then simply the mean of that distribution.</span>
<span id="cb57-80"><a href="#cb57-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-81"><a href="#cb57-81" aria-hidden="true" tabindex="-1"></a>:::aside</span>
<span id="cb57-82"><a href="#cb57-82" aria-hidden="true" tabindex="-1"></a>Multilevel models do not have to be Bayesian - the can just as well be estimated using maximum likelihood estimation.</span>
<span id="cb57-83"><a href="#cb57-83" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-84"><a href="#cb57-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-85"><a href="#cb57-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-86"><a href="#cb57-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-87"><a href="#cb57-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## Frequentist statistics</span></span>
<span id="cb57-88"><a href="#cb57-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-89"><a href="#cb57-89" aria-hidden="true" tabindex="-1"></a>The traditional approach to statistics is know as frequentist statistics. We will first go through the basic principles, then discuss some of the limitations associated with this approach, and then contrast this with the Bayesian approach. </span>
<span id="cb57-90"><a href="#cb57-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-91"><a href="#cb57-91" aria-hidden="true" tabindex="-1"></a>In a traditional approach to statistics, students are usually taught a variety of different approaches (far too many to discuss here - there are enough enough different models to fill entire text books). In a nutshell, most of these approachs perform the following steps (I will use an equal-variance independent samples t-test as a running example, as well as the problem of estimating the probability of sucess parameter in a model of Bernoulli trials):</span>
<span id="cb57-92"><a href="#cb57-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-93"><a href="#cb57-93" aria-hidden="true" tabindex="-1"></a>1) A statistical model is assumed. In the case of the t-test all observations within both groups are assumed to be normally distributed, with the groups differing in the means of the distributions (assuming that the standard deviations of both groups are equal to $\sigma$).</span>
<span id="cb57-94"><a href="#cb57-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-95"><a href="#cb57-95" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb57-96"><a href="#cb57-96" aria-hidden="true" tabindex="-1"></a>y_{ij} \sim \mathcal{N}(\mu_j, \sigma^2)</span>
<span id="cb57-97"><a href="#cb57-97" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-98"><a href="#cb57-98" aria-hidden="true" tabindex="-1"></a>$y_{ij}$ refers to the $i_{th}$ observation in group $j \in <span class="sc">\{</span>1, 2<span class="sc">\}</span>$. This observations are assumed to be independent random variates from a normal distribution (i.i.d)^<span class="co">[</span><span class="ot">independent and identically distributed</span><span class="co">]</span>, with the distribution's mean $\mu_j$ depending on the group. In a t-test, we are interested in the difference between the group means, $\mu_2 - \mu_1$.</span>
<span id="cb57-99"><a href="#cb57-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-100"><a href="#cb57-100" aria-hidden="true" tabindex="-1"></a>2) The group means are estimated. This can be done, e.g. using maximum likelihood estimation. You will be more familiar with the technique of "simply" using the sample means to estimate $\mu_j$, which actually corresponds to the maximum likelihood estimate. The third parameter that has to be estimated is $\sigma$. This is the __parameter estimation__ step. $\sigma$ can be estimated using maximum likelihood, or simply "calculated" as the pooled standard deviation $s_p$. In either case, the parameter is estimated.</span>
<span id="cb57-101"><a href="#cb57-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-102"><a href="#cb57-102" aria-hidden="true" tabindex="-1"></a>3) A test statistic is calculated. In this case, the difference $\mu_1 - \mu_2$ is of interest, and using this difference, a test statistic is computed. The test statistic can be computed as </span>
<span id="cb57-103"><a href="#cb57-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-104"><a href="#cb57-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-105"><a href="#cb57-105" aria-hidden="true" tabindex="-1"></a>t = \frac{\bar{x_1} - \bar{x_2}}{s_p \sqrt{2/n}}</span>
<span id="cb57-106"><a href="#cb57-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-107"><a href="#cb57-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-108"><a href="#cb57-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-109"><a href="#cb57-109" aria-hidden="true" tabindex="-1"></a>where $s_p \sqrt{2/n}$ is the standard error of the difference. $t$ is assumed to follow a Student-t distribution (under the null hypothesis), and we can compute the probability of observing a test statistic (estimated from the data) that is at least as extreme (depending on the hypothesis) as the observed one (or rather the t value estimated from the actually observed data), under the assumption that the null hypothesis is true. This tail probability is known as a *p-value*.</span>
<span id="cb57-110"><a href="#cb57-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-111"><a href="#cb57-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### What can we discover with NHST?</span></span>
<span id="cb57-112"><a href="#cb57-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-113"><a href="#cb57-113" aria-hidden="true" tabindex="-1"></a>It is important to think about what exactly this approach can tell us. For instance, can we discover how probable the null hypothesis is? Can we ask how probable it is that the null hypothesis is false, and that therefore the alternative is true? </span>
<span id="cb57-114"><a href="#cb57-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-115"><a href="#cb57-115" aria-hidden="true" tabindex="-1"></a>Unfortunately, the answer to both of these questions is **no**, we cannot. </span>
<span id="cb57-116"><a href="#cb57-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-117"><a href="#cb57-117" aria-hidden="true" tabindex="-1"></a>This approach cannot tell us anything about probabilities of hypotheses - all that we can discover is how likely the data (as summarized by the parameters) are to have occured under the assumption that the null is true. In this case, how likely is the t value, given that there is no difference between the groups, or mathematically, $\mu_1 - \mu_2 = 0$. A small p-value can tell us that the data are unlikely, but that is not really the question we are asking. </span>
<span id="cb57-118"><a href="#cb57-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-119"><a href="#cb57-119" aria-hidden="true" tabindex="-1"></a>What we would like to know (intuitively) is how likely are our hypotheses <span class="co">[</span><span class="ot">@wagenmakersPracticalSolutionPervasive2007</span><span class="co">]</span>. But this is something that frequentist statistics cannot tell us, by design. In the frequentist interpretation, a probability is the relative frequency of occurence of an event. Under this interpretation, a parameter does not have an associated probability distribution, and the question of how probable certain parameter values are is meaningless. In contrast, in Bayesian statistics, a probability is assumed to represent the degree of belief (either subjective or objective) that a certain parameter value is true. As an example, for a frequentist, talking about the probability that it will rain tomorrow is meaningless, because this is not an experiment that can be repeated infinitely many times. For a Bayesian, on the other hand, the probability of rain is merely an expression of belief, a summary of our state of knowledge.</span>
<span id="cb57-120"><a href="#cb57-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-121"><a href="#cb57-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-122"><a href="#cb57-122" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problems with NHST</span></span>
<span id="cb57-123"><a href="#cb57-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-124"><a href="#cb57-124" aria-hidden="true" tabindex="-1"></a>Even scientists routinely mistake p-values for probabilities of hypotheses. This is actually one of the most common misconceptions, and lead @wassersteinASAStatementPValues2016 to write a *Statement on Statistical</span>
<span id="cb57-125"><a href="#cb57-125" aria-hidden="true" tabindex="-1"></a>Significance and P-Values* in the American Statistical Association. They clarify:</span>
<span id="cb57-126"><a href="#cb57-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-127"><a href="#cb57-127" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>P-values can indicate how incompatible the data are with a specified</span>
<span id="cb57-128"><a href="#cb57-128" aria-hidden="true" tabindex="-1"></a>  statistical model.</span>
<span id="cb57-129"><a href="#cb57-129" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.</span>
<span id="cb57-130"><a href="#cb57-130" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.</span>
<span id="cb57-131"><a href="#cb57-131" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-132"><a href="#cb57-132" aria-hidden="true" tabindex="-1"></a>A further complication is that the commonly used threshold $p = 0.05$ is completely arbitrary, and based on a combination of ideas by Fisher and Pearson/Neyman. The modern usage actually reflects neither approaches, and  is often considered to be a bit of a mess <span class="co">[</span><span class="ot">@amrheinScientistsRiseStatistical2019</span><span class="co">]</span>. @gigerenzerMindlessStatistics2004;@gigerenzerStatisticalRitualsReplication2018a provides an interesting comment on modern usage of hypothesis testing.</span>
<span id="cb57-133"><a href="#cb57-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-134"><a href="#cb57-134" aria-hidden="true" tabindex="-1"></a>Apart from not being particularly intuitive and not allowing us to adress the questions we would like to  answer, NHST also has the problem that incentive structures in scientific publishing can often lead to misapplications of NHST procedures. Various questionable research practices are associated with this, including _p-hacking_. This refers to the practice of performing multiple significance tests, and selecting only those that yield significant results for reporting. In other words - NHST, if used correclty, can be a very useful tool. A big problem is that it os often not used correctly, and this can lead to misleading results.</span>
<span id="cb57-135"><a href="#cb57-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-136"><a href="#cb57-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-137"><a href="#cb57-137" aria-hidden="true" tabindex="-1"></a><span class="fu">## Case studies</span></span>
<span id="cb57-138"><a href="#cb57-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-139"><a href="#cb57-139" aria-hidden="true" tabindex="-1"></a>We will briefly explore the frequentist approach to data analysis using two examples:</span>
<span id="cb57-140"><a href="#cb57-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-141"><a href="#cb57-141" aria-hidden="true" tabindex="-1"></a>1) an independent samples t-test, and</span>
<span id="cb57-142"><a href="#cb57-142" aria-hidden="true" tabindex="-1"></a>2) the card game from the previous chapter, and</span>
<span id="cb57-143"><a href="#cb57-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-144"><a href="#cb57-144" aria-hidden="true" tabindex="-1"></a><span class="fu">### T-test {#sec-fancyhat}</span></span>
<span id="cb57-145"><a href="#cb57-145" aria-hidden="true" tabindex="-1"></a>This example uses simulated data, but is based on an actual study. </span>
<span id="cb57-146"><a href="#cb57-146" aria-hidden="true" tabindex="-1"></a>In the code below, we simulate creativity scores from two groups. One group was instructed to wear a fancy hat, the other, woth no head gear, serves as a control group. We would like to know whether the fancy hat group and the control group differ in their mean creativity.</span>
<span id="cb57-147"><a href="#cb57-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-148"><a href="#cb57-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-151"><a href="#cb57-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-152"><a href="#cb57-152" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb57-153"><a href="#cb57-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-154"><a href="#cb57-154" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12</span>)</span>
<span id="cb57-155"><a href="#cb57-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-156"><a href="#cb57-156" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of people wearing fancy hats</span></span>
<span id="cb57-157"><a href="#cb57-157" aria-hidden="true" tabindex="-1"></a>N_fancyhats <span class="ot">&lt;-</span> <span class="dv">50</span> </span>
<span id="cb57-158"><a href="#cb57-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-159"><a href="#cb57-159" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of people not wearing fancy hats</span></span>
<span id="cb57-160"><a href="#cb57-160" aria-hidden="true" tabindex="-1"></a>N_nofancyhats <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb57-161"><a href="#cb57-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-162"><a href="#cb57-162" aria-hidden="true" tabindex="-1"></a><span class="co"># Population mean of creativity for people wearing fancy hats</span></span>
<span id="cb57-163"><a href="#cb57-163" aria-hidden="true" tabindex="-1"></a>mu_fancyhats <span class="ot">&lt;-</span> <span class="dv">103</span> </span>
<span id="cb57-164"><a href="#cb57-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-165"><a href="#cb57-165" aria-hidden="true" tabindex="-1"></a><span class="co"># Population mean of creativity for people wearing no fancy hats</span></span>
<span id="cb57-166"><a href="#cb57-166" aria-hidden="true" tabindex="-1"></a>mu_nofancyhats <span class="ot">&lt;-</span> <span class="dv">98</span> </span>
<span id="cb57-167"><a href="#cb57-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-168"><a href="#cb57-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Average population standard deviation of both groups</span></span>
<span id="cb57-169"><a href="#cb57-169" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">15</span> </span>
<span id="cb57-170"><a href="#cb57-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-171"><a href="#cb57-171" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb57-172"><a href="#cb57-172" aria-hidden="true" tabindex="-1"></a>fancyhats <span class="ot">=</span> <span class="fu">tibble</span>(<span class="at">Creativity =</span> <span class="fu">rnorm</span>(N_fancyhats, mu_fancyhats, sigma),</span>
<span id="cb57-173"><a href="#cb57-173" aria-hidden="true" tabindex="-1"></a>               <span class="at">Group =</span> <span class="st">"Fancy Hat"</span>)</span>
<span id="cb57-174"><a href="#cb57-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-175"><a href="#cb57-175" aria-hidden="true" tabindex="-1"></a>nofancyhats <span class="ot">=</span> <span class="fu">tibble</span>(<span class="at">Creativity =</span> <span class="fu">rnorm</span>(N_nofancyhats, mu_nofancyhats, sigma),</span>
<span id="cb57-176"><a href="#cb57-176" aria-hidden="true" tabindex="-1"></a>                 <span class="at">Group =</span> <span class="st">"No Fancy Hat"</span>)</span>
<span id="cb57-177"><a href="#cb57-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-178"><a href="#cb57-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-179"><a href="#cb57-179" aria-hidden="true" tabindex="-1"></a>FancyHat <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(fancyhats, nofancyhats)  <span class="sc">|&gt;</span></span>
<span id="cb57-180"><a href="#cb57-180" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">Group =</span> <span class="fu">fct_relevel</span>(<span class="fu">as.factor</span>(Group), <span class="st">"No Fancy Hat"</span>))</span>
<span id="cb57-181"><a href="#cb57-181" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-182"><a href="#cb57-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-183"><a href="#cb57-183" aria-hidden="true" tabindex="-1"></a>We have created a dataframe called <span class="in">`FancyHat`</span>. </span>
<span id="cb57-184"><a href="#cb57-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-187"><a href="#cb57-187" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-188"><a href="#cb57-188" aria-hidden="true" tabindex="-1"></a>FancyHat</span>
<span id="cb57-189"><a href="#cb57-189" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-190"><a href="#cb57-190" aria-hidden="true" tabindex="-1"></a>We can now pretend that we don't know how the data were generated, and treat them as data from an experiment.</span>
<span id="cb57-191"><a href="#cb57-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-192"><a href="#cb57-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-193"><a href="#cb57-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-196"><a href="#cb57-196" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-197"><a href="#cb57-197" aria-hidden="true" tabindex="-1"></a>FancyHat <span class="sc">|&gt;</span> </span>
<span id="cb57-198"><a href="#cb57-198" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb57-199"><a href="#cb57-199" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> Creativity, <span class="at">x =</span> Group)) <span class="sc">+</span></span>
<span id="cb57-200"><a href="#cb57-200" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title=</span> <span class="st">"Box Plot of Creativity Values"</span>) <span class="sc">+</span></span>
<span id="cb57-201"><a href="#cb57-201" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb57-202"><a href="#cb57-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-203"><a href="#cb57-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-204"><a href="#cb57-204" aria-hidden="true" tabindex="-1"></a>Assuming that both groups are normally distributed with the same variance, we can use the t-test to determine whether the mean of the two groups is different.</span>
<span id="cb57-205"><a href="#cb57-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-208"><a href="#cb57-208" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-209"><a href="#cb57-209" aria-hidden="true" tabindex="-1"></a>fancyhat_ttest <span class="ot">&lt;-</span> <span class="fu">t.test</span>(Creativity <span class="sc">~</span> Group,</span>
<span id="cb57-210"><a href="#cb57-210" aria-hidden="true" tabindex="-1"></a>       <span class="at">var.equal =</span> <span class="cn">TRUE</span>,</span>
<span id="cb57-211"><a href="#cb57-211" aria-hidden="true" tabindex="-1"></a>       <span class="at">data =</span> FancyHat)</span>
<span id="cb57-212"><a href="#cb57-212" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-213"><a href="#cb57-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-216"><a href="#cb57-216" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-217"><a href="#cb57-217" aria-hidden="true" tabindex="-1"></a>fancyhat_ttest_tab <span class="ot">&lt;-</span> broom<span class="sc">::</span><span class="fu">tidy</span>(fancyhat_ttest)</span>
<span id="cb57-218"><a href="#cb57-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-219"><a href="#cb57-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-222"><a href="#cb57-222" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-223"><a href="#cb57-223" aria-hidden="true" tabindex="-1"></a>fancyhat_ttest_tab <span class="sc">|&gt;</span></span>
<span id="cb57-224"><a href="#cb57-224" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(estimate, estimate1, estimate2, statistic, p.value, conf.low, conf.high) <span class="sc">|&gt;</span></span>
<span id="cb57-225"><a href="#cb57-225" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="dv">3</span>)</span>
<span id="cb57-226"><a href="#cb57-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-227"><a href="#cb57-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-228"><a href="#cb57-228" aria-hidden="true" tabindex="-1"></a>It might not be obvious at first glance what we have done here:</span>
<span id="cb57-229"><a href="#cb57-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-230"><a href="#cb57-230" aria-hidden="true" tabindex="-1"></a>1) We assumed that the data are conditionally normally distributed, given the means, with the same variance.</span>
<span id="cb57-231"><a href="#cb57-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-232"><a href="#cb57-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-233"><a href="#cb57-233" aria-hidden="true" tabindex="-1"></a>y_{ij} \sim \mathcal{N}(\mu_j, \sigma^2)</span>
<span id="cb57-234"><a href="#cb57-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-235"><a href="#cb57-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-236"><a href="#cb57-236" aria-hidden="true" tabindex="-1"></a>2) we estimated three parameters: $\mu_1$, $\mu_2$ ^<span class="co">[</span><span class="ot">`estimate1` und `estimate2`</span><span class="co">]</span>, and $\sigma$.</span>
<span id="cb57-237"><a href="#cb57-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-238"><a href="#cb57-238" aria-hidden="true" tabindex="-1"></a>3) We computed the difference between groups as $\mu_1 - \mu_2$. This gives us an estimate of the difference between the means ^<span class="co">[</span><span class="ot">`estimate`</span><span class="co">]</span>.</span>
<span id="cb57-239"><a href="#cb57-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-240"><a href="#cb57-240" aria-hidden="true" tabindex="-1"></a>4) We compute a test statistic (empirical t value)^<span class="co">[</span><span class="ot">`statistic`</span><span class="co">]</span>.</span>
<span id="cb57-241"><a href="#cb57-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-242"><a href="#cb57-242" aria-hidden="true" tabindex="-1"></a>5) We computed the probability of observing the observed difference divided by the standard error, under the null hypothesis that the means are the same ($\mu_1 = \mu_2$)^<span class="co">[</span><span class="ot">`p.value`</span><span class="co">]</span>. This is a two-sided test, i.e. we have no hypothesis as to which group has the larger mean.</span>
<span id="cb57-243"><a href="#cb57-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-244"><a href="#cb57-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-245"><a href="#cb57-245" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb57-246"><a href="#cb57-246" aria-hidden="true" tabindex="-1"></a>What does the p-value tell us? Does it tell you what you want to know? Do you know what the confidence interval tells you ^<span class="co">[</span><span class="ot">`conf.low` and `conf.high`</span><span class="co">]</span>? </span>
<span id="cb57-247"><a href="#cb57-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-248"><a href="#cb57-248" aria-hidden="true" tabindex="-1"></a>The p-value is <span class="in">`r fancyhat_ttest$p.value |&gt; round(2)`</span> This is larger that the conventional threshold $\alpha=0.05$. What does this mean?</span>
<span id="cb57-249"><a href="#cb57-249" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-250"><a href="#cb57-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-251"><a href="#cb57-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-252"><a href="#cb57-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-253"><a href="#cb57-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Card game</span></span>
<span id="cb57-254"><a href="#cb57-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-255"><a href="#cb57-255" aria-hidden="true" tabindex="-1"></a>Two players are playing a game of cards. You observe that they play 9 games, and that player A wins 6 of those. Now you would like to estimate the probability that player A will win the next game. Another way of putting it that you would to estimate the ability of player A to beat player B at this particular game.</span>
<span id="cb57-256"><a href="#cb57-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-257"><a href="#cb57-257" aria-hidden="true" tabindex="-1"></a>You know that the probability of success must lie in the range $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$. What you might not be aware of is that you are assuming a certain probability model, and the probability of success is a parameter of that model. Let's take a closer look:</span>
<span id="cb57-258"><a href="#cb57-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-259"><a href="#cb57-259" aria-hidden="true" tabindex="-1"></a>We know that the number of $k$ successes in $n$ games follows a binomial distribution with parameters $n$ and $\theta$. To make things simpler, we also know that each individual game is independent of the others, and the probability of success $\theta$ is the same for each game. Each success therefore follows a Bernoulli distribution with parameter $\theta$.</span>
<span id="cb57-260"><a href="#cb57-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-261"><a href="#cb57-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-262"><a href="#cb57-262" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb57-263"><a href="#cb57-263" aria-hidden="true" tabindex="-1"></a>y_i \sim \mathcal{Bernoulli}(\theta)</span>
<span id="cb57-264"><a href="#cb57-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-265"><a href="#cb57-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-266"><a href="#cb57-266" aria-hidden="true" tabindex="-1"></a>:::{callout-tip}</span>
<span id="cb57-267"><a href="#cb57-267" aria-hidden="true" tabindex="-1"></a>I will generally use the notation $y$ for a variable that is observed, i.e. the data.</span>
<span id="cb57-268"><a href="#cb57-268" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-269"><a href="#cb57-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-270"><a href="#cb57-270" aria-hidden="true" tabindex="-1"></a>$y_i$ is the $i$ th observation in the data, meaning that it tells us whether player A won the game or not on trial $i$. $\theta$ is the probability of success for each individual game; this is a parameter of our model ($\mathcal{M}$).</span>
<span id="cb57-271"><a href="#cb57-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-272"><a href="#cb57-272" aria-hidden="true" tabindex="-1"></a>Previously, we used maximum likelihood to estimate $\theta$ - we will repeat this briefly here.</span>
<span id="cb57-273"><a href="#cb57-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-276"><a href="#cb57-276" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-277"><a href="#cb57-277" aria-hidden="true" tabindex="-1"></a>wins <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb57-278"><a href="#cb57-278" aria-hidden="true" tabindex="-1"></a>games <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb57-279"><a href="#cb57-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-280"><a href="#cb57-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-281"><a href="#cb57-281" aria-hidden="true" tabindex="-1"></a>The goal is to figure out the "best" value of $\theta$, i.e. the value that maximizes the likelihood of observing the data. To do this, we need to consider a range of possible values of $\theta$ (we already know that this range is $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$, so that part is easy). We will consider 100 values of $\theta$ between 0 and 1, and compute the likelihood of observing the data for each value of $\theta$.</span>
<span id="cb57-282"><a href="#cb57-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-285"><a href="#cb57-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-286"><a href="#cb57-286" aria-hidden="true" tabindex="-1"></a>n_points <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb57-287"><a href="#cb57-287" aria-hidden="true" tabindex="-1"></a>theta_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>( <span class="at">from=</span><span class="dv">0</span> , <span class="at">to=</span><span class="dv">1</span> , <span class="at">length.out =</span> n_points )</span>
<span id="cb57-288"><a href="#cb57-288" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-289"><a href="#cb57-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-290"><a href="#cb57-290" aria-hidden="true" tabindex="-1"></a>Assuming that both players have an equal chance of winning, the parameter should be $\theta = 0.5$. The probability of the data given $\theta = 0.5$ is:</span>
<span id="cb57-291"><a href="#cb57-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-294"><a href="#cb57-294" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-295"><a href="#cb57-295" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="at">x =</span> wins, <span class="at">size =</span> games, <span class="at">prob =</span> <span class="fl">0.5</span>)</span>
<span id="cb57-296"><a href="#cb57-296" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-297"><a href="#cb57-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-298"><a href="#cb57-298" aria-hidden="true" tabindex="-1"></a>The probability of winning 6 out of 9 games, given that both players are equally likely to win, is <span class="in">`r dbinom(x = wins, size = games, prob = 0.5)`</span>.</span>
<span id="cb57-299"><a href="#cb57-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-300"><a href="#cb57-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-301"><a href="#cb57-301" aria-hidden="true" tabindex="-1"></a>We can also compute the probability of A winning 6, 7, 8 or 9 games, using the cumulative distribution function of the binomial distribution.</span>
<span id="cb57-302"><a href="#cb57-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-305"><a href="#cb57-305" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-306"><a href="#cb57-306" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pbinom</span>(<span class="at">q =</span> <span class="dv">5</span>, <span class="at">size =</span> games, <span class="at">prob =</span> <span class="fl">0.5</span>)</span>
<span id="cb57-307"><a href="#cb57-307" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-308"><a href="#cb57-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-309"><a href="#cb57-309" aria-hidden="true" tabindex="-1"></a>or</span>
<span id="cb57-310"><a href="#cb57-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-313"><a href="#cb57-313" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-314"><a href="#cb57-314" aria-hidden="true" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="at">q =</span> <span class="dv">5</span>, <span class="at">size =</span> games, <span class="at">prob =</span> <span class="fl">0.5</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb57-315"><a href="#cb57-315" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-316"><a href="#cb57-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-317"><a href="#cb57-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-318"><a href="#cb57-318" aria-hidden="true" tabindex="-1"></a>:::aside</span>
<span id="cb57-319"><a href="#cb57-319" aria-hidden="true" tabindex="-1"></a><span class="in">`pbinom()`</span> gives us the lower tail probability, which is the probability that the number of successes is less than or equal to the given value, by default.</span>
<span id="cb57-320"><a href="#cb57-320" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-321"><a href="#cb57-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-322"><a href="#cb57-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-323"><a href="#cb57-323" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb57-324"><a href="#cb57-324" aria-hidden="true" tabindex="-1"></a><span class="fu">## p-value</span></span>
<span id="cb57-325"><a href="#cb57-325" aria-hidden="true" tabindex="-1"></a>Does this seem familiar?</span>
<span id="cb57-326"><a href="#cb57-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-327"><a href="#cb57-327" aria-hidden="true" tabindex="-1"></a>If we want to quantify our _null_ hypothsis that both players are equally likely to win, we would assume that $\theta=0.5$. Computing the probability of the data under the null is exactly what we have just done. We then plug in the actual data, i.e. 6 out of 9, and the upper tail probability is the p-value. In this case, the p-value is approximately $0.25$. Using cut-off of 0.05, we would not reject the null hypothesis, and conclude that there is not enough evidence that player A is better than player B (this is a one-sided test).</span>
<span id="cb57-328"><a href="#cb57-328" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-329"><a href="#cb57-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-330"><a href="#cb57-330" aria-hidden="true" tabindex="-1"></a>Now we compute the probability of the data under all parameter values under consideration. In R, this is very simple, since all functions are vectorized.</span>
<span id="cb57-331"><a href="#cb57-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-334"><a href="#cb57-334" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-335"><a href="#cb57-335" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(wins , <span class="at">size =</span> games , <span class="at">prob =</span> theta_grid)</span>
<span id="cb57-336"><a href="#cb57-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-337"><a href="#cb57-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-340"><a href="#cb57-340" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-341"><a href="#cb57-341" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(likelihood)</span>
<span id="cb57-342"><a href="#cb57-342" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-343"><a href="#cb57-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-344"><a href="#cb57-344" aria-hidden="true" tabindex="-1"></a>We can see in the above figure that the probability of observing the data is small for a lot of values of $\theta$. The probability of observing the data, or the likelihood, is maximal for the value <span class="in">`r 6/9`</span>:</span>
<span id="cb57-345"><a href="#cb57-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-346"><a href="#cb57-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-349"><a href="#cb57-349" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-350"><a href="#cb57-350" aria-hidden="true" tabindex="-1"></a>theta_grid[<span class="fu">which.max</span>(likelihood)]</span>
<span id="cb57-351"><a href="#cb57-351" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-352"><a href="#cb57-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-353"><a href="#cb57-353" aria-hidden="true" tabindex="-1"></a>What we have done so far highlights the distinction between parameter estimation and hypothesis testing. Computing the tail probability under the null ($\theta=0.5$) is a hypothesis test, and estimating $\theta$ is parameter estimation.</span>
<span id="cb57-354"><a href="#cb57-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-355"><a href="#cb57-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-356"><a href="#cb57-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-357"><a href="#cb57-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-358"><a href="#cb57-358" aria-hidden="true" tabindex="-1"></a><span class="fu">## An introduction to Bayesian inference</span></span>
<span id="cb57-359"><a href="#cb57-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-360"><a href="#cb57-360" aria-hidden="true" tabindex="-1"></a>So far, we haven't considered any prior knowledge we might have had about which parameters are the most likely a priori. In fact, as we will see a bit further down, we have implicitly assumed that all parameters are equally likely. We will now introduce a new concept: a prior distribution for the parameter(s) we are trying to estimate^<span class="co">[</span><span class="ot">In frequentist statistics, the concept is meaningless - parameters cannot have distribution. In Bayesian statistics, a prior distribution should reflect everything we know about the parameter, before we consider the data. The prior reflects our belief, which can be subjective, or objective.</span><span class="co">]</span>.</span>
<span id="cb57-361"><a href="#cb57-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-362"><a href="#cb57-362" aria-hidden="true" tabindex="-1"></a>We will then use that __prior__ belief in order to obtain a __posterior__ belief</span>
<span id="cb57-363"><a href="#cb57-363" aria-hidden="true" tabindex="-1"></a>over the possible parameter values. To do this, we need to multiply the prior probability of each parameter value by the likelihood of the data, i.e. by the probability of observing the data given that parameter value. This is an application of Bayes theorem:</span>
<span id="cb57-364"><a href="#cb57-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-365"><a href="#cb57-365" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb57-366"><a href="#cb57-366" aria-hidden="true" tabindex="-1"></a>p(\theta|y) = \frac{ p(y|\theta) * p(\theta) } {p(y)}</span>
<span id="cb57-367"><a href="#cb57-367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-368"><a href="#cb57-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-369"><a href="#cb57-369" aria-hidden="true" tabindex="-1"></a>This states that the posterior probability of $\theta$ given the observed data $y$ is equal to the probability of the data, multiplied by how probable each value of $\theta$ is a priori. You can think of it like this: each parameter value is weighted according to how well it predicts the data. The product $p(y|\theta) * p(\theta)$ is then divided by the probability of the data, which in this case is summed over all possible parameter values. This step serves to normalize the posterior, so that it sums to $1$. This essentially turns the unnormalized posterior into a proper probability distribution.</span>
<span id="cb57-370"><a href="#cb57-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-371"><a href="#cb57-371" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-372"><a href="#cb57-372" aria-hidden="true" tabindex="-1"></a>p(y) = \sum_{\theta}p(y|\theta) * p(\theta) </span>
<span id="cb57-373"><a href="#cb57-373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-374"><a href="#cb57-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-375"><a href="#cb57-375" aria-hidden="true" tabindex="-1"></a>When we are interested in estimating the parameters of a given model, we can often neglect the (constant for a model) normalizing term $p(y)$. This term, often called the evidence, reflects the prbability of the data, averaged over all parameter values. Written without the normalizing constant, Bayes rule is often written as:</span>
<span id="cb57-376"><a href="#cb57-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-377"><a href="#cb57-377" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb57-378"><a href="#cb57-378" aria-hidden="true" tabindex="-1"></a>p(\theta|y) \propto  p(y|\theta) * p(\theta) </span>
<span id="cb57-379"><a href="#cb57-379" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-380"><a href="#cb57-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-381"><a href="#cb57-381" aria-hidden="true" tabindex="-1"></a><span class="fu">## Recap </span></span>
<span id="cb57-382"><a href="#cb57-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-383"><a href="#cb57-383" aria-hidden="true" tabindex="-1"></a>Bayesian inference, in a nutshell, consists of:</span>
<span id="cb57-384"><a href="#cb57-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-385"><a href="#cb57-385" aria-hidden="true" tabindex="-1"></a>1) Represent your prior belief by a probability distribution over the possible parameter values. This is a principled way of dealing with uncertainty.</span>
<span id="cb57-386"><a href="#cb57-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-387"><a href="#cb57-387" aria-hidden="true" tabindex="-1"></a>2) Use the likelihood to weight the prior belief.</span>
<span id="cb57-388"><a href="#cb57-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-389"><a href="#cb57-389" aria-hidden="true" tabindex="-1"></a>3) Obtain a posterior belief over the possible parameter values.</span>
<span id="cb57-390"><a href="#cb57-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-391"><a href="#cb57-391" aria-hidden="true" tabindex="-1"></a>:::aside</span>
<span id="cb57-392"><a href="#cb57-392" aria-hidden="true" tabindex="-1"></a>The term _belief_ is used synonymously with _probability distribution_.</span>
<span id="cb57-393"><a href="#cb57-393" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-394"><a href="#cb57-394" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb57-395"><a href="#cb57-395" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian inference in the card game: a numerical example</span></span>
<span id="cb57-396"><a href="#cb57-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-397"><a href="#cb57-397" aria-hidden="true" tabindex="-1"></a>Recall that we defined a sequence of 100 points between 0 and 1, which represented the possible $\theta$ values.</span>
<span id="cb57-398"><a href="#cb57-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-401"><a href="#cb57-401" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-402"><a href="#cb57-402" aria-hidden="true" tabindex="-1"></a>n_points <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb57-403"><a href="#cb57-403" aria-hidden="true" tabindex="-1"></a>theta_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>( <span class="at">from=</span><span class="dv">0</span> , <span class="at">to=</span><span class="dv">1</span> , <span class="at">length.out =</span> n_points )</span>
<span id="cb57-404"><a href="#cb57-404" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-405"><a href="#cb57-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-406"><a href="#cb57-406" aria-hidden="true" tabindex="-1"></a>For each of these, we computed the likelihood, that is the probability of observing the (fixed) data, given the parameter. Now, we can make our knowledge about the probability of each parameter value explicit. At first, we will assume that all parameters are equally likely. We will assign the probability of 1 to each parameter value. This is our prior distribution. </span>
<span id="cb57-407"><a href="#cb57-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-408"><a href="#cb57-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-411"><a href="#cb57-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-412"><a href="#cb57-412" aria-hidden="true" tabindex="-1"></a>prior_1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">length</span>(theta_grid))</span>
<span id="cb57-413"><a href="#cb57-413" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-414"><a href="#cb57-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-417"><a href="#cb57-417" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-418"><a href="#cb57-418" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta_grid, prior_1, <span class="st">"type"</span> <span class="ot">=</span> <span class="st">"l"</span>)</span>
<span id="cb57-419"><a href="#cb57-419" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-420"><a href="#cb57-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-421"><a href="#cb57-421" aria-hidden="true" tabindex="-1"></a>We could also express the belief that player is at least as good as player B, i.e. they are equally good or A is better than B. One way of doing this is to assign a probability of $1$ to parameter values greater than or equal to $0.5$, and the value $0$ to parameter values less than $0.5$.</span>
<span id="cb57-422"><a href="#cb57-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-425"><a href="#cb57-425" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-426"><a href="#cb57-426" aria-hidden="true" tabindex="-1"></a>prior_2 <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(theta_grid <span class="sc">&lt;</span> <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb57-427"><a href="#cb57-427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-428"><a href="#cb57-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-431"><a href="#cb57-431" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-432"><a href="#cb57-432" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta_grid, prior_2, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb57-433"><a href="#cb57-433" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-434"><a href="#cb57-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-435"><a href="#cb57-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-436"><a href="#cb57-436" aria-hidden="true" tabindex="-1"></a>A more systematic way of doing this is to use a parameterized probability distribution that expresses our beliefs about the parameter. </span>
<span id="cb57-437"><a href="#cb57-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-438"><a href="#cb57-438" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb57-439"><a href="#cb57-439" aria-hidden="true" tabindex="-1"></a>A family of probability distributions that are suitable for parameters that lie in the interval $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ is the <span class="co">[</span><span class="ot">Beta distribution</span><span class="co">](https://en.wikipedia.org/wiki/Beta_distribution)</span>. This distribution has $2$ parameters $\alpha$ und $\beta$, </span>
<span id="cb57-440"><a href="#cb57-440" aria-hidden="true" tabindex="-1"></a>which can be interpreted as the prior number of successes and the number of failures, respectively. The number of trials is therefore $\alpha + \beta$. @ref(fig:betadists) shows a number of possible Beta distributions for various settings of $\alpha$ and $\beta$.</span>
<span id="cb57-441"><a href="#cb57-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-442"><a href="#cb57-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-445"><a href="#cb57-445" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-446"><a href="#cb57-446" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: betadist</span></span>
<span id="cb57-447"><a href="#cb57-447" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Beta distributions"</span></span>
<span id="cb57-448"><a href="#cb57-448" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb57-449"><a href="#cb57-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-450"><a href="#cb57-450" aria-hidden="true" tabindex="-1"></a>length <span class="ot">&lt;-</span> <span class="fl">1e4</span></span>
<span id="cb57-451"><a href="#cb57-451" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">crossing</span>(<span class="at">shape1 =</span> <span class="fu">c</span>(.<span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>),</span>
<span id="cb57-452"><a href="#cb57-452" aria-hidden="true" tabindex="-1"></a>           <span class="at">shape2 =</span> <span class="fu">c</span>(.<span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)) <span class="sc">|&gt;</span></span>
<span id="cb57-453"><a href="#cb57-453" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">expand</span>(<span class="fu">nesting</span>(shape1, shape2),</span>
<span id="cb57-454"><a href="#cb57-454" aria-hidden="true" tabindex="-1"></a>         <span class="at">x =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> length)) <span class="sc">|&gt;</span> </span>
<span id="cb57-455"><a href="#cb57-455" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">a =</span> <span class="fu">str_c</span>(<span class="st">"a = "</span>, shape1),</span>
<span id="cb57-456"><a href="#cb57-456" aria-hidden="true" tabindex="-1"></a>         <span class="at">b =</span> <span class="fu">str_c</span>(<span class="st">"b = "</span>, shape2),</span>
<span id="cb57-457"><a href="#cb57-457" aria-hidden="true" tabindex="-1"></a>         <span class="at">group =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>length, <span class="at">each =</span> <span class="dv">25</span>))</span>
<span id="cb57-458"><a href="#cb57-458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-461"><a href="#cb57-461" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-462"><a href="#cb57-462" aria-hidden="true" tabindex="-1"></a>d <span class="sc">|&gt;</span> </span>
<span id="cb57-463"><a href="#cb57-463" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">group =</span> group)) <span class="sc">+</span></span>
<span id="cb57-464"><a href="#cb57-464" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-465"><a href="#cb57-465" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">dbeta</span>(x, <span class="at">shape1 =</span> shape1, <span class="at">shape2 =</span> shape2)),</span>
<span id="cb57-466"><a href="#cb57-466" aria-hidden="true" tabindex="-1"></a>            <span class="at">color =</span> <span class="st">"steelblue4"</span>, <span class="at">size =</span> <span class="fl">1.1</span>) <span class="sc">+</span></span>
<span id="cb57-467"><a href="#cb57-467" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="fu">expression</span>(theta), <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">5</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb57-468"><a href="#cb57-468" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>)) <span class="sc">+</span></span>
<span id="cb57-469"><a href="#cb57-469" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Beta distributions"</span>,</span>
<span id="cb57-470"><a href="#cb57-470" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">p</span>(theta<span class="sc">*</span><span class="st">"|"</span><span class="sc">*</span>a<span class="sc">*</span><span class="st">", "</span><span class="sc">*</span>b))) <span class="sc">+</span></span>
<span id="cb57-471"><a href="#cb57-471" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">panel.grid =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb57-472"><a href="#cb57-472" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(b<span class="sc">~</span>a)</span>
<span id="cb57-473"><a href="#cb57-473" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-474"><a href="#cb57-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-475"><a href="#cb57-475" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-476"><a href="#cb57-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-477"><a href="#cb57-477" aria-hidden="true" tabindex="-1"></a>If we want to use a Beta distribution to express the belief that all values of $\theta$ are equally likely (uniform prior), we can a Beta distribution with $\alpha = 1$ and $\beta = 1$.</span>
<span id="cb57-478"><a href="#cb57-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-481"><a href="#cb57-481" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-482"><a href="#cb57-482" aria-hidden="true" tabindex="-1"></a>prior_3 <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(<span class="at">x =</span> theta_grid, <span class="at">shape1 =</span> <span class="dv">1</span>, <span class="at">shape2 =</span> <span class="dv">1</span>)</span>
<span id="cb57-483"><a href="#cb57-483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-484"><a href="#cb57-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-487"><a href="#cb57-487" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-488"><a href="#cb57-488" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta_grid, prior_3, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb57-489"><a href="#cb57-489" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-490"><a href="#cb57-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-491"><a href="#cb57-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-492"><a href="#cb57-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-493"><a href="#cb57-493" aria-hidden="true" tabindex="-1"></a>Finally, we could express the following prior information as a Beta distribution: imagine you had previously observed $100$ games between A and B, and each won half of the games. You can set $\alpha$ and $\beta$ to the number of games won by A, and the number of games won by B, respectively.  A won $50$, and B won $50$:</span>
<span id="cb57-494"><a href="#cb57-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-497"><a href="#cb57-497" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-498"><a href="#cb57-498" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(<span class="at">x =</span> theta_grid, <span class="at">shape1 =</span> <span class="dv">50</span>, <span class="at">shape2 =</span> <span class="dv">50</span>)</span>
<span id="cb57-499"><a href="#cb57-499" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-500"><a href="#cb57-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-503"><a href="#cb57-503" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-504"><a href="#cb57-504" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta_grid, prior, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb57-505"><a href="#cb57-505" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-506"><a href="#cb57-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-507"><a href="#cb57-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-508"><a href="#cb57-508" aria-hidden="true" tabindex="-1"></a>Now, we can combine the prior and the likelihood by multiplying them element-wise, i.e. we need to multiply each parameter value with the probability of the data given that parameter. </span>
<span id="cb57-509"><a href="#cb57-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-510"><a href="#cb57-510" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb57-511"><a href="#cb57-511" aria-hidden="true" tabindex="-1"></a>Recall Bayes theorem:</span>
<span id="cb57-512"><a href="#cb57-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-513"><a href="#cb57-513" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-514"><a href="#cb57-514" aria-hidden="true" tabindex="-1"></a>p(\theta|y) = \frac{ p(y|\theta) * p(\theta) } {p(y)}</span>
<span id="cb57-515"><a href="#cb57-515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-516"><a href="#cb57-516" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-517"><a href="#cb57-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-518"><a href="#cb57-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-519"><a href="#cb57-519" aria-hidden="true" tabindex="-1"></a>In R, this is simply:</span>
<span id="cb57-520"><a href="#cb57-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-521"><a href="#cb57-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-524"><a href="#cb57-524" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-525"><a href="#cb57-525" aria-hidden="true" tabindex="-1"></a>wins <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb57-526"><a href="#cb57-526" aria-hidden="true" tabindex="-1"></a>games <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb57-527"><a href="#cb57-527" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-528"><a href="#cb57-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-531"><a href="#cb57-531" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-532"><a href="#cb57-532" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(<span class="at">x =</span> theta_grid, <span class="at">shape1 =</span> <span class="dv">4</span>, <span class="at">shape2 =</span> <span class="dv">4</span>)</span>
<span id="cb57-533"><a href="#cb57-533" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(wins , <span class="at">size =</span> games , <span class="at">prob =</span> theta_grid)</span>
<span id="cb57-534"><a href="#cb57-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-535"><a href="#cb57-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-536"><a href="#cb57-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-539"><a href="#cb57-539" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-540"><a href="#cb57-540" aria-hidden="true" tabindex="-1"></a>unstandardized_posterior <span class="ot">&lt;-</span> likelihood <span class="sc">*</span> prior</span>
<span id="cb57-541"><a href="#cb57-541" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-542"><a href="#cb57-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-543"><a href="#cb57-543" aria-hidden="true" tabindex="-1"></a>This gives us the unnormalized posterior:</span>
<span id="cb57-544"><a href="#cb57-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-545"><a href="#cb57-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-546"><a href="#cb57-546" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb57-547"><a href="#cb57-547" aria-hidden="true" tabindex="-1"></a>p(\theta|y) \propto  p(y|\theta) * p(\theta)</span>
<span id="cb57-548"><a href="#cb57-548" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb57-549"><a href="#cb57-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-550"><a href="#cb57-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-551"><a href="#cb57-551" aria-hidden="true" tabindex="-1"></a>We can then normalize the posterior distribution by dividing it by $p(y) = \sum_{\theta}p(y|\theta) * p(\theta)$. In R we can use the <span class="in">`sum()`</span> function: <span class="in">`sum(unstandardized_posterior)`</span>.</span>
<span id="cb57-552"><a href="#cb57-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-553"><a href="#cb57-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-556"><a href="#cb57-556" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-557"><a href="#cb57-557" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> unstandardized_posterior <span class="sc">/</span> <span class="fu">sum</span>(unstandardized_posterior)</span>
<span id="cb57-558"><a href="#cb57-558" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-559"><a href="#cb57-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-560"><a href="#cb57-560" aria-hidden="true" tabindex="-1"></a>We can now plot the resulting normalized posterior distribution. </span>
<span id="cb57-561"><a href="#cb57-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-564"><a href="#cb57-564" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-565"><a href="#cb57-565" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta_grid, posterior, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>, <span class="at">ylab =</span> <span class="st">'Probability'</span>, </span>
<span id="cb57-566"><a href="#cb57-566" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Posterior"</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>, <span class="at">cex.main =</span> <span class="dv">3</span>)</span>
<span id="cb57-567"><a href="#cb57-567" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-568"><a href="#cb57-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-569"><a href="#cb57-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-570"><a href="#cb57-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-571"><a href="#cb57-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-572"><a href="#cb57-572" aria-hidden="true" tabindex="-1"></a>To make this repeatable, we will write two functions. The first, <span class="in">`compute_posterior()`</span>, will compute the posterior, given the prior and likelihood, and return a dataframe containg prior, likelihood and posterior. The second, <span class="in">`plot_posterior()`</span>, will plot all three side-by-side. You can also pass in the maximum likelihood estimate, e.g. <span class="in">`6/9`</span>, and this will be plotted as well.</span>
<span id="cb57-573"><a href="#cb57-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-576"><a href="#cb57-576" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-577"><a href="#cb57-577" aria-hidden="true" tabindex="-1"></a>compute_posterior <span class="ot">=</span> <span class="cf">function</span>(likelihood, prior){</span>
<span id="cb57-578"><a href="#cb57-578" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute product of likelihood and prior</span></span>
<span id="cb57-579"><a href="#cb57-579" aria-hidden="true" tabindex="-1"></a>  unstandardized_posterior <span class="ot">&lt;-</span> likelihood <span class="sc">*</span> prior</span>
<span id="cb57-580"><a href="#cb57-580" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-581"><a href="#cb57-581" aria-hidden="true" tabindex="-1"></a>  <span class="co"># standardize the posterior, so it sums to 1</span></span>
<span id="cb57-582"><a href="#cb57-582" aria-hidden="true" tabindex="-1"></a>  posterior <span class="ot">&lt;-</span> unstandardized_posterior <span class="sc">/</span> <span class="fu">sum</span>(unstandardized_posterior)</span>
<span id="cb57-583"><a href="#cb57-583" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-584"><a href="#cb57-584" aria-hidden="true" tabindex="-1"></a>  out <span class="ot">&lt;-</span> <span class="fu">tibble</span>(prior, likelihood, posterior)</span>
<span id="cb57-585"><a href="#cb57-585" aria-hidden="true" tabindex="-1"></a>  out</span>
<span id="cb57-586"><a href="#cb57-586" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb57-587"><a href="#cb57-587" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-588"><a href="#cb57-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-591"><a href="#cb57-591" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-592"><a href="#cb57-592" aria-hidden="true" tabindex="-1"></a>plot_posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(df, <span class="at">mle =</span> <span class="dv">6</span><span class="sc">/</span><span class="dv">9</span>){</span>
<span id="cb57-593"><a href="#cb57-593" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(df, {</span>
<span id="cb57-594"><a href="#cb57-594" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb57-595"><a href="#cb57-595" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(theta_grid , prior, <span class="at">type=</span><span class="st">"l"</span>, <span class="at">main=</span><span class="st">"Prior"</span>, <span class="at">col =</span> <span class="st">"dodgerblue3"</span>, </span>
<span id="cb57-596"><a href="#cb57-596" aria-hidden="true" tabindex="-1"></a>            <span class="at">lwd =</span> <span class="dv">4</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>, <span class="at">ylab =</span> <span class="st">'Probability'</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>, <span class="at">cex.main =</span> <span class="dv">3</span>)</span>
<span id="cb57-597"><a href="#cb57-597" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(theta_grid , likelihood, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"Likelihood"</span>, <span class="at">col =</span> <span class="st">"firebrick3"</span>, </span>
<span id="cb57-598"><a href="#cb57-598" aria-hidden="true" tabindex="-1"></a>            <span class="at">lwd =</span> <span class="dv">4</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>, <span class="at">ylab =</span> <span class="st">''</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>, <span class="at">cex.main =</span> <span class="dv">3</span>)</span>
<span id="cb57-599"><a href="#cb57-599" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(theta_grid , posterior , <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"Posterior"</span>, <span class="at">col =</span> <span class="st">"darkorchid3"</span>, </span>
<span id="cb57-600"><a href="#cb57-600" aria-hidden="true" tabindex="-1"></a>            <span class="at">lwd =</span> <span class="dv">4</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>, <span class="at">ylab =</span> <span class="st">''</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>, <span class="at">cex.main =</span> <span class="dv">3</span>)</span>
<span id="cb57-601"><a href="#cb57-601" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v =</span> mle, <span class="at">col =</span> <span class="dv">4</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb57-602"><a href="#cb57-602" aria-hidden="true" tabindex="-1"></a>  } )</span>
<span id="cb57-603"><a href="#cb57-603" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb57-604"><a href="#cb57-604" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-605"><a href="#cb57-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-606"><a href="#cb57-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-607"><a href="#cb57-607" aria-hidden="true" tabindex="-1"></a>You can now try out various prior distributions, and observe the effect on the posterior.</span>
<span id="cb57-608"><a href="#cb57-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-609"><a href="#cb57-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-610"><a href="#cb57-610" aria-hidden="true" tabindex="-1"></a>Let's try out a uniform prior first:</span>
<span id="cb57-611"><a href="#cb57-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-614"><a href="#cb57-614" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-615"><a href="#cb57-615" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(<span class="at">x =</span> theta_grid, <span class="at">shape1 =</span> <span class="dv">1</span>, <span class="at">shape2 =</span> <span class="dv">1</span>)</span>
<span id="cb57-616"><a href="#cb57-616" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(wins , <span class="at">size =</span> games , <span class="at">prob =</span> theta_grid)</span>
<span id="cb57-617"><a href="#cb57-617" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-618"><a href="#cb57-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-619"><a href="#cb57-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-620"><a href="#cb57-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-623"><a href="#cb57-623" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-624"><a href="#cb57-624" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">compute_posterior</span>(likelihood, prior)</span>
<span id="cb57-625"><a href="#cb57-625" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-626"><a href="#cb57-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-627"><a href="#cb57-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-630"><a href="#cb57-630" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-631"><a href="#cb57-631" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_posterior</span>(df)</span>
<span id="cb57-632"><a href="#cb57-632" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-633"><a href="#cb57-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-634"><a href="#cb57-634" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb57-635"><a href="#cb57-635" aria-hidden="true" tabindex="-1"></a>In this case, the maximum likelihood estimate coincides with the value that maximizes the posterior probability. This is the case because maximum likelihood estimate uses only the likelihood and does not consider prior knowledge. If we are are using a uniform prior, that amounts to saying that all values of $\theta$ are equally likely. In other words, we have no information as which values are more probable.z</span>
<span id="cb57-636"><a href="#cb57-636" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-637"><a href="#cb57-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-638"><a href="#cb57-638" aria-hidden="true" tabindex="-1"></a>Now let's try out our prior expressing the belief that player A cannot be worse than player B:</span>
<span id="cb57-639"><a href="#cb57-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-642"><a href="#cb57-642" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-643"><a href="#cb57-643" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(theta_grid <span class="sc">&lt;</span> <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb57-644"><a href="#cb57-644" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(wins , <span class="at">size =</span> games , <span class="at">prob =</span> theta_grid)</span>
<span id="cb57-645"><a href="#cb57-645" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-646"><a href="#cb57-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-647"><a href="#cb57-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-648"><a href="#cb57-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-651"><a href="#cb57-651" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-652"><a href="#cb57-652" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">compute_posterior</span>(likelihood, prior)</span>
<span id="cb57-653"><a href="#cb57-653" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-654"><a href="#cb57-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-657"><a href="#cb57-657" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-658"><a href="#cb57-658" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_posterior</span>(df)</span>
<span id="cb57-659"><a href="#cb57-659" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-660"><a href="#cb57-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-661"><a href="#cb57-661" aria-hidden="true" tabindex="-1"></a>The resulting posterior distribution is zero for all values of $\theta$ that are less than 0.5. This is because our prior allocates zero probability those values.</span>
<span id="cb57-662"><a href="#cb57-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-663"><a href="#cb57-663" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb57-664"><a href="#cb57-664" aria-hidden="true" tabindex="-1"></a>Try out various priors. How do they affect the posterior?</span>
<span id="cb57-665"><a href="#cb57-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-666"><a href="#cb57-666" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A prior exspressing the belief that B is better than A.</span>
<span id="cb57-667"><a href="#cb57-667" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A prior expressing that belief that either A is a lot better, or B is a lot better.</span>
<span id="cb57-668"><a href="#cb57-668" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A prior expressing the belief that the players are probably as good as each other, and that it is unlikely that either is much better than the other.</span>
<span id="cb57-669"><a href="#cb57-669" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-670"><a href="#cb57-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-671"><a href="#cb57-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-672"><a href="#cb57-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-673"><a href="#cb57-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-674"><a href="#cb57-674" aria-hidden="true" tabindex="-1"></a>The posterior represents our belief about the the possible parameter values, after having observed the data. You can therefore think of Bayesian inference as a method for updating your beliefs, conditional on observed data. You are re-allocating probabilities over parameter values, depending on how well those parameter values predicted the data.</span>
<span id="cb57-675"><a href="#cb57-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-676"><a href="#cb57-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-677"><a href="#cb57-677" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summarizing the posterior</span></span>
<span id="cb57-678"><a href="#cb57-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-679"><a href="#cb57-679" aria-hidden="true" tabindex="-1"></a>We now need one last step: we need to summarize the posterior distribution. We can do this, for example by computing the mean and standard deviation of the posterior distribution. However, the method we have looked at so far is very simple, and only works for single parameters with a well-defined range. For real-world inference problems, we will numerical methods to approximate the posterior distribution. These methods are know collectively as *Monte Carlo sampling*, or *Markov Chain Monte Carlo* (MCMC). Using MCMC, we will not obtain an analytical description of posterior distributions, but instead a collection of random numbers (samples), that are drawn from the posterior distribution. We will use these samples to represent the posterior distribution.</span>
<span id="cb57-680"><a href="#cb57-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-681"><a href="#cb57-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-682"><a href="#cb57-682" aria-hidden="true" tabindex="-1"></a>To show how this works, we can draw a thousand samples from the posterior distribution. First, we'll create a posterior.</span>
<span id="cb57-683"><a href="#cb57-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-686"><a href="#cb57-686" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-687"><a href="#cb57-687" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(<span class="at">x =</span> theta_grid, <span class="at">shape1 =</span> <span class="dv">1</span>, <span class="at">shape2 =</span> <span class="dv">1</span>)</span>
<span id="cb57-688"><a href="#cb57-688" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(wins, <span class="at">size =</span> games , <span class="at">prob =</span> theta_grid)</span>
<span id="cb57-689"><a href="#cb57-689" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-690"><a href="#cb57-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-691"><a href="#cb57-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-694"><a href="#cb57-694" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-695"><a href="#cb57-695" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">compute_posterior</span>(likelihood, prior)</span>
<span id="cb57-696"><a href="#cb57-696" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-697"><a href="#cb57-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-700"><a href="#cb57-700" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-701"><a href="#cb57-701" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_posterior</span>(df)</span>
<span id="cb57-702"><a href="#cb57-702" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-703"><a href="#cb57-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-704"><a href="#cb57-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-705"><a href="#cb57-705" aria-hidden="true" tabindex="-1"></a>Now we'll draw 1000 random numbers from the posterior.</span>
<span id="cb57-706"><a href="#cb57-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-709"><a href="#cb57-709" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-710"><a href="#cb57-710" aria-hidden="true" tabindex="-1"></a>n_samples <span class="ot">&lt;-</span> <span class="fl">1e3</span></span>
<span id="cb57-711"><a href="#cb57-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-712"><a href="#cb57-712" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> theta_grid <span class="sc">|&gt;</span> <span class="fu">sample</span>(<span class="at">size =</span> n_samples, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> df<span class="sc">$</span>posterior)</span>
<span id="cb57-713"><a href="#cb57-713" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-714"><a href="#cb57-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-717"><a href="#cb57-717" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-718"><a href="#cb57-718" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(samples, <span class="dv">10</span>)</span>
<span id="cb57-719"><a href="#cb57-719" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-720"><a href="#cb57-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-721"><a href="#cb57-721" aria-hidden="true" tabindex="-1"></a>Now we can summarize the samples, e.g. by computing the mean or quantiles.</span>
<span id="cb57-722"><a href="#cb57-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-725"><a href="#cb57-725" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-726"><a href="#cb57-726" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(samples)</span>
<span id="cb57-727"><a href="#cb57-727" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-728"><a href="#cb57-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-729"><a href="#cb57-729" aria-hidden="true" tabindex="-1"></a>The following will give us the median, and a 50% **credible interval**, i.e. an interval that contains 50% of the mass of the distribution.</span>
<span id="cb57-730"><a href="#cb57-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-731"><a href="#cb57-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-734"><a href="#cb57-734" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-735"><a href="#cb57-735" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(samples, <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>))</span>
<span id="cb57-736"><a href="#cb57-736" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-737"><a href="#cb57-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-738"><a href="#cb57-738" aria-hidden="true" tabindex="-1"></a>We can also use this approach to compute a 95% credible interval.</span>
<span id="cb57-739"><a href="#cb57-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-742"><a href="#cb57-742" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb57-743"><a href="#cb57-743" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb57-744"><a href="#cb57-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-745"><a href="#cb57-745" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(samples, <span class="fu">c</span>(alpha<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb57-746"><a href="#cb57-746" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb57-747"><a href="#cb57-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-748"><a href="#cb57-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-749"><a href="#cb57-749" aria-hidden="true" tabindex="-1"></a>:::{.callout-warning}</span>
<span id="cb57-750"><a href="#cb57-750" aria-hidden="true" tabindex="-1"></a>This should not be confused with the 95% confidence interval. Can you remember how a confidence interval is defined? What is the difference between a confidence interval and a credible interval?</span>
<span id="cb57-751"><a href="#cb57-751" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb57-752"><a href="#cb57-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-753"><a href="#cb57-753" aria-hidden="true" tabindex="-1"></a>What we have done so far is to look at Bayesian inference for parameter estimation (in a very simple model). Let's call this model $\mathcal{M}$. We have only considered one model at a time, but in the next session we will consider two or models $\mathcal{M_j}, which differ in the prior distributions they use. We will then look at methods for comparing models, in order to perform hypothesis testing in a Bayesian framework.</span>
<span id="cb57-754"><a href="#cb57-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-755"><a href="#cb57-755" aria-hidden="true" tabindex="-1"></a>In particular, we will look at how Bayesian model comparison can help, in the case that we have no significant results (see @sec-fancyhat).</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">© Copyright 2022, Andrew Ellis</div>   
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
<li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kogpsy/neuroscicomplabFS22">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
</div>
  </div>
</footer>


</body></html>