[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neurowissenschaft Computerlab",
    "section": "",
    "text": "Fr√ºhjahrssemester 2022"
  },
  {
    "objectID": "slides/04_process_data.html#data-cleaning",
    "href": "slides/04_process_data.html#data-cleaning",
    "title": "4. Sitzung",
    "section": "Data cleaning",
    "text": "Data cleaning\nNun wollen wir versuchen, einzelne Trials, zu identifizieren, in denen Versuchpersonen nicht aufgepasst haben, oder einfach geraten wurde. In solchen F√§llen w√§re die Qualit√§t der Daten nicht gut genug.\n\nAm h√§ufigsten werden die folgenden beiden Kriterien verwendet, um entweder einzelne Datenpunkte, oder Versuchspersonen, auszuschliessen:\n\nVersuchspersonen, deren Accuracy < 50% ist.\nTrials, in denen die Antwort zu schnell oder zu langsam war."
  },
  {
    "objectID": "slides/04_process_data.html#daten-zusammenfassen",
    "href": "slides/04_process_data.html#daten-zusammenfassen",
    "title": "4. Sitzung",
    "section": "Daten zusammenfassen",
    "text": "Daten zusammenfassen\n\nPro VP / Bedingung\n√ºber VPs aggregieren"
  },
  {
    "objectID": "slides/04_process_data.html#probieren-sie-es-selber",
    "href": "slides/04_process_data.html#probieren-sie-es-selber",
    "title": "4. Sitzung",
    "section": "Probieren Sie es selber!",
    "text": "Probieren Sie es selber!\n\nLearning by doing! Expertise in R erreicht man nur, wenn man selber ausprobiert.\n\n\nDaten mit esquisse visualisieren:\n\ninstall.packages(\"esquisse\")\nlibrary(esquisse)\nesquisser()\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/05_signal_detection.html#signal-detection-theory",
    "href": "slides/05_signal_detection.html#signal-detection-theory",
    "title": "5. Sitzung",
    "section": "Signal detection theory",
    "text": "Signal detection theory\n\nWhat happpens when a person must whether or not an event has occured, using information that insufficient to determine the answer?\nSignal detection theory is a widely-used method for analyzing this type of situation and of separating characteristics of the signal from those of the person who is detecting it.\nWe can begin to investigate how information is used to guide a decision."
  },
  {
    "objectID": "slides/05_signal_detection.html#signal-detection-theory-1",
    "href": "slides/05_signal_detection.html#signal-detection-theory-1",
    "title": "5. Sitzung",
    "section": "Signal detection theory",
    "text": "Signal detection theory\n\n\nWhat is a decision?\n\nA doctor is examining a patient and trying to make a diagnosis.\nA witness to a crime is asked to identify a suspect. Was this person present at the time of the crime or not?\nPurchase decision, gambling decisions, perceptual decisions, etc.\n\n\nBinary choice\n\n\n\n\nSignal\n\n\n\n\n\nResponse\nYes\nNo\n\n\n‚Äî‚Äî‚Äì\n‚Äî‚Äî‚Äî‚Äî‚Äì\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n\n\nYes\nHit\nFalse alarm (FA)\n\n\nNo\nMiss\nCorrect rejection (CR)"
  },
  {
    "objectID": "slides/05_signal_detection.html#signal-detection-theory-2",
    "href": "slides/05_signal_detection.html#signal-detection-theory-2",
    "title": "5. Sitzung",
    "section": "Signal detection theory",
    "text": "Signal detection theory\n\nAssumptions of SDTGraphical representation\n\n\nThe key assumptions of SDT\n\nSeparate representation and decision-making.\nSignal and noise trials can be represented as values along a uni-dimensional ‚Äústrength‚Äù construct.\nBoth types trials produce Gaussian random variables that represent strength.\nBoth variances are assumed to be the same.\nThe decision-making assumption of is that yes and no responses are produced by comparing the strength of the current trial to a fixed criterion. If the strength exceeds the criterion a ‚Äúyes‚Äù response is made, otherwise a ‚Äúno‚Äù response is made. ]\n\n\n\n\n\n\n\n\nEqual-variance Signal Detection Model"
  },
  {
    "objectID": "slides/05_signal_detection.html#c-and-d",
    "href": "slides/05_signal_detection.html#c-and-d",
    "title": "5. Sitzung",
    "section": "c and d'",
    "text": "c and d'\n\n\n\n\n\n\n\nThe mean of the signal distribution is d'. This makes d' a measure of the discriminability of the signal trials from the noise trials ‚Äì it corresponds to the distance between the two distributions.\nThe strength value d/2 is special, because it is the criterion value that maximizes the probability of a correct classification when signal and noise trials are equally likely to occur.\nc a measure of bias, because it corresponds to how different the actual criterion is from the unbiased one. Positive values of c correspond to a bias towards saying no, and so to an increase in correct rejections at the expense of an increase in misses. Negative values of c correspond to a bias towards saying yes, and so to an increase in hits at the expense of an increase in false alarms."
  },
  {
    "objectID": "slides/05_signal_detection.html#computing-sdt-measures-in-r",
    "href": "slides/05_signal_detection.html#computing-sdt-measures-in-r",
    "title": "5. Sitzung",
    "section": "Computing SDT Measures in R",
    "text": "Computing SDT Measures in R\n\nDistribution function: codeDistribution function: figure\n\n\n\nlibrary(tidyverse)\ntibble(x = seq(-3, 3, by = 0.1)) %>% \n  ggplot(aes(x)) +\n  stat_function(fun = pnorm, colour = \"steelblue3\", \n                  args = list(mean = 0, sd = 1),\n                  size = 1.5) +\n  labs(y = \"Probability\", x = \"Z Score\") +\n  scale_y_continuous(limits = c(0, 1)) +\n  scale_x_continuous(limits = c(-3.5, 3.5), breaks = -3:3) + \n  ggtitle(\"Cumulative distribution function / pnorm()\") +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nCumulative distribution function: pnorm()"
  },
  {
    "objectID": "slides/05_signal_detection.html#using-signal-detection-to-analyze-data-in-r",
    "href": "slides/05_signal_detection.html#using-signal-detection-to-analyze-data-in-r",
    "title": "5. Sitzung",
    "section": "Using Signal Detection to Analyze Data in R",
    "text": "Using Signal Detection to Analyze Data in R\nWe will look at an example using data from a recognition memory test.\n\nSubjects first learn words (study phase).\nIn the test phase, they are presented with words that were on the list, and with new words.\nSubjects have to classify words into\n\nold (yes, this was on the list)\nnew (no, this wasn‚Äôt on the list)\n\nThese kind of data are usually analyzed using a Signal Detection model.\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/07_response_times.html#what-are-response-times",
    "href": "slides/07_response_times.html#what-are-response-times",
    "title": "7. Sitzung",
    "section": "What are Response Times?",
    "text": "What are Response Times?\n\n\n\n\nTypical distribution of response times.\n\nTime taken to complete a task.\nTypically, some observed RTs are not a result of the process of interest,but contaminants. Luce (1986) demonstrated that genuine RTs have a minimum value of \\(> 100\\) ms: the time needed for physiological processes (stimulus perception, motor responses).\n\n\n\nWhelan, R. (2008). Effective Analysis of Reaction Time Data. The Psychological Record, 58(3), 475‚Äì482. https://doi.org/10.1007/BF03395630"
  },
  {
    "objectID": "slides/07_response_times.html#tadpoles",
    "href": "slides/07_response_times.html#tadpoles",
    "title": "7. Sitzung",
    "section": "Tadpoles",
    "text": "Tadpoles\n\n\n\n\n\n\n\n\nRoberts, A., Borisyuk, R., Buhl, E., Ferrario, A., Koutsikou, S., Li, W.-C., & Soffe, S. R. (2019). The decision to move: Response times, neuronal circuits and sensory memory in a simple vertebrate. Proceedings of the Royal Society B: Biological Sciences, 286(1899), 20190297. https://doi.org/10.1098/rspb.2019.0297"
  },
  {
    "objectID": "slides/07_response_times.html#adhd",
    "href": "slides/07_response_times.html#adhd",
    "title": "7. Sitzung",
    "section": "ADHD",
    "text": "ADHD\n\n\n\n\nPerformance variability is the essence of ADHD.\nChildren with ADHD demonstrate greater RT variability than nonclinical controls on speeded RT tasks.\n\n\nResponse variability is related to a distributed brain network including frontal lobes (implicated in the pathophysiology of ADHD).\n\n\nTraditional RT measures (sample mean and standard deviation) showed that children with ADHD were slower and more variable in responding than controls.\n\n\n\n\n\nChildren with ADHD had a greater number of RTs beyond their mean performance than the control group. Were not generally slower but were prone to attentional lapses.\n\n\n\n\n\n\nHervey, A. S., Epstein, J. N., Curry, J. F., Tonev, S., Eugene Arnold, L., Keith Conners, C., Hinshaw, S. P., Swanson, J. M., & Hechtman, L. (2006). Reaction Time Distribution Analysis of Neuropsychological Performance in an ADHD Sample. Child Neuropsychology, 12(2), 125‚Äì140. https://doi.org/10.1080/09297040500499081"
  },
  {
    "objectID": "slides/07_response_times.html#central-tendency-mean",
    "href": "slides/07_response_times.html#central-tendency-mean",
    "title": "7. Sitzung",
    "section": "Central tendency: Mean",
    "text": "Central tendency: Mean\n\nMost common method of analyzing RT data is to report a central tendency parameter (e.g., the mean or median) and a dispersion parameter (e.g., the standard deviation). The mean difference in RT across conditions is then often analyzed by using ANOVA.\nUsing hypothesis tests on data that are skewed, contain outliers, are heteroscedastic, or have a combination of these characteristics (raw RT data typically have at least the first two) reduces the power of these tests and can result in a failure to detect a real difference between conditions (Wilcox, 1998).\nNeither the mean nor standard deviation are said to be robust measures. That is, the mean is not reflective of the typical response if the distribution is skewed, because the mean is distorted in the direction of the skew."
  },
  {
    "objectID": "slides/07_response_times.html#central-tendency-median",
    "href": "slides/07_response_times.html#central-tendency-median",
    "title": "7. Sitzung",
    "section": "Central tendency: Median",
    "text": "Central tendency: Median\n\nMany researchers report the median RT as a central tendency parameter, because it is less susceptible to departures from normality (i.e., robust). The interquartile range (IQR, the range between the third and first quartiles) is a robust method of estimating the dispersion.\nA difficulty with using the median is that unlike the sample mean, it is a biased estimator of the population median when the population is skewed (a biased estimator does not, on average, equal the value of the parameter or function that it estimates).\nThe bias becomes more extreme as the sample size becomes smaller (Miller, 1988) and thus the median is more likely to be overestimated in the condition with fewer trials."
  },
  {
    "objectID": "slides/07_response_times.html#beyond-mean-differences",
    "href": "slides/07_response_times.html#beyond-mean-differences",
    "title": "7. Sitzung",
    "section": "Beyond mean differences",
    "text": "Beyond mean differences\n\n\n\nMost analyses of RT data are conducted by using the statistical techniques with which psychologists are most familiar, such as analysis of variance (ANOVA) on the sample mean. Unfortunately, these methods are usually inappropriate for RT data, because they have little power to detect genuine differences in RT between conditions.\n\n\n\n\n\n\nRousselet, G. A., Pernet, C. R., & Wilcox, R. R. (2017). Beyond differences in means: Robust graphical methods to compare two groups in neuroscience. European Journal of Neuroscience, 46(2), 1738‚Äì1748. [https://doi.org/10.1111/ejn.13610](https://doi.org/10.1111/"
  },
  {
    "objectID": "slides/07_response_times.html#beyond-mean-differences-1",
    "href": "slides/07_response_times.html#beyond-mean-differences-1",
    "title": "7. Sitzung",
    "section": "Beyond mean differences",
    "text": "Beyond mean differences\n\n\nDeciles\n\n\nShift function\n\n\n\n\nFor each decile, the shift function illustrates by how much one distribution needs to be shifted to match another one.\nIn our example, we illustrate by how much we need to shift deciles from group 2 to match deciles from group 1."
  },
  {
    "objectID": "slides/07_response_times.html#beyond-mean-differences-2",
    "href": "slides/07_response_times.html#beyond-mean-differences-2",
    "title": "7. Sitzung",
    "section": "Beyond mean differences",
    "text": "Beyond mean differences\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/08_response_times_ii.html#next-sessions",
    "href": "slides/08_response_times_ii.html#next-sessions",
    "title": "8. Sitzung",
    "section": "Next sessions",
    "text": "Next sessions\n\nEvidence accumulation\nBayesian data analysis\nIs the brain Bayesian?"
  },
  {
    "objectID": "slides/08_response_times_ii.html#adhd-response-time-variability",
    "href": "slides/08_response_times_ii.html#adhd-response-time-variability",
    "title": "8. Sitzung",
    "section": "ADHD: Response Time Variability",
    "text": "ADHD: Response Time Variability\n\nChildren with ADHD show cognitive impairments in attention, inhibitory control, and working memory compared with typically-developing children.\nNo specific cognitive impairment is universal across patients with ADHD. One of the more consistent findings in the ADHD neuropsychology literature is increased reaction time variability (RTV), also known as intra-individual variability, on computerized tasks.\nChildren with ADHD have also been shown to be less able to deactivate the DMN than controls, and this inability to deactivate the DMN has been directly related to RTV.\nDoes RTV reflect a single construct or process? The vast majority of studies have used a reaction time standard deviation (RTSD)."
  },
  {
    "objectID": "slides/08_response_times_ii.html#facilitation",
    "href": "slides/08_response_times_ii.html#facilitation",
    "title": "8. Sitzung",
    "section": "Facilitation",
    "text": "Facilitation\n\nStroop task vs Simon task\nFacilitation effects in both task.\nStroop effects are smallest for fast responses and increase as responses slow.\nSimon effects are largest for fast responses but decrease, and even reverse, as responses slow (see DEMO).\n\nPratte, M. S., Rouder, J. N., Morey, R. D., & Feng, C. (2010). Exploring the differences in distributional properties between Stroop and Simon effects using delta plots. Attention, Perception, & Psychophysics, 72(7), 2013‚Äì2025. https://doi.org/10.3758/APP.72.7.2013"
  },
  {
    "objectID": "slides/08_response_times_ii.html#experimental-manipulations",
    "href": "slides/08_response_times_ii.html#experimental-manipulations",
    "title": "8. Sitzung",
    "section": "Experimental manipulations",
    "text": "Experimental manipulations\n\n\n\naffects most strongly slow behavioural responses, but with limited effects on fast responses.\naffects all responses, fast and slow, similarly.\nhas stronger effects on fast responses, and weaker ones for slow responses.\n\n\n\n\n\nDistribution analysis provides much stronger constraints on the underlying cognitive architecture than comparisons limited to e.g.¬†mean or median reaction times across participants."
  },
  {
    "objectID": "slides/08_response_times_ii.html#shift-function-independent-groups",
    "href": "slides/08_response_times_ii.html#shift-function-independent-groups",
    "title": "8. Sitzung",
    "section": "Shift Function: Independent Groups",
    "text": "Shift Function: Independent Groups\n\nUniform shiftShift functionPlot shift function\n\n\n\n\n\n\n\n\n\n\n\nout <- shifthd_pbci(df, nboot = 200, adj_ci = FALSE)\np <- plot_sf(out, plot_theme = 1)[[1]] + \n     theme(axis.text = element_text(size = 16, colour=\"black\"))"
  },
  {
    "objectID": "slides/08_response_times_ii.html#hierarchical-shift-function",
    "href": "slides/08_response_times_ii.html#hierarchical-shift-function",
    "title": "8. Sitzung",
    "section": "Hierarchical Shift Function",
    "text": "Hierarchical Shift Function\n\nquantiles are computed for the distribution of measurements from each condition and each participant.\nthe quantiles are subtracted in each participant.\na trimmed mean is computed across participants for each quantile.\nConfidence intervals are computed using the percentile bootstrap."
  },
  {
    "objectID": "slides/08_response_times_ii.html#hierarchical-shift-function-1",
    "href": "slides/08_response_times_ii.html#hierarchical-shift-function-1",
    "title": "8. Sitzung",
    "section": "Hierarchical Shift Function",
    "text": "Hierarchical Shift Function\n\n\n\n\nShift functionPlot shift functionStochastic dominance\n\n\n\n\nout <- hsf(df, rt ~ cond + id)\n\n\n\n\n\n\np <- plot_hsf(out)\np\n\n\n\n\n\n\n\nParticipants with all quantile differences > 0\n\nnq <- length(out$quantiles)\npdmt0 <- apply(out$individual_sf > 0, 2, sum)\nprint(paste0('In ',sum(pdmt0 == nq),' participants (',round(100 * sum(pdmt0 == nq) / np, digits = 1),'%), all quantile differences are more than to zero'))\n\n[1] \"In 0 participants (0%), all quantile differences are more than to zero\"\n\n\nParticipants with all quantile differences < 0\n\npdlt0 <- apply(out$individual_sf < 0, 2, sum)\nprint(paste0('In ',sum(pdlt0 == nq),' participants (',round(100 * sum(pdlt0 == nq) / np, digits = 1),'%), all quantile differences are less than to zero'))\n\n[1] \"In 22 participants (73.3%), all quantile differences are less than to zero\""
  },
  {
    "objectID": "slides/08_response_times_ii.html#hierarchical-shift-function-2",
    "href": "slides/08_response_times_ii.html#hierarchical-shift-function-2",
    "title": "8. Sitzung",
    "section": "Hierarchical Shift Function",
    "text": "Hierarchical Shift Function\nAlternative bootstrapping method: hsf_pb()\n\nset.seed(8899)\nout <- rogme::hsf_pb(df, rt ~ cond + id)\n\n\nrogme::plot_hsf_pb(out, interv = \"ci\")"
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping",
    "href": "slides/08_response_times_ii.html#bootstrapping",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n‚ÄúThe bootstrap is a computer-based method for assigning measures of accuracy to statistical estimates.‚Äù Efron & Tibshirani, An introduction to the bootstrap, 1993\n\n\n‚ÄúThe central idea is that it may sometimes be better to draw conclusions about the characteristics of a population strictly from the sample at hand, rather than by making perhaps unrealistic assumptions about the population.‚Äù Mooney & Duval, Bootstrapping, 1993\n\n\nLike all bootstrap methods, the percentile bootstrap relies on a simple & intuitive idea: instead of making assumptions about the underlying distributions from which our observations could have been sampled, we use the data themselves to estimate sampling distributions.\nIn turn, we can use these estimated sampling distributions to compute confidence intervals, estimate standard errors, estimate bias, and test hypotheses (Efron & Tibshirani, 1993; Mooney & Duval, 1993; Wilcox, 2012).\nThe core principle to estimate sampling distributions is resampling. The technique was developed & popularised by Brad Efron as the bootstrap."
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping-1",
    "href": "slides/08_response_times_ii.html#bootstrapping-1",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nheights <- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185)\n\n\nv1 <- sample(heights, replace = TRUE)\nv1\n\n [1] 188 183 188 183 182 177 188 188 188 192\n\n\n\nv2 <- sample(heights, replace = TRUE)\nv2\n\n [1] 192 183 185 185 177 185 185 185 182 192\n\n\nEssentially, we are doing fake experiments using only the observations from our sample. And for each of these fake experiments, or bootstrap sample, we can compute any estimate of interest, for instance the median.\nFor a visualization, see this demo."
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping-2",
    "href": "slides/08_response_times_ii.html#bootstrapping-2",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nFunction to resample and compute meanRepeat N timesVisualize\n\n\n\nf <- function(v) {\n  vv <- sample(v, replace = TRUE)\n  mean(vv)\n}\n\n\n\n\nN <- 4000\n\n\nb1 <- replicate(n = N, f(heights))\n\n\nlibrary(purrr)\nb2 <- map_dbl(1:N, ~f(heights))\n\n\nd <- tibble(b1 = b1, b2 = b2) |> \n  pivot_longer(everything(), names_to = \"bootstrap\", values_to = \"mean\")\n\n\n\n\n\nd |> \n  ggplot(aes(mean, fill = bootstrap)) + \n  geom_histogram() +\n  facet_wrap(~bootstrap)"
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping-3",
    "href": "slides/08_response_times_ii.html#bootstrapping-3",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapped standard error of the mean:\n\nMethod 1Method 2\n\n\n\nd |> \n  group_by(bootstrap) |> \n  tidybayes::mean_qi()\n\n# A tibble: 2 √ó 7\n  bootstrap  mean .lower .upper .width .point .interval\n  <chr>     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 b1         185.    182   187    0.95 mean   qi       \n2 b2         185.    182   187.   0.95 mean   qi       \n\n\n\n\n\nd |> \n  group_by(bootstrap) |> \n  summarize(estimate = mean(mean), \n            lower = quantile(mean, probs = 0.025),\n            upper = quantile(mean, probs = 0.975))\n\n# A tibble: 2 √ó 4\n  bootstrap estimate lower upper\n  <chr>        <dbl> <dbl> <dbl>\n1 b1            185.   182  187 \n2 b2            185.   182  187.\n\n\n\n\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/01_introduction.html#model-based-cognitive-neuroscience",
    "href": "slides/01_introduction.html#model-based-cognitive-neuroscience",
    "title": "1. Sitzung",
    "section": "(Model-based) Cognitive Neuroscience",
    "text": "(Model-based) Cognitive Neuroscience\n\n\nWas heisst Model-based Neuroscience?\nWelche Kenntnisse brauchen wir, um Experiment durchzuf√ºhren und Daten auszuwerten?\nWelche Programmiertechniken/sprachen brauchen wir?"
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience-beispiel",
    "href": "slides/01_introduction.html#model-based-neuroscience-beispiel",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience: Beispiel",
    "text": "Model-based Neuroscience: Beispiel\nMulder, M. J., Wagenmakers, E.-J., Ratcliff, R., Boekel, W., & Forstmann, B. U. (2012). Bias in the Brain: A Diffusion Model Analysis of Prior Probability and Potential Payoff. Journal of Neuroscience, 32(7), 2335‚Äì2343.\nüëâ https://www.jneurosci.org/content/32/7/2335\nIn dieser Studie geht es darum, den Einfluss von Vorwissen (prior knowledge) auf eine simple perzeptuelle Entscheidung zu untersuchen.\n\nAls Task haben die Autoren ein Random Dot Motion Experiment benutzt.\nF√ºr die Datenanalyse wurde unter anderem ein Diffusion Decision Model verwendet."
  },
  {
    "objectID": "slides/01_introduction.html#diffusion-decision-model",
    "href": "slides/01_introduction.html#diffusion-decision-model",
    "title": "1. Sitzung",
    "section": "Diffusion Decision Model",
    "text": "Diffusion Decision Model"
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience",
    "href": "slides/01_introduction.html#model-based-neuroscience",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience",
    "text": "Model-based Neuroscience\n\n√úberfliegen Sie das Paper, und achten Sie dabei darauf, welche Skills Sie ben√∂tigen, um eine solche Studie durchzuf√ºhren.\n\nWelches theoretische Wissen brauchen Sie?\nWelche Programmierkenntnisse brauchen Sie?\n\nf√ºr das Experiment\nf√ºr die Datenanalyse\n\nWelche statistischen Verfahen brauchen Sie, um die Daten auszuwerten?\nWarum wurde das Experiment im Scanner und ausserhalb des Scanners durchgef√ºhrt?\nWas kann man mit einer solchen Studie herausfinden?"
  },
  {
    "objectID": "slides/01_introduction.html#vorwissen",
    "href": "slides/01_introduction.html#vorwissen",
    "title": "1. Sitzung",
    "section": "Vorwissen",
    "text": "Vorwissen\nEs wurden zwei verschiedene Typen von Vorwissen benutzt.\n\nA-Priori Wahrscheinlichkeit, dass die Punktwolke sich nach rechts oder nach links bewegte.\nAsymmetrische Belohnung f√ºr korrekte links/rechts Entscheidungen."
  },
  {
    "objectID": "slides/01_introduction.html#diffusion-decision-model-1",
    "href": "slides/01_introduction.html#diffusion-decision-model-1",
    "title": "1. Sitzung",
    "section": "Diffusion Decision Model",
    "text": "Diffusion Decision Model"
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience-1",
    "href": "slides/01_introduction.html#model-based-neuroscience-1",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience",
    "text": "Model-based Neuroscience\n\n\nSchematische Darstellung der erwarteten Resultate.\n\nStarting point: korrekte und inkorrekte RTs unterschieden sich.\nDrift rate: korrekte und inkorrekte RTs sind sich √§hnlich.\n\n\n\nTats√§chliche Resultate: Quantifizierung des Bias anhand des DDM."
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience-2",
    "href": "slides/01_introduction.html#model-based-neuroscience-2",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience",
    "text": "Model-based Neuroscience\nBOLD Responses der Areale welche besonder stark sowohl auf die ‚Äúprior probability‚Äù als auch auf die ‚Äúpayoff‚Äù Manipulation reagierten.\n\n\n\nright MedFG (right medial frontal gyrus)\nACG (anterior cingulate cortex)\nSFG (superior frontal gyrus)\nleft middle temporal gyrus\nIPS (intra-parietal sulcus).\n\n\n\n\n\nDiese Areale sollen eine besondere Rolle in der Verarbeitung von Bias im Entscheidungsverhalten haben."
  },
  {
    "objectID": "slides/01_introduction.html#wichtige-skills",
    "href": "slides/01_introduction.html#wichtige-skills",
    "title": "1. Sitzung",
    "section": "Wichtige Skills",
    "text": "Wichtige Skills\n\n\n\nTheorien √ºber Entscheidungsverhalten\nExperimente programmieren\n\nTiming (inside/outside scanner)\n\nData cleaning and manipulation (data wrangling)\nStatistische Verfahren f√ºr messwiederholte Daten\n\nPsychometric curve\nBinary choices / Reaktionszeiten\nrepeated-measures ANOVA\n\n\n\n\nGrafische Darstellung der Resultate\nKognitive Prozessmodelle\n\nfit Diffusion Decision Model (DDM)\n\nAuswertung von fRMI Daten\n\n\n\n\nMit diesen Themen (ausser der Analyse von fMRI Daten) besch√§ftigen wir uns in diesem Kurs.\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/02_psychopy.html#bias-rdk-experiment",
    "href": "slides/02_psychopy.html#bias-rdk-experiment",
    "title": "2. Sitzung",
    "section": "Bias RDK Experiment",
    "text": "Bias RDK Experiment\n\n\n\nRandom-dot motion direction-discrimination task\nInside/outside scanner (timing)\nBias: cue (probability left/right/unbiased)\nFixation cross\nRDK: 3x3 pixels, coherence\n40 bias trials, 40 neutral trials\n32 valid, 8 invalid trials"
  },
  {
    "objectID": "slides/02_psychopy.html#psychopy",
    "href": "slides/02_psychopy.html#psychopy",
    "title": "2. Sitzung",
    "section": "PsychoPy",
    "text": "PsychoPy\n\n\n\nPsychoPy Website\nRessourcen\nWalk-through: Builder\nDiskussionsforum\nKapitel: Verhaltensexperiment mit PsychoPy"
  },
  {
    "objectID": "slides/02_psychopy.html#pavlovia",
    "href": "slides/02_psychopy.html#pavlovia",
    "title": "2. Sitzung",
    "section": "Pavlovia",
    "text": "Pavlovia\n\nPavlovia:\n\n\nPavlovia is a place for the wide community of researchers in the behavioural sciences to run, share, and explore experiments online.\n\n\nExperimente suchen.\nZum Beispiel ChoiceRTT ausprobieren und den Code anschauen."
  },
  {
    "objectID": "slides/02_psychopy.html#understanding-your-computer",
    "href": "slides/02_psychopy.html#understanding-your-computer",
    "title": "2. Sitzung",
    "section": "Understanding your Computer",
    "text": "Understanding your Computer\n\nRefresh rate: 60 Hz. Ein Frame dauert 1/60 Sekunde, oder 16.667 ms.\n\nfrom psychopy import visual\n\nwin = visual.Window()\nwin.getActualFrameRate()\n\nKeyboard timing: Variabilit√§t ~15 ms.\nScreen refresh f√§ngt oben an und endet (~10 ms sp√§ter) unten."
  },
  {
    "objectID": "slides/02_psychopy.html#probieren-sie-es-selber",
    "href": "slides/02_psychopy.html#probieren-sie-es-selber",
    "title": "2. Sitzung",
    "section": "Probieren Sie es selber!",
    "text": "Probieren Sie es selber!\n\nVersuchen Sie selber, Teile des Experiments in PsychoPy zu implementieren\n\n\nWenn Sie eine Starthilfe ben√∂tigen, downloaden Sie ein Beipiel: üëâ Practice Trials\nEine Einf√ºhrung finden Sie hier: üëâ Verhaltensexperiment mit PsychoPy\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#set-up-rrstudio",
    "href": "slides/03_import_and_process_data.html#set-up-rrstudio",
    "title": "3. Sitzung",
    "section": "Set up R/RStudio",
    "text": "Set up R/RStudio\n\n\nüëâ Download R\nüëâ Download RStudio\n\n\nRStudio √∂ffnen\nRStudio einrichten\nPackages installieren\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#r-kenntnisse",
    "href": "slides/03_import_and_process_data.html#r-kenntnisse",
    "title": "3. Sitzung",
    "section": "R Kenntnisse",
    "text": "R Kenntnisse\n\n\nüëâ Einf√ºhrung in R\n\nEinf√ºhrung in RStudio\nR Sprache\nControl flow / Funktionen\nDaten importieren / tidy data / visualisieren\nDeskriptive Statistik ]\n\n\nüëâ Data Skills for Reproducible Research\n\nReproducible Workflows\nData visualization\nData wrangling\nIteration & Functions"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#datensatz-importieren",
    "href": "slides/03_import_and_process_data.html#datensatz-importieren",
    "title": "3. Sitzung",
    "section": "Datensatz importieren",
    "text": "Datensatz importieren\nüëâ Download Rstudio Projekt\n\nZZ_rdk-discrimination_2022_Mar_07_1403 aus dem testdata Ordner importieren.\nPractice Trials l√∂schen.\nVariablen ausw√§hlen und umbennen.\n\n    - Trial Index\n    - ID\n    - Cue\n    - Direction\n    - Response / RT\n\nAntworten rekodieren."
  },
  {
    "objectID": "slides/03_import_and_process_data.html#mehrere-datens√§tze-importieren",
    "href": "slides/03_import_and_process_data.html#mehrere-datens√§tze-importieren",
    "title": "3. Sitzung",
    "section": "Mehrere Datens√§tze importieren",
    "text": "Mehrere Datens√§tze importieren\n\n\n\nAlle .csv Files aus dem data Ordner importieren.\nDieselben Schritte wie oben auf alle Datens√§tze anwenden."
  },
  {
    "objectID": "slides/03_import_and_process_data.html#anzahl-korrekter-antworten",
    "href": "slides/03_import_and_process_data.html#anzahl-korrekter-antworten",
    "title": "3. Sitzung",
    "section": "Anzahl korrekter Antworten",
    "text": "Anzahl korrekter Antworten\n\nAccuracy pro Person und pro Bedingung berechnen.\nBedingungen:\n\n   - valid\n   - invalid\n   - neutral"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#probieren-sie-es-selber",
    "href": "slides/03_import_and_process_data.html#probieren-sie-es-selber",
    "title": "3. Sitzung",
    "section": "Probieren Sie es selber!",
    "text": "Probieren Sie es selber!\n\nLearning by doing! Expertise in R erreicht man nur, wenn man selber ausprobiert.\n\n\nKapitel Daten importieren bearbeiten.\nFragen stellen.\nDaten mit esquisse visualisieren:\n\ninstall.packages(\"esquisse\")\nlibrary(esquisse)\nesquisser()\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/06_signal_detection_examples.html#visual-motion",
    "href": "slides/06_signal_detection_examples.html#visual-motion",
    "title": "6. Sitzung",
    "section": "Visual Motion",
    "text": "Visual Motion\n\nDecision variable: Lateral Intraparietal Cortex (LIP)\n\n\n\n\n\n\n\n\n\n\nShadlen, M. N., & Kiani, R. (2013). Decision Making as a Window on Cognition. Neuron, 80(3), 791‚Äì806. https://doi.org/10.1016/j.neuron.2013.10.047"
  },
  {
    "objectID": "slides/06_signal_detection_examples.html#cognitive-fatigue",
    "href": "slides/06_signal_detection_examples.html#cognitive-fatigue",
    "title": "6. Sitzung",
    "section": "Cognitive Fatigue",
    "text": "Cognitive Fatigue\n\n\nWhen we are fatigued, we feel that our performance is worse than when we are fresh.\nThe metrics of signal detection theory (SDT)‚Äîresponse bias (criterion) and perceptual certainty (d‚Äô) may change as a function of fatigue, but no work has yet been done to examine whether these metrics covary with fatigue.\nWe induced fatigue through repetitive performance of the n-back working memory task, while functional magnetic resonance imaging (fMRI) data was acquired.\nOur results show that both criterion and d‚Äô were correlated with changes in cognitive fatigue: as fatigue increased, subjects became more conservative in their response bias and their perceptual certainty declined. Furthermore, activation in the striatum of the basal ganglia was also related to cognitive fatigue, criterion, and d‚Äô.\nThe following behavioral data were analyzed: overall accuracy, the reaction times (RTs) of the correct trials, and signal detection metrics.\nParticipants were presented with a visual analog scale (VAS) before and after each block. Participants were asked: ‚ÄúHow mentally fatigued are you right now?‚Äù\n\nWylie, G. R., Yao, B., Sandry, J., & DeLuca, J. (2020). Using Signal Detection Theory to Better Understand Cognitive Fatigue. Frontiers in Psychology, 11, 579188. https://doi.org/10.3389/fpsyg.2020.579188"
  },
  {
    "objectID": "slides/06_signal_detection_examples.html#cognitive-fatigue-1",
    "href": "slides/06_signal_detection_examples.html#cognitive-fatigue-1",
    "title": "6. Sitzung",
    "section": "Cognitive Fatigue",
    "text": "Cognitive Fatigue\n\n\n\n\n\n\n\n\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html",
    "href": "pages/chapters/02_importing_data.html",
    "title": "Daten importieren",
    "section": "",
    "text": "Nun wollen wir die Datens√§tze aus dem Verhaltensexperiment von der letzten Sitzung in R importieren.\nLaden Sie das RStudio Projekt und √∂ffnen Sie es. Im Projekt ist ein R Script File enthalten (import-data.R).\n\nFalls Sie nur den R Code m√∂chten, k√∂nnen Sie das File hier downloaden: üëâ R Code"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#csv-file-importieren",
    "href": "pages/chapters/02_importing_data.html#csv-file-importieren",
    "title": "Daten importieren",
    "section": "CSV File importieren",
    "text": "CSV File importieren\n\ntestdata <- read_csv(\"testdata/ZZ_rdk-discrimination_2022_Mar_07_1403.csv\") \n\nVariablen √ºberpr√ºfen\n\nglimpse(testdata)\n\nRows: 167\nColumns: 39\n$ cue                                        <chr> \"none\", \"left\", \"right\", \"l‚Ä¶\n$ direction                                  <chr> \"right\", \"right\", \"right\", ‚Ä¶\n$ practice_block_loop.thisRepN               <dbl> 0, 0, 0, 0, 0, 0, NA, NA, N‚Ä¶\n$ practice_block_loop.thisTrialN             <dbl> 0, 1, 2, 3, 4, 5, NA, NA, N‚Ä¶\n$ practice_block_loop.thisN                  <dbl> 0, 1, 2, 3, 4, 5, NA, NA, N‚Ä¶\n$ practice_block_loop.thisIndex              <dbl> 5, 2, 1, 0, 4, 3, NA, NA, N‚Ä¶\n$ main_blocks_loop.thisRepN                  <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ main_blocks_loop.thisTrialN                <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ main_blocks_loop.thisN                     <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ main_blocks_loop.thisIndex                 <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ static_isi.started                         <dbl> 0.01033428, 0.03202713, 0.0‚Ä¶\n$ static_isi.stopped                         <dbl> 2.010334, 2.032027, 2.03217‚Ä¶\n$ fixation_pre.started                       <dbl> 26.79425, 36.16522, 44.7852‚Ä¶\n$ fixation_pre.stopped                       <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ image.started                              <dbl> 27.19849, 36.28205, 46.0032‚Ä¶\n$ image.stopped                              <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ fixation_post.started                      <dbl> 28.17814, 37.28240, 47.0037‚Ä¶\n$ fixation_post.stopped                      <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_background.started                    <dbl> 32.18642, 41.30145, 52.0107‚Ä¶\n$ dots_background.stopped                    <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_stimulus.started                      <dbl> 32.18642, 41.30145, 52.0107‚Ä¶\n$ dots_stimulus.stopped                      <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_keyboard_response.keys                <chr> \"None\", \"f\", \"j\", \"f\", \"Non‚Ä¶\n$ dots_keyboard_response.started             <dbl> 32.18642, 41.30145, 52.0107‚Ä¶\n$ dots_keyboard_response.stopped             <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ feedback_text.started                      <dbl> 33.70200, 42.28899, 52.9229‚Ä¶\n$ feedback_text.stopped                      <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_keyboard_response.rt                  <dbl> NA, 0.9339199, 0.8488816, 0‚Ä¶\n$ instruction_main_text.started              <dbl> NA, NA, NA, NA, NA, NA, 81.‚Ä¶\n$ instruction_main_text.stopped              <chr> NA, NA, NA, NA, NA, NA, \"No‚Ä¶\n$ instruction_main_keyboard_response.keys    <chr> NA, NA, NA, NA, NA, NA, \"sp‚Ä¶\n$ instruction_main_keyboard_response.rt      <dbl> NA, NA, NA, NA, NA, NA, 3.1‚Ä¶\n$ instruction_main_keyboard_response.started <dbl> NA, NA, NA, NA, NA, NA, 81.‚Ä¶\n$ instruction_main_keyboard_response.stopped <chr> NA, NA, NA, NA, NA, NA, \"No‚Ä¶\n$ Pseudonym                                  <chr> \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ‚Ä¶\n$ date                                       <chr> \"2022_Mar_07_1403\", \"2022_M‚Ä¶\n$ expName                                    <chr> \"rdk-discrimination\", \"rdk-‚Ä¶\n$ psychopyVersion                            <chr> \"03.02.21\", \"03.02.21\", \"03‚Ä¶\n$ frameRate                                  <dbl> 59.9, 59.9, 59.9, 59.9, 59.‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#practice-trials-l√∂schen",
    "href": "pages/chapters/02_importing_data.html#practice-trials-l√∂schen",
    "title": "Daten importieren",
    "section": "Practice Trials l√∂schen",
    "text": "Practice Trials l√∂schen\n\nlibrary(kableExtra)\n\ntestdata |> \n  slice_head(n = 12) |> \n  kbl() |> \n  kable_paper(\"striped\", full_width = FALSE) |> \n  column_spec(2:7, bold = TRUE) |> \n  row_spec(1:6, bold = TRUE, color = \"white\", background = \"#D7261E\")\n\n\n\n cue \n    direction \n    practice_block_loop.thisRepN \n    practice_block_loop.thisTrialN \n    practice_block_loop.thisN \n    practice_block_loop.thisIndex \n    main_blocks_loop.thisRepN \n    main_blocks_loop.thisTrialN \n    main_blocks_loop.thisN \n    main_blocks_loop.thisIndex \n    static_isi.started \n    static_isi.stopped \n    fixation_pre.started \n    fixation_pre.stopped \n    image.started \n    image.stopped \n    fixation_post.started \n    fixation_post.stopped \n    dots_background.started \n    dots_background.stopped \n    dots_stimulus.started \n    dots_stimulus.stopped \n    dots_keyboard_response.keys \n    dots_keyboard_response.started \n    dots_keyboard_response.stopped \n    feedback_text.started \n    feedback_text.stopped \n    dots_keyboard_response.rt \n    instruction_main_text.started \n    instruction_main_text.stopped \n    instruction_main_keyboard_response.keys \n    instruction_main_keyboard_response.rt \n    instruction_main_keyboard_response.started \n    instruction_main_keyboard_response.stopped \n    Pseudonym \n    date \n    expName \n    psychopyVersion \n    frameRate \n  \n\n\n none \n    right \n    0 \n    0 \n    0 \n    5 \n    NA \n    NA \n    NA \n    NA \n    0.0103343 \n    2.010334 \n    26.79425 \n    None \n    27.19849 \n    None \n    28.17814 \n    None \n    32.18642 \n    None \n    32.18642 \n    None \n    None \n    32.18642 \n    None \n    33.70200 \n    None \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n left \n    right \n    0 \n    1 \n    1 \n    2 \n    NA \n    NA \n    NA \n    NA \n    0.0320271 \n    2.032027 \n    36.16522 \n    None \n    36.28205 \n    None \n    37.28240 \n    None \n    41.30145 \n    None \n    41.30145 \n    None \n    f \n    41.30145 \n    None \n    42.28899 \n    None \n    0.9339199 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    right \n    0 \n    2 \n    2 \n    1 \n    NA \n    NA \n    NA \n    NA \n    0.0321732 \n    2.032173 \n    44.78521 \n    None \n    46.00329 \n    None \n    47.00374 \n    None \n    52.01072 \n    None \n    52.01072 \n    None \n    j \n    52.01072 \n    None \n    52.92295 \n    None \n    0.8488816 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n left \n    left \n    0 \n    3 \n    3 \n    0 \n    NA \n    NA \n    NA \n    NA \n    0.0321533 \n    2.032153 \n    55.39138 \n    None \n    56.19407 \n    None \n    57.22527 \n    None \n    61.23181 \n    None \n    61.23181 \n    None \n    f \n    61.23181 \n    None \n    62.21611 \n    None \n    0.9396018 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n none \n    left \n    0 \n    4 \n    4 \n    4 \n    NA \n    NA \n    NA \n    NA \n    0.0321391 \n    2.032139 \n    64.71204 \n    None \n    64.81315 \n    None \n    65.84603 \n    None \n    69.25240 \n    None \n    69.25240 \n    None \n    None \n    69.25240 \n    None \n    70.78541 \n    None \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    left \n    0 \n    5 \n    5 \n    3 \n    NA \n    NA \n    NA \n    NA \n    0.0323178 \n    2.032318 \n    73.24960 \n    None \n    74.45209 \n    None \n    75.48391 \n    None \n    79.99045 \n    None \n    79.99045 \n    None \n    f \n    79.99045 \n    None \n    80.80311 \n    None \n    0.7490084 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    81.30346 \n    None \n    space \n    3.187924 \n    81.30346 \n    None \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    0 \n    0 \n    18 \n    0.0160001 \n    2.016000 \n    86.52245 \n    None \n    86.89231 \n    None \n    87.92302 \n    None \n    92.92987 \n    None \n    92.92987 \n    None \n    j \n    92.92987 \n    None \n    93.70924 \n    None \n    0.7136441 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    1 \n    1 \n    31 \n    0.0318162 \n    2.031816 \n    96.17699 \n    None \n    96.54602 \n    None \n    97.57770 \n    None \n    101.58423 \n    None \n    101.58423 \n    None \n    j \n    101.58423 \n    None \n    102.26673 \n    None \n    0.6271285 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n none \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    2 \n    2 \n    66 \n    0.0321148 \n    2.032115 \n    104.76463 \n    None \n    105.13302 \n    None \n    106.16508 \n    None \n    110.67183 \n    None \n    110.67183 \n    None \n    f \n    110.67183 \n    None \n    111.38828 \n    None \n    0.6703410 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n none \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    3 \n    3 \n    75 \n    0.0321121 \n    2.032112 \n    113.88535 \n    None \n    115.08794 \n    None \n    116.11989 \n    None \n    119.52612 \n    None \n    119.52612 \n    None \n    j \n    119.52612 \n    None \n    120.15512 \n    None \n    0.5738488 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n left \n    left \n    NA \n    NA \n    NA \n    NA \n    0 \n    4 \n    4 \n    13 \n    0.0321118 \n    2.032112 \n    122.62295 \n    None \n    123.82583 \n    None \n    124.85742 \n    None \n    129.36397 \n    None \n    129.36397 \n    None \n    j \n    129.36397 \n    None \n    130.25975 \n    None \n    0.8405913 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n\n\n\n\ntestdata |> \n  slice_head(n = 12) |> \n  select(starts_with(\"main_block\")) |> \n  kbl() |> \n  kable_paper(\"striped\", full_width = FALSE) |> \n  row_spec(1:7, bold = TRUE, color = \"white\", background = \"#D7261E\")\n\n\n\n main_blocks_loop.thisRepN \n    main_blocks_loop.thisTrialN \n    main_blocks_loop.thisN \n    main_blocks_loop.thisIndex \n  \n\n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n 0 \n    0 \n    0 \n    18 \n  \n\n 0 \n    1 \n    1 \n    31 \n  \n\n 0 \n    2 \n    2 \n    66 \n  \n\n 0 \n    3 \n    3 \n    75 \n  \n\n 0 \n    4 \n    4 \n    13 \n  \n\n\n\n\nDie Variable main_blocks_loop.thisN ist die Trialnummer. Diese k√∂nnen wir verwenden, um die Zeilen auszuschliessen, die nicht zum Main Block geh√∂ren.\n\ntestdata |> \n    filter(!is.na(main_blocks_loop.thisN)) |>\n    select(-contains(\"practice_block_loop\"))\n\n# A tibble: 160 √ó 35\n   cue   direction main_blocks_loop.thisRepN main_blocks_loop.‚Ä¶ main_blocks_loo‚Ä¶\n   <chr> <chr>                         <dbl>              <dbl>            <dbl>\n 1 right right                             0                  0                0\n 2 right right                             0                  1                1\n 3 none  right                             0                  2                2\n 4 none  right                             0                  3                3\n 5 left  left                              0                  4                4\n 6 none  right                             0                  5                5\n 7 none  left                              0                  6                6\n 8 left  left                              0                  7                7\n 9 left  right                             0                  8                8\n10 none  right                             0                  9                9\n# ‚Ä¶ with 150 more rows, and 30 more variables:\n#   main_blocks_loop.thisIndex <dbl>, static_isi.started <dbl>,\n#   static_isi.stopped <dbl>, fixation_pre.started <dbl>,\n#   fixation_pre.stopped <chr>, image.started <dbl>, image.stopped <chr>,\n#   fixation_post.started <dbl>, fixation_post.stopped <chr>,\n#   dots_background.started <dbl>, dots_background.stopped <chr>,\n#   dots_stimulus.started <dbl>, dots_stimulus.stopped <chr>, ‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen",
    "href": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen",
    "title": "Daten importieren",
    "section": "Variablen ausw√§hlen",
    "text": "Variablen ausw√§hlen\n\ntestdata |>\n    select(-contains(\"static\"),\n           -contains(\"fixation\"),\n           -contains(\"image\"),\n           -contains(\"instruction\"),\n           -contains(\"feedback\"))\n\n# A tibble: 167 √ó 23\n   cue   direction practice_block_loop.thisRe‚Ä¶ practice_block_‚Ä¶ practice_block_‚Ä¶\n   <chr> <chr>                           <dbl>            <dbl>            <dbl>\n 1 none  right                               0                0                0\n 2 left  right                               0                1                1\n 3 right right                               0                2                2\n 4 left  left                                0                3                3\n 5 none  left                                0                4                4\n 6 right left                                0                5                5\n 7 <NA>  <NA>                               NA               NA               NA\n 8 right right                              NA               NA               NA\n 9 right right                              NA               NA               NA\n10 none  right                              NA               NA               NA\n# ‚Ä¶ with 157 more rows, and 18 more variables:\n#   practice_block_loop.thisIndex <dbl>, main_blocks_loop.thisRepN <dbl>,\n#   main_blocks_loop.thisTrialN <dbl>, main_blocks_loop.thisN <dbl>,\n#   main_blocks_loop.thisIndex <dbl>, dots_background.started <dbl>,\n#   dots_background.stopped <chr>, dots_stimulus.started <dbl>,\n#   dots_stimulus.stopped <chr>, dots_keyboard_response.keys <chr>,\n#   dots_keyboard_response.started <dbl>, ‚Ä¶\n\n\n\ntestdata <- testdata |>\n    select(-contains(\"static\"),\n           -contains(\"fixation\"),\n           -contains(\"image\"),\n           -contains(\"instruction\"),\n           -contains(\"feedback\"))\n\n\ntestdata\n\n# A tibble: 167 √ó 23\n   cue   direction practice_block_loop.thisRe‚Ä¶ practice_block_‚Ä¶ practice_block_‚Ä¶\n   <chr> <chr>                           <dbl>            <dbl>            <dbl>\n 1 none  right                               0                0                0\n 2 left  right                               0                1                1\n 3 right right                               0                2                2\n 4 left  left                                0                3                3\n 5 none  left                                0                4                4\n 6 right left                                0                5                5\n 7 <NA>  <NA>                               NA               NA               NA\n 8 right right                              NA               NA               NA\n 9 right right                              NA               NA               NA\n10 none  right                              NA               NA               NA\n# ‚Ä¶ with 157 more rows, and 18 more variables:\n#   practice_block_loop.thisIndex <dbl>, main_blocks_loop.thisRepN <dbl>,\n#   main_blocks_loop.thisTrialN <dbl>, main_blocks_loop.thisN <dbl>,\n#   main_blocks_loop.thisIndex <dbl>, dots_background.started <dbl>,\n#   dots_background.stopped <chr>, dots_stimulus.started <dbl>,\n#   dots_stimulus.stopped <chr>, dots_keyboard_response.keys <chr>,\n#   dots_keyboard_response.started <dbl>, ‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#variablen-umbennen",
    "href": "pages/chapters/02_importing_data.html#variablen-umbennen",
    "title": "Daten importieren",
    "section": "Variablen umbennen",
    "text": "Variablen umbennen\n\ntestdata <- testdata |>\n    select(trial = main_blocks_loop.thisN,\n           ID = Pseudonym,\n           cue,\n           direction,\n           response = dots_keyboard_response.keys,\n           rt = dots_keyboard_response.rt)\n\n\ntestdata\n\n# A tibble: 167 √ó 6\n   trial ID    cue   direction response     rt\n   <dbl> <chr> <chr> <chr>     <chr>     <dbl>\n 1    NA ZZ    none  right     None     NA    \n 2    NA ZZ    left  right     f         0.934\n 3    NA ZZ    right right     j         0.849\n 4    NA ZZ    left  left      f         0.940\n 5    NA ZZ    none  left      None     NA    \n 6    NA ZZ    right left      f         0.749\n 7    NA ZZ    <NA>  <NA>      <NA>     NA    \n 8     0 ZZ    right right     j         0.714\n 9     1 ZZ    right right     j         0.627\n10     2 ZZ    none  right     f         0.670\n# ‚Ä¶ with 157 more rows"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#neue-variablen-definieren",
    "href": "pages/chapters/02_importing_data.html#neue-variablen-definieren",
    "title": "Daten importieren",
    "section": "Neue Variablen definieren",
    "text": "Neue Variablen definieren\n\ntestdata <- testdata |>\n    mutate(choice = if_else(response == \"j\", \"right\", \"left\"),\n           response = if_else(choice == \"right\", 1, 0))\n\nAlternative:\n\ntestdata <- testdata |>\n    mutate(choice = if_else(response == \"j\", \"right\", \"left\"),\n           response = as.numeric(choice == \"right\"))\n\nWir erstellen ausserdem hier eine Variable, welche angibt, ob der Cue valid, invalid oder neutral war. Ein Cue ist genau dann valide, wenn er dieselbe Richtung hat wie der RDK Stimulus, d.h. cue == direction.\n\ntestdata <- testdata |>\n    mutate(condition = case_when(cue == \"none\" ~ \"neutral\",\n                                 cue == direction ~ \"valid\",\n                                 cue != direction ~ \"invalid\"))\n\n\ntestdata <- testdata |>\n    mutate(correct = as.numeric(choice == direction))"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#gruppierungsvariablen",
    "href": "pages/chapters/02_importing_data.html#gruppierungsvariablen",
    "title": "Daten importieren",
    "section": "Gruppierungsvariablen",
    "text": "Gruppierungsvariablen\n\nglimpse(testdata)\n\nRows: 167\nColumns: 9\n$ trial     <dbl> NA, NA, NA, NA, NA, NA, NA, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10‚Ä¶\n$ ID        <chr> \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", ‚Ä¶\n$ cue       <chr> \"none\", \"left\", \"right\", \"left\", \"none\", \"right\", NA, \"right‚Ä¶\n$ direction <chr> \"right\", \"right\", \"right\", \"left\", \"left\", \"left\", NA, \"righ‚Ä¶\n$ response  <dbl> 0, 0, 1, 0, 0, 0, NA, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,‚Ä¶\n$ rt        <dbl> NA, 0.9339199, 0.8488816, 0.9396018, NA, 0.7490084, NA, 0.71‚Ä¶\n$ choice    <chr> \"left\", \"left\", \"right\", \"left\", \"left\", \"left\", NA, \"right\"‚Ä¶\n$ condition <chr> \"neutral\", \"invalid\", \"valid\", \"valid\", \"neutral\", \"invalid\"‚Ä¶\n$ correct   <dbl> 0, 0, 1, 1, 1, 1, NA, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,‚Ä¶\n\n\n\ntestdata <- testdata |>\n    mutate_if(is.character, as.factor)\n\n\nglimpse(testdata)\n\nRows: 167\nColumns: 9\n$ trial     <dbl> NA, NA, NA, NA, NA, NA, NA, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10‚Ä¶\n$ ID        <fct> ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ‚Ä¶\n$ cue       <fct> none, left, right, left, none, right, NA, right, right, none‚Ä¶\n$ direction <fct> right, right, right, left, left, left, NA, right, right, rig‚Ä¶\n$ response  <dbl> 0, 0, 1, 0, 0, 0, NA, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,‚Ä¶\n$ rt        <dbl> NA, 0.9339199, 0.8488816, 0.9396018, NA, 0.7490084, NA, 0.71‚Ä¶\n$ choice    <fct> left, left, right, left, left, left, NA, right, right, left,‚Ä¶\n$ condition <fct> neutral, invalid, valid, valid, neutral, invalid, NA, valid,‚Ä¶\n$ correct   <dbl> 0, 0, 1, 1, 1, 1, NA, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#accuracy-pro-bedingung",
    "href": "pages/chapters/02_importing_data.html#accuracy-pro-bedingung",
    "title": "Daten importieren",
    "section": "Accuracy pro Bedingung",
    "text": "Accuracy pro Bedingung\nWir k√∂nnen nun die accuracy in jeder Cue-Bedingung berechnen. Es gibt hier zwei M√∂glichkeiten: wir berechen die Anzahl Trials (N), und die Anzahl korrekter Antworten (ncorrect) separat. Der Anteil korrekter Antworten ist dann einfach ncorrect/N. Dasselbe Ergebnis erhalten wir, wenn wir einfach den Mittelwert der korrekten Antworten nehmen.\n\ntestaccuracy <- testdata |>\n    group_by(condition) |>\n    summarise(N = n(),\n              ncorrect = sum(correct),\n              accuracy = ncorrect/N,\n              accuracy2 = mean(correct))\n\ntestaccuracy\n\n# A tibble: 4 √ó 5\n  condition     N ncorrect accuracy accuracy2\n  <fct>     <int>    <dbl>    <dbl>     <dbl>\n1 invalid      18       14    0.778     0.778\n2 neutral      82       67    0.817     0.817\n3 valid        66       62    0.939     0.939\n4 <NA>          1       NA   NA        NA"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#funktion-definieren",
    "href": "pages/chapters/02_importing_data.html#funktion-definieren",
    "title": "Daten importieren",
    "section": "Funktion definieren",
    "text": "Funktion definieren\nNun wollen wir die ersten paar Schritte gleichzeitig auf mehrere Files anwenden:\n\n\nCSV File einlesen\nFilename hinzuf√ºgen\nPractice Trials l√∂schen\nPractice Variablen l√∂schen\n\nDieser Vorgang ist in R ziemlich elegant. Anstatt dass wir manuell √ºber alle Files iterieren m√ºssen, k√∂nnen wir eine Funktion definieren, die wir auf ein File anwenden k√∂nnen, und dann wenden wir diese Funktion auf alle Files an.\n\nMit map_* Funktionen k√∂nnen wir eine Funktion auf alle Elemente einer Liste anwenden. map_dfr macht genau das, und gibt einen Dataframe als Output, in welchem die einzelnen Elemente row-wise zusamengesetzt werden.\nDie Funktion, welche wir auf ein einzelnes .csv File anweden m√∂chten, ist diese:\n\nimport_function <- function(filename) {\n    read_csv(filename) |>\n        mutate(filename = basename(filename)) |>\n        filter(!is.na(main_blocks_loop.thisN)) |>\n        select(-contains(\"practice_block_loop\"))\n}\n\n\nProbieren Sie die Funktion mit dem einzelnen .csv File von oben."
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#alle-files-in-einem-ordner-auflisten",
    "href": "pages/chapters/02_importing_data.html#alle-files-in-einem-ordner-auflisten",
    "title": "Daten importieren",
    "section": "Alle Files in einem Ordner auflisten",
    "text": "Alle Files in einem Ordner auflisten\n\ndatadir <- \"data/\"\nlist_of_files <- datadir |>\n    list.files(pattern = \"csv\", recursive = TRUE, full.names = TRUE)\n\n\nlist_of_files\n\n[1] \"data//JH_rdk-discrimination_2022_Mar_07_1403.csv\"   \n[2] \"data//NS_rdk-discrimination_2022_Mar_07_1331.csv\"   \n[3] \"data//rh_rdk-discrimination_2022_Mar_02_1105.csv\"   \n[4] \"data//sb_rdk-discrimination_2022_Mar_06_0746.csv\"   \n[5] \"data//SS91_rdk-discrimination_2022_Mar_06_0953.csv\" \n[6] \"data//VP1_rdk-discrimination_2022_Mar_07_1237.csv\"  \n[7] \"data//VP2_rdk-discrimination_2022_Mar_07_1302.csv\"  \n[8] \"data//VPN01_rdk-discrimination_2022_Mar_01_2142.csv\"\n[9] \"data//VPN02_rdk-discrimination_2022_Mar_01_2208.csv\""
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#funktion-auf-liste-anwenden",
    "href": "pages/chapters/02_importing_data.html#funktion-auf-liste-anwenden",
    "title": "Daten importieren",
    "section": "Funktion auf Liste anwenden",
    "text": "Funktion auf Liste anwenden\n\ndata <- list_of_files |> \n    map_dfr(~ import_function(.x))"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen-und-umbennen",
    "href": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen-und-umbennen",
    "title": "Daten importieren",
    "section": "Variablen ausw√§hlen und umbennen",
    "text": "Variablen ausw√§hlen und umbennen\n\ndata <- data |>\n    select(-contains(\"static\"),\n           -contains(\"fixation\"),\n           -contains(\"image\"),\n           -contains(\"instruction\"),\n           -contains(\"feedback\"))\n\n\ndata <- data |>\n    select(trial = main_blocks_loop.thisN,\n           ID = Pseudonym,\n           cue,\n           direction,\n           response = dots_keyboard_response.keys,\n           rt = dots_keyboard_response.rt)"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#neue-variablen-definieren-1",
    "href": "pages/chapters/02_importing_data.html#neue-variablen-definieren-1",
    "title": "Daten importieren",
    "section": "Neue Variablen definieren",
    "text": "Neue Variablen definieren\nKorrekte Antworten\n\ndata <- data |>\n    mutate(choice = if_else(response == \"j\", \"right\", \"left\"),\n           response = if_else(choice == \"right\", 1, 0))\n\n\ndata <- data |>\n    mutate(correct = as.numeric(choice == direction))\n\n\nglimpse(data)\n\nRows: 1,440\nColumns: 8\n$ trial     <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ ID        <chr> \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", ‚Ä¶\n$ cue       <chr> \"right\", \"right\", \"none\", \"none\", \"left\", \"none\", \"none\", \"l‚Ä¶\n$ direction <chr> \"right\", \"right\", \"right\", \"right\", \"left\", \"right\", \"left\",‚Ä¶\n$ response  <dbl> 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, ‚Ä¶\n$ rt        <dbl> 0.7136441, 0.6271285, 0.6703410, 0.5738488, 0.8405913, 0.667‚Ä¶\n$ choice    <chr> \"right\", \"right\", \"left\", \"right\", \"right\", \"right\", \"right\"‚Ä¶\n$ correct   <dbl> 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n\n\n\ndata |> \n  slice_head(n = 20)\n\n# A tibble: 20 √ó 8\n   trial ID    cue   direction response    rt choice correct\n   <dbl> <chr> <chr> <chr>        <dbl> <dbl> <chr>    <dbl>\n 1     0 JH    right right            1 0.714 right        1\n 2     1 JH    right right            1 0.627 right        1\n 3     2 JH    none  right            0 0.670 left         0\n 4     3 JH    none  right            1 0.574 right        1\n 5     4 JH    left  left             1 0.841 right        0\n 6     5 JH    none  right            1 0.668 right        1\n 7     6 JH    none  left             1 1.12  right        0\n 8     7 JH    left  left             0 0.640 left         1\n 9     8 JH    left  right            0 1.13  left         0\n10     9 JH    none  right            1 1.03  right        1\n11    10 JH    none  left             0 1.35  left         1\n12    11 JH    left  left             0 0.688 left         1\n13    12 JH    left  left             0 0.721 left         1\n14    13 JH    none  left             0 0.655 left         1\n15    14 JH    right right            1 1.02  right        1\n16    15 JH    none  right            1 1.12  right        1\n17    16 JH    left  left             0 1.08  left         1\n18    17 JH    right left             0 0.643 left         1\n19    18 JH    right right            1 0.716 right        1\n20    19 JH    left  left             0 0.578 left         1\n\n\nCue-Bedingungsvariable\n\ndata <- data |>\n    mutate(condition = case_when(cue == \"none\" ~ \"neutral\",\n                                 cue == direction ~ \"valid\",\n                                 cue != direction ~ \"invalid\"))\n\nDaten als CSV speichern\nAn dieser Stelle speichern wir den neu kreierten Datensatz als .csv File. Somit k√∂nnen wir die Daten einfach importieren, ohne die ganzen Schritte wiederholen zu m√ºssen.\n\ndata |> write_csv(file = \"data_clean/rdkdata.csv\")\n\n\ndata |> \n  slice_head(n = 20)\n\n# A tibble: 20 √ó 9\n   trial ID    cue   direction response    rt choice correct condition\n   <dbl> <chr> <chr> <chr>        <dbl> <dbl> <chr>    <dbl> <chr>    \n 1     0 JH    right right            1 0.714 right        1 valid    \n 2     1 JH    right right            1 0.627 right        1 valid    \n 3     2 JH    none  right            0 0.670 left         0 neutral  \n 4     3 JH    none  right            1 0.574 right        1 neutral  \n 5     4 JH    left  left             1 0.841 right        0 valid    \n 6     5 JH    none  right            1 0.668 right        1 neutral  \n 7     6 JH    none  left             1 1.12  right        0 neutral  \n 8     7 JH    left  left             0 0.640 left         1 valid    \n 9     8 JH    left  right            0 1.13  left         0 invalid  \n10     9 JH    none  right            1 1.03  right        1 neutral  \n11    10 JH    none  left             0 1.35  left         1 neutral  \n12    11 JH    left  left             0 0.688 left         1 valid    \n13    12 JH    left  left             0 0.721 left         1 valid    \n14    13 JH    none  left             0 0.655 left         1 neutral  \n15    14 JH    right right            1 1.02  right        1 valid    \n16    15 JH    none  right            1 1.12  right        1 neutral  \n17    16 JH    left  left             0 1.08  left         1 valid    \n18    17 JH    right left             0 0.643 left         1 invalid  \n19    18 JH    right right            1 0.716 right        1 valid    \n20    19 JH    left  left             0 0.578 left         1 valid"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#gruppierungsvariablen-1",
    "href": "pages/chapters/02_importing_data.html#gruppierungsvariablen-1",
    "title": "Daten importieren",
    "section": "Gruppierungsvariablen",
    "text": "Gruppierungsvariablen\n\ndata <- data |>\n    mutate_if(is.character, as.factor)\n\n\nglimpse(data)\n\nRows: 1,440\nColumns: 9\n$ trial     <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ ID        <fct> JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, ‚Ä¶\n$ cue       <fct> right, right, none, none, left, none, none, left, left, none‚Ä¶\n$ direction <fct> right, right, right, right, left, right, left, left, right, ‚Ä¶\n$ response  <dbl> 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, ‚Ä¶\n$ rt        <dbl> 0.7136441, 0.6271285, 0.6703410, 0.5738488, 0.8405913, 0.667‚Ä¶\n$ choice    <fct> right, right, left, right, right, right, right, left, left, ‚Ä¶\n$ correct   <dbl> 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ condition <fct> valid, valid, neutral, neutral, valid, neutral, neutral, val‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#accuracy-pro-personbedingung",
    "href": "pages/chapters/02_importing_data.html#accuracy-pro-personbedingung",
    "title": "Daten importieren",
    "section": "Accuracy pro Person/Bedingung",
    "text": "Accuracy pro Person/Bedingung\nAccuracy pro Person und pro Bedingung berechnen.\n\naccuracy <- data |>\n    group_by(ID, condition) |>\n    summarise(N = n(),\n              ncorrect = sum(correct),\n              accuracy = mean(correct))\n\n`summarise()` has grouped output by 'ID'. You can override using the `.groups`\nargument.\n\n\n\naccuracy\n\n# A tibble: 27 √ó 5\n# Groups:   ID [9]\n   ID    condition     N ncorrect accuracy\n   <fct> <fct>     <int>    <dbl>    <dbl>\n 1 JH    invalid      16       13   0.812 \n 2 JH    neutral      80       66   0.825 \n 3 JH    valid        64       60   0.938 \n 4 NS    invalid      16       11   0.688 \n 5 NS    neutral      80       56   0.7   \n 6 NS    valid        64       58   0.906 \n 7 rh    invalid      16        2   0.125 \n 8 rh    neutral      80       64   0.8   \n 9 rh    valid        64       61   0.953 \n10 sb    invalid      16        1   0.0625\n# ‚Ä¶ with 17 more rows"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#visualisieren",
    "href": "pages/chapters/02_importing_data.html#visualisieren",
    "title": "Daten importieren",
    "section": "Visualisieren",
    "text": "Visualisieren\n\naccuracy |> \n  ggplot(aes(x = condition, y = accuracy, fill = condition)) +\n  geom_col() +\n  scale_fill_manual(\n    values = c(invalid = \"#9E0142\",\n    neutral = \"#C4C4B7\",\n    valid = \"#2EC762\")\n  ) +\n  labs(\n    x = \"Cue\",\n    y = \"Proportion correct\",\n    title = \"Accuracy per person/condition\"\n  ) +\n  theme_linedraw(base_size = 28) +\n  facet_wrap(~ID)"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html",
    "href": "pages/chapters/04_summarizing_data.html",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nOb eine Variable als factor definiert ist, wird als Attribut gespeichert. Attribute werden aber in einem .csv. File nicht mitgespeichert; deshalb m√ºssen wir die Gruppierungsvariablen wieder als factor definieren."
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#pro-versuchsperson",
    "href": "pages/chapters/04_summarizing_data.html#pro-versuchsperson",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Pro Versuchsperson",
    "text": "Pro Versuchsperson\n\ndata\n\n# A tibble: 1,440 √ó 9\n   trial ID    cue   direction response    rt choice correct condition\n   <dbl> <fct> <fct> <fct>        <dbl> <dbl> <fct>    <dbl> <fct>    \n 1     0 JH    right right            1 0.714 right        1 valid    \n 2     1 JH    right right            1 0.627 right        1 valid    \n 3     2 JH    none  right            0 0.670 left         0 neutral  \n 4     3 JH    none  right            1 0.574 right        1 neutral  \n 5     4 JH    left  left             1 0.841 right        0 valid    \n 6     5 JH    none  right            1 0.668 right        1 neutral  \n 7     6 JH    none  left             1 1.12  right        0 neutral  \n 8     7 JH    left  left             0 0.640 left         1 valid    \n 9     8 JH    left  right            0 1.13  left         0 invalid  \n10     9 JH    none  right            1 1.03  right        1 neutral  \n# ‚Ä¶ with 1,430 more rows\n\n\n\ndata |> \n  group_by(ID, condition)\n\n# A tibble: 1,440 √ó 9\n# Groups:   ID, condition [27]\n   trial ID    cue   direction response    rt choice correct condition\n   <dbl> <fct> <fct> <fct>        <dbl> <dbl> <fct>    <dbl> <fct>    \n 1     0 JH    right right            1 0.714 right        1 valid    \n 2     1 JH    right right            1 0.627 right        1 valid    \n 3     2 JH    none  right            0 0.670 left         0 neutral  \n 4     3 JH    none  right            1 0.574 right        1 neutral  \n 5     4 JH    left  left             1 0.841 right        0 valid    \n 6     5 JH    none  right            1 0.668 right        1 neutral  \n 7     6 JH    none  left             1 1.12  right        0 neutral  \n 8     7 JH    left  left             0 0.640 left         1 valid    \n 9     8 JH    left  right            0 1.13  left         0 invalid  \n10     9 JH    none  right            1 1.03  right        1 neutral  \n# ‚Ä¶ with 1,430 more rows\n\n\n\naccuracy <- data |>\n    group_by(ID, condition) |>\n    summarise(N = n(),\n              ncorrect = sum(correct),\n              accuracy = mean(correct))\n\n\naccuracy\n\n# A tibble: 27 √ó 5\n# Groups:   ID [9]\n   ID    condition     N ncorrect accuracy\n   <fct> <fct>     <int>    <dbl>    <dbl>\n 1 JH    invalid      16       13   0.812 \n 2 JH    neutral      80       66   0.825 \n 3 JH    valid        64       60   0.938 \n 4 NS    invalid      16       11   0.688 \n 5 NS    neutral      80       56   0.7   \n 6 NS    valid        64       58   0.906 \n 7 rh    invalid      16        2   0.125 \n 8 rh    neutral      80       64   0.8   \n 9 rh    valid        64       61   0.953 \n10 sb    invalid      16        1   0.0625\n# ‚Ä¶ with 17 more rows"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#visualisieren",
    "href": "pages/chapters/04_summarizing_data.html#visualisieren",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Visualisieren",
    "text": "Visualisieren\n\naccuracy |> \n  ggplot(aes(x = condition, y = accuracy, fill = condition)) +\n  geom_col() +\n  geom_line(aes(group = ID), size = 2) +\n  geom_point(size = 8) +\n  scale_fill_manual(\n    values = c(invalid = \"#9E0142\",\n    neutral = \"#C4C4B7\",\n    valid = \"#2EC762\")\n  ) +\n  labs(\n    x = \"Cue\",\n    y = \"Proportion correct\",\n    title = \"Accuracy per person/condition\"\n  ) +\n  theme_linedraw(base_size = 28) +\n  facet_wrap(~ID)"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren",
    "href": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "√úber Versuchsperson aggregieren",
    "text": "√úber Versuchsperson aggregieren"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#ein-exkurs-√ºber-within-person-standardfehler",
    "href": "pages/chapters/04_summarizing_data.html#ein-exkurs-√ºber-within-person-standardfehler",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Ein Exkurs √ºber Within-person Standardfehler",
    "text": "Ein Exkurs √ºber Within-person Standardfehler\n\nlibrary(tidyverse)\n\ndfw <- tribble(\n ~subject, ~pretest, ~posttest,\n       1,   59.4,     64.5,\n       2,   46.4,     52.4,\n       3,   46.0,     49.7,\n       4,   49.0,     48.7,\n       5,   32.5,     37.4,\n       6,   45.2,     49.5,\n       7,   60.3,     59.9,\n       8,   54.3,     54.1,\n       9,   45.4,     49.6,\n      10,   38.9,     48.5) |>\n    mutate(subject = as.factor(subject))\n\n\ndfl <- dfw |>\n    pivot_longer(contains(\"test\"),\n                 names_to = \"condition\",\n                 values_to = \"value\") |>\n    mutate(condition = as_factor(condition))\n\n\ndflsum <- dfl |>\n    Rmisc::summarySEwithin(measurevar = \"value\",\n                               withinvars = \"condition\",\n                               idvar = \"subject\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\n\n\ndflsum |>\n    ggplot(aes(x = condition, y = value, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = 0.1, aes(ymin = value-ci, ymax = value+ci)) +\n    geom_point(shape = 21, size = 3, fill = \"white\") +\n    ylim(40,60)\n\n\n\n\n\n# Use a consistent y range\nymax <- max(dfl$value)\nymin <- min(dfl$value)\n\n\n# Plot the individuals\ndfl |>\n    ggplot(aes(x=condition, y=value, colour=subject, group=subject)) +\n    geom_line() + geom_point(shape=21, fill=\"white\") +\n    ylim(ymin,ymax)\n\n\n\n\n\ndfNorm_long <- Rmisc::normDataWithin(data=dfl, idvar=\"subject\", measurevar=\"value\")\n?Rmisc::normDataWithin\n\ndfNorm_long |>\n    ggplot(aes(x=condition, y=valueNormed, colour=subject, group=subject)) +\n    geom_line() + geom_point(shape=21, fill=\"white\") +\n    ylim(ymin,ymax)\n\n\n\n\n\n# Instead of summarySEwithin, use summarySE, which treats condition as though it were a between-subjects variable\ndflsum_between <- Rmisc::summarySE(data = dfl, \n                                   measurevar = \"value\", \n                                   groupvars = \"condition\", \n                                   na.rm = FALSE, \n                                   conf.interval = .95)\ndflsum_between\n\n  condition  N value       sd       se       ci\n1   pretest 10 47.74 8.598992 2.719240 6.151348\n2  posttest 10 51.43 7.253972 2.293907 5.189179\n\n\n\n# Show the between-S CI's in red, and the within-S CI's in black\ndflsum_between |>\n    ggplot(aes(x=condition, y=value, group=1)) +\n    geom_line() +\n    geom_errorbar(width=.1, aes(ymin=value-ci, ymax=value+ci), colour=\"red\") +\n    geom_errorbar(width=.1, aes(ymin=value-ci, ymax=value+ci), data=dflsum) +\n    geom_point(shape=21, size=3, fill=\"white\") +\n    ylim(ymin,ymax)"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#within-person-standardfehler",
    "href": "pages/chapters/04_summarizing_data.html#within-person-standardfehler",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Within-person Standardfehler",
    "text": "Within-person Standardfehler\n\naccuracy |> \n  ggplot(aes(x = condition, y = accuracy, colour = ID, group = ID)) +\n    geom_line() + \n  geom_point(shape=21, fill=\"white\")\n\n\n\n\nDer Standardfehler is definiert als: \\[SE = sd/ \\sqrt{n}\\]\nLeider gibt es in R keine Funktion, welche den Standardfehler berechnet (sch√§tzt); wir k√∂nnen aber ganz einfach selber eine Funktion definieren.\n\nse <- function(x) sd(x)/sqrt(length(x))\n\n\ndatasum <- data |>\n   group_by(condition) |> \n   summarise(N = n(),\n             ccuracy = mean(correct),\n             sd = sd(correct),\n             se = se(correct))\ndatasum\n\n# A tibble: 3 √ó 5\n  condition     N ccuracy    sd     se\n  <fct>     <int>   <dbl> <dbl>  <dbl>\n1 invalid     144   0.389 0.489 0.0408\n2 neutral     720   0.629 0.483 0.0180\n3 valid       576   0.825 0.381 0.0159\n\n\n\ndatasum_2 <- data |>\n    Rmisc::summarySE(measurevar = \"correct\",\n                              groupvars = \"condition\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\ndatasum_2\n\n  condition   N   correct        sd         se         ci\n1   invalid 144 0.3888889 0.4891996 0.04076663 0.08058308\n2   neutral 720 0.6291667 0.4833637 0.01801390 0.03536613\n3     valid 576 0.8246528 0.3805943 0.01585810 0.03114686\n\n\n\ndatasum_3 <- data |>\n    Rmisc::summarySEwithin(measurevar = \"correct\",\n                               withinvars = \"condition\",\n                               idvar = \"ID\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\ndatasum_3\n\n  condition   N   correct        sd         se         ci\n1   invalid 144 0.3888889 0.5773528 0.04811273 0.09510406\n2   neutral 720 0.6291667 0.5726512 0.02134145 0.04189901\n3     valid 576 0.8246528 0.4523391 0.01884746 0.03701827\n\n\n\np_accuracy <- datasum_3 |>\n    ggplot(aes(x = condition, y = correct, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = correct-se, ymax = correct+se), colour=\"red\") +\n    geom_point(shape=21, size=3, fill=\"white\")\np_accuracy"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#pro-versuchsperson-1",
    "href": "pages/chapters/04_summarizing_data.html#pro-versuchsperson-1",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Pro Versuchsperson",
    "text": "Pro Versuchsperson\nWir fassen die Daten pro Person pro Block mit Mittelwert, Median und Standarabweichung zusammen.\n\nfuns <- list(mean = mean, median = median, sd = sd)\n\nby_subj <- data %>%\n  drop_na(rt) |> \n  group_by(ID, condition) %>% \n  dplyr::summarise(across(rt, funs, .names = \"{.fn}\"))\n\n\nby_subj \n\n# A tibble: 27 √ó 5\n# Groups:   ID [9]\n   ID    condition  mean median     sd\n   <fct> <fct>     <dbl>  <dbl>  <dbl>\n 1 JH    invalid   0.775  0.739 0.163 \n 2 JH    neutral   0.799  0.733 0.202 \n 3 JH    valid     0.696  0.658 0.190 \n 4 NS    invalid   0.894  0.913 0.207 \n 5 NS    neutral   0.885  0.844 0.201 \n 6 NS    valid     0.738  0.715 0.191 \n 7 rh    invalid   0.423  0.389 0.151 \n 8 rh    neutral   0.525  0.503 0.0841\n 9 rh    valid     0.443  0.390 0.185 \n10 sb    invalid   0.376  0.341 0.0924\n# ‚Ä¶ with 17 more rows\n\n\nEinfachere Version:\n\nby_subj <- data |> \n  drop_na(rt) |> \n  group_by(ID, condition) |>  \n  dplyr::summarise(mean = mean(rt),\n                   median = median(rt),\n                   sd = sd(rt))\n\n\nby_subj |> \n  ggplot(aes(x = condition, y = mean, fill = condition)) +\n  geom_col() +\n  geom_line(aes(group = ID), size = 2) +\n  geom_point(size = 8) +\n  scale_fill_manual(\n    values = c(invalid = \"#9E0142\",\n    neutral = \"#C4C4B7\",\n    valid = \"#2EC762\")\n  ) +\n  labs(\n    x = \"Cue\",\n    y = \"Response time\") +\n  theme_linedraw(base_size = 28) +\n  facet_wrap(~ID)\n\n\n\n\n\nse <- function(x, ...) sd(x, ...)/sqrt(length(x))\n\nby_subj <- data %>% \n  group_by(ID, condition) %>% \n  summarise(mean = mean(rt, na.rm = TRUE), \n            median = median(rt, na.rm = TRUE), \n            sd = sd(rt, na.rm = TRUE), \n            se = se(rt, na.rm = TRUE))\n\n\nby_subj |> \n  ggplot(aes(condition, mean)) +\n  geom_line(aes(group = 1), linetype = 3) +    \n  geom_errorbar(aes(ymin = mean-se, ymax = mean+se),\n                width = 0.2, size=1, color=\"blue\") +\n  geom_point(size = 2) +\n  facet_wrap(~ID, scales = \"free_y\")"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren-1",
    "href": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren-1",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "√úber Versuchsperson aggregieren",
    "text": "√úber Versuchsperson aggregieren\n\nrtsum <- data |>\n  drop_na(rt) |> \n    Rmisc::summarySEwithin(measurevar = \"rt\",\n                               withinvars = \"condition\",\n                               idvar = \"ID\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\nrtsum\n\n  condition   N        rt        sd         se         ci\n1   invalid 141 0.7055247 0.2204498 0.01856522 0.03670444\n2   neutral 710 0.7238269 0.2449543 0.00919297 0.01804870\n3     valid 568 0.6716487 0.2482698 0.01041717 0.02046095\n\n\n\np_rt <- rtsum |>\n    ggplot(aes(x = condition, y = rt, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = rt-se, ymax = rt+se), colour=\"red\") +\n    geom_point(shape=21, size=3, fill=\"white\")\n\n\np_rt\n\n\n\n\n\nlibrary(patchwork)\n\n\np_accuracy / p_rt"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html",
    "href": "pages/chapters/06_signal_detection_ii.html",
    "title": "Signal Detection Theory: II",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nüëâ Daten downloaden"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#daten-importieren",
    "href": "pages/chapters/06_signal_detection_ii.html#daten-importieren",
    "title": "Signal Detection Theory: II",
    "section": "Daten importieren",
    "text": "Daten importieren\nZuerst die Daten downloaden, und speichern.\n\nlibrary(tidyverse)\nd <- read_csv(\"data/session-6.csv\")"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#variablen-bearbeiten",
    "href": "pages/chapters/06_signal_detection_ii.html#variablen-bearbeiten",
    "title": "Signal Detection Theory: II",
    "section": "Variablen bearbeiten",
    "text": "Variablen bearbeiten\nZu factor konvertieren, etc.\n\nd <- d |>\n    select(ID, condition, cue, direction, choice) |>\n    mutate(across(where(is.character), ~as_factor(.)),\n           cue = fct_relevel(cue, \"left\", \"none\", \"right\")) |>\n    drop_na()"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#trials-klassifizieren",
    "href": "pages/chapters/06_signal_detection_ii.html#trials-klassifizieren",
    "title": "Signal Detection Theory: II",
    "section": "Trials klassifizieren",
    "text": "Trials klassifizieren\nAls Hit, Miss, CR und FA.\n\nsdt <- d |>\n    mutate(type = case_when(\n        direction == \"___\" & choice == \"___\" ~ \"___\"),\n        ___,\n        ___,\n        ___)\n\n\nsdt\n\n# A tibble: 2,362 √ó 6\n   ID     condition cue   direction choice type \n   <fct>  <fct>     <fct> <fct>     <fct>  <chr>\n 1 chch04 valid     left  left      left   CR   \n 2 chch04 valid     left  left      left   CR   \n 3 chch04 valid     left  left      right  FA   \n 4 chch04 invalid   right left      left   CR   \n 5 chch04 neutral   none  left      left   CR   \n 6 chch04 valid     left  left      left   CR   \n 7 chch04 invalid   right left      left   CR   \n 8 chch04 valid     left  left      left   CR   \n 9 chch04 neutral   none  left      left   CR   \n10 chch04 neutral   none  right     left   Miss \n# ‚Ä¶ with 2,352 more rows"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-zusammenz√§hlen",
    "href": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-zusammenz√§hlen",
    "title": "Signal Detection Theory: II",
    "section": "SDT Kennzahlen zusammenz√§hlen",
    "text": "SDT Kennzahlen zusammenz√§hlen\n\nsdt_summary <- sdt |>\n    group_by(ID, cue) |>\n    count(type)\n\n\nsdt_summary\n\n# A tibble: 170 √ó 4\n# Groups:   ID, cue [45]\n   ID     cue   type      n\n   <fct>  <fct> <chr> <int>\n 1 chch04 left  CR       29\n 2 chch04 left  FA        3\n 3 chch04 left  Hit       7\n 4 chch04 left  Miss      1\n 5 chch04 none  CR       38\n 6 chch04 none  FA        2\n 7 chch04 none  Hit      34\n 8 chch04 none  Miss      6\n 9 chch04 right CR        5\n10 chch04 right FA        3\n# ‚Ä¶ with 160 more rows"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#von-wide-zu-long-konvertieren",
    "href": "pages/chapters/06_signal_detection_ii.html#von-wide-zu-long-konvertieren",
    "title": "Signal Detection Theory: II",
    "section": "Von wide zu long konvertieren",
    "text": "Von wide zu long konvertieren\n\nsdt_summary <- sdt_summary |>\n    pivot_wider(names_from = type, values_from = n)\n\n\nsdt_summary\n\n# A tibble: 45 √ó 6\n# Groups:   ID, cue [45]\n   ID     cue      CR    FA   Hit  Miss\n   <fct>  <fct> <int> <int> <int> <int>\n 1 chch04 left     29     3     7     1\n 2 chch04 none     38     2    34     6\n 3 chch04 right     5     3    25     7\n 4 chmi14 left     21    10     5     3\n 5 chmi14 none     18    19    29     7\n 6 chmi14 right     3     4    26     4\n 7 J      left     19    12     5     3\n 8 J      none     23    16    33     6\n 9 J      right     6     2    20    12\n10 jh     left     32    NA     5     3\n# ‚Ä¶ with 35 more rows"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#funktionen-definieren",
    "href": "pages/chapters/06_signal_detection_ii.html#funktionen-definieren",
    "title": "Signal Detection Theory: II",
    "section": "Funktionen definieren",
    "text": "Funktionen definieren\n\nreplace_NA <- function(x) {\n    x = ifelse(is.na(x), 0, x)\n    x\n}\n\ncorrect_zero_one <- function(x) {\n    if (identical(x, 0)) {\n        x = x + 0.001\n    } else if (identical(x, 1)) {\n        x = x - 0.001\n    }\n    x\n}"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#nas-ersetzen",
    "href": "pages/chapters/06_signal_detection_ii.html#nas-ersetzen",
    "title": "Signal Detection Theory: II",
    "section": "NAs ersetzen",
    "text": "NAs ersetzen\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(Hit, Miss, FA, CR), replace_NA))"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#hit-rate-und-false-alarm-rate-berechnen",
    "href": "pages/chapters/06_signal_detection_ii.html#hit-rate-und-false-alarm-rate-berechnen",
    "title": "Signal Detection Theory: II",
    "section": "Hit Rate und False Alarm Rate berechnen",
    "text": "Hit Rate und False Alarm Rate berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(hit_rate = ___,\n           fa_rate = ___)"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#werte-0-und-1-korrigieren",
    "href": "pages/chapters/06_signal_detection_ii.html#werte-0-und-1-korrigieren",
    "title": "Signal Detection Theory: II",
    "section": "Werte 0 und 1 korrigieren",
    "text": "Werte 0 und 1 korrigieren\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(hit_rate, fa_rate), correct_zero_one))"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#z-transformation",
    "href": "pages/chapters/06_signal_detection_ii.html#z-transformation",
    "title": "Signal Detection Theory: II",
    "section": "Z-Transformation",
    "text": "Z-Transformation\n\nsdt_summary <- sdt_summary |>\n    mutate(zhr = ___,\n           zfa = ___)"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-berechnen",
    "href": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-berechnen",
    "title": "Signal Detection Theory: II",
    "section": "SDT Kennzahlen berechnen",
    "text": "SDT Kennzahlen berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(dprime = ___,\n           k = ___,\n           c = ___) |>\n    mutate(across(c(dprime, k, c), round, 2))"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#variablen-ausw√§hlen",
    "href": "pages/chapters/06_signal_detection_ii.html#variablen-ausw√§hlen",
    "title": "Signal Detection Theory: II",
    "section": "Variablen ausw√§hlen",
    "text": "Variablen ausw√§hlen\n\nsdt_final <- sdt_summary |>\n    select(ID, cue, dprime, k, c)"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#eine-person-ausw√§hlen.",
    "href": "pages/chapters/06_signal_detection_ii.html#eine-person-ausw√§hlen.",
    "title": "Signal Detection Theory: II",
    "section": "Eine Person ausw√§hlen.",
    "text": "Eine Person ausw√§hlen.\n\nSU6460 <- d |>\n    filter(ID %in% \"SU6460\")\n\nSU6460_sdt <- sdt_final |>\n    filter(ID %in% \"SU6460\")"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#visualisieren",
    "href": "pages/chapters/06_signal_detection_ii.html#visualisieren",
    "title": "Signal Detection Theory: II",
    "section": "Visualisieren",
    "text": "Visualisieren\n\nSU6460_sdt\n\n# A tibble: 3 √ó 5\n# Groups:   ID, cue [3]\n  ID     cue   dprime     k     c\n  <fct>  <fct>  <dbl> <dbl> <dbl>\n1 SU6460 left    0.32  0    -0.16\n2 SU6460 none    0.3  -0.23 -0.38\n3 SU6460 right  -0.27 -0.67 -0.54\n\n\n\nSU6460_sdt |>\n    ggplot(aes(x = cue, y = dprime, group = 1)) +\n    geom_line() +\n    geom_point(shape = 21, size = 3, fill = \"white\")\n\n\n\n\n\nSU6460_sdt |>\n    ggplot(aes(x = cue, y = c, group = 1)) +\n    geom_line() +\n    geom_point(shape = 21, size = 3, fill = \"white\")"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#generalized-linear-model",
    "href": "pages/chapters/06_signal_detection_ii.html#generalized-linear-model",
    "title": "Signal Detection Theory: II",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nCheck levels: right muss die zweite Faktorstufe sein!\n\nlevels(SU6460$choice)\n\n[1] \"left\"  \"right\"\n\n\n\nSU6460_glm_k_left <- glm(choice ~ direction,\n                      family = binomial(link = \"probit\"),\n                      data = SU6460 |> filter(cue == \"left\"))\n\nsummary(SU6460_glm_k_left)\n\n\nCall:\nglm(formula = choice ~ direction, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"left\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4006  -1.1774   0.9695   1.1774   1.1774  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)\n(Intercept)    -1.250e-16  2.216e-01   0.000    1.000\ndirectionright  3.186e-01  5.028e-01   0.634    0.526\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.352  on 39  degrees of freedom\nResidual deviance: 54.946  on 38  degrees of freedom\nAIC: 58.946\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nSU6460_glm_k_right <- glm(choice ~ direction,\n                       family = binomial(link = \"probit\"),\n                       data = SU6460 |> filter(cue == \"right\"))\n\nsummary(SU6460_glm_k_right)\n\n\nCall:\nglm(formula = choice ~ direction, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"right\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6651  -1.4614   0.9178   0.9178   0.9178  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)\n(Intercept)      0.6745     0.4818   1.400    0.162\ndirectionright  -0.2722     0.5331  -0.511    0.610\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 50.446  on 39  degrees of freedom\nResidual deviance: 50.181  on 38  degrees of freedom\nAIC: 54.181\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nSU6460 <- SU6460 |>\n    mutate(dir = if_else(direction == \"left\", -1/2, 1/2))\n\n\nSU6460_glm_c_left <- glm(choice ~ dir,\n                       family = binomial(link = \"probit\"),\n                       data = SU6460 |> filter(cue == \"left\"))\nsummary(SU6460_glm_c_left)\n\n\nCall:\nglm(formula = choice ~ dir, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"left\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4006  -1.1774   0.9695   1.1774   1.1774  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   0.1593     0.2514   0.634    0.526\ndir           0.3186     0.5028   0.634    0.526\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.352  on 39  degrees of freedom\nResidual deviance: 54.946  on 38  degrees of freedom\nAIC: 58.946\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nSU6460_glm_c_right <- glm(choice ~ dir,\n                        family = binomial(link = \"probit\"),\n                        data = SU6460 |> filter(cue == \"right\"))\n\nsummary(SU6460_glm_c_right)\n\n\nCall:\nglm(formula = choice ~ dir, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"right\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6651  -1.4614   0.9178   0.9178   0.9178  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)   0.5384     0.2665   2.020   0.0434 *\ndir          -0.2722     0.5331  -0.511   0.6096  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 50.446  on 39  degrees of freedom\nResidual deviance: 50.181  on 38  degrees of freedom\nAIC: 54.181\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "pages/chapters/08_response_times_ii.html",
    "href": "pages/chapters/08_response_times_ii.html",
    "title": "Reaktionszeiten: II",
    "section": "",
    "text": "Hierarchical Shift Function\nWir schauen uns Daten aus einem Lexical Decision Task (Wagenmakers and Brown 2007) an, bei dem Versuchspersonen W√∂rter als entweder word oder non-word klassifizieren mussten. Es ist bekannt, dass W√∂rter welche h√§ufiger vorkommen schneller klassifiziert werden k√∂nnen, als seltene W√∂rter. In diesem Experiment mussten Versuchspersonen diesen Task unter zwei Bedingungen durchf√ºhren. In der speed Bedingung mussten sie sich so schnell wie m√∂glich entscheiden, in der accuracy Bedingung mit so wenig Fehler wie m√∂glich.\nHier untersuchen wir also den Unterschied in der Reaktionszeit zwischen zwei ‚Äúwithin‚Äù Bedingungen. Die Daten befinden sich im Package rtdists, welches zuerst installiert werden sollte.\n\nlibrary(tidyverse)\nlibrary(rtdists)\nlibrary(viridis)\n\ndata(speed_acc) \n\nspeed_acc <- speed_acc |>\n  as_tibble()\n\n\ndf_speed_acc <- speed_acc |> \n   # zwischen 180 ms and 3000 ms\n  filter(rt > 0.18, rt < 3) |> \n   # zu Character konvertieren (damit filter funktioniert)\n  mutate(across(c(stim_cat, response), as.character)) |> \n  # Korrekte Antworten\n  filter(response != 'error', stim_cat == response) |> \n  # wieder zu Factor konvertieren\n  mutate(across(c(stim_cat, response), as_factor))\n\n\ndf_speed_acc\n\n# A tibble: 27,936 √ó 9\n   id    block condition stim  stim_cat frequency   response    rt censor\n   <fct> <fct> <fct>     <fct> <fct>    <fct>       <fct>    <dbl> <lgl> \n 1 1     1     speed     5015  nonword  nw_low      nonword  0.7   FALSE \n 2 1     1     speed     6481  nonword  nw_very_low nonword  0.46  FALSE \n 3 1     1     speed     3305  word     very_low    word     0.455 FALSE \n 4 1     1     speed     4468  nonword  nw_high     nonword  0.773 FALSE \n 5 1     1     speed     1047  word     high        word     0.39  FALSE \n 6 1     1     speed     5036  nonword  nw_low      nonword  0.603 FALSE \n 7 1     1     speed     1111  word     high        word     0.435 FALSE \n 8 1     1     speed     6561  nonword  nw_very_low nonword  0.524 FALSE \n 9 1     1     speed     1670  word     high        word     0.427 FALSE \n10 1     1     speed     6207  nonword  nw_very_low nonword  0.456 FALSE \n# ‚Ä¶ with 27,926 more rows\n\n\nWir schauen uns vier Versuchspersonen grafisch an:\n\ndata_plot <- df_speed_acc |> \n  filter(id %in% c(1, 8, 11, 15))\n\ndata_plot |> \n  ggplot(aes(x = rt)) + \n    geom_histogram(aes(fill = condition), alpha = 0.5, bins = 60) + \n    facet_wrap(~id) +\n    coord_cartesian(xlim=c(0, 1.6)) +\n    scale_fill_viridis(discrete = TRUE, option = \"E\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSchauen Sie sich alle Vpn an.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWas w√ºrden Sie anhand der Histogramme erwarten?\n\n\n\n\n\n\n\n\nNote\n\n\n\nBerechnen Sie nun die Differenzen der Dezile zwischen den Bedingungen f√ºr jede Versuchsperson.\n\n\n\nout_speed_acc <- rogme::hsf_pb(df_speed_acc, rt ~ condition + id)\n\n\np_speed_acc <- rogme::plot_hsf_pb(out_speed_acc, interv = \"ci\")\np_speed_acc\n\n\n\n\nIn dieser Grafik sehen wir auf der X-Achse die Dezile der accuracy Bedingung und auf der Y-Achse die Differenz accuracy - speed. Die Differenz ist bei jedem Dezil positiv und scheint steig gr√∂sser zu werden. Die accuracy Bedingung f√ºhrt also zu l√§ngeren und variableren Reaktionszeiten. Die Bedingungen unterscheiden sich im Median, aber wenn wir nur das ber√ºcksichtigt h√§tten, w√ºrden wir verpassen, dass sich die Verteilungen sehr stark am rechten Ende der Verteilung unterscheiden.\nZum Vergleich berechnen wir noch Bedingungsmittelwerte der Median Reaktionszeiten.\n\nby_subject <- df_speed_acc |> \n  group_by(id, condition) |> \n  summarise(mean = median(rt))\n\nagg <- Rmisc::summarySEwithin(by_subject,\n                       measurevar = \"mean\",\n                       withinvars = \"condition\",\n                       idvar = \"id\",\n                       na.rm = FALSE,\n                       conf.interval = .95)\n\n\nagg |> \n  ggplot(aes(condition, mean, fill = condition)) +\n  geom_col(alpha = 0.8) +\n  geom_line(aes(group = 1), linetype = 3) +   \n  geom_errorbar(aes(ymin = mean-se, ymax = mean+se),\n                width = 0.1, size=1, color=\"black\") +\n  scale_fill_viridis(discrete=TRUE, option=\"cividis\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nReferences\n\nWagenmakers, Eric-Jan, and Scott Brown. 2007. ‚ÄúOn the Linear Relation Between the Mean and the Standard Deviation of a Response Time Distribution.‚Äù Psychological Review 114 (3): 830‚Äì41. https://doi.org/10.1037/0033-295X.114.3.830.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {Reaktionszeiten: {II}},\n  date = {2022-04-12},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/08_response_times_ii.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 2022. ‚ÄúReaktionszeiten: II.‚Äù April 12, 2022.\nhttps://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/08_response_times_ii.html."
  },
  {
    "objectID": "pages/chapters/07_response_times_i.html",
    "href": "pages/chapters/07_response_times_i.html",
    "title": "Reaktionszeiten: I",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nüëâ Daten downloaden"
  },
  {
    "objectID": "pages/chapters/07_response_times_i.html#zusammenfassen-zentrale-tendenz-und-dispersion",
    "href": "pages/chapters/07_response_times_i.html#zusammenfassen-zentrale-tendenz-und-dispersion",
    "title": "Reaktionszeiten: I",
    "section": "Zusammenfassen: zentrale Tendenz und Dispersion",
    "text": "Zusammenfassen: zentrale Tendenz und Dispersion\nMittelwert und Standardabweichung\n\nd %>% \n  group_by(group) %>% \n  summarise(mean = mean(rt),\n            sd = sd(rt))\n\n# A tibble: 2 √ó 3\n  group    mean    sd\n  <fct>   <dbl> <dbl>\n1 control  822.  171.\n2 adhd     839.  214.\n\n\nMedian und Interquartilsbereich\nDer Interquartilsbereich repr√§ntiert den Unterschied zwischen dem ersten (25. Perzentil) und dritten (75. Perzentil) Quartil. In diesem Bereich befinden sich 50% der Datenpunkte.\n\nd %>% \n  group_by(group) %>% \n  summarise(mean = median(rt),\n            q25 = quantile(rt, probs = 0.25),\n            q75 = quantile(rt, probs = 0.75)) |> \n  mutate(IQR = q75 - q25)\n\n# A tibble: 2 √ó 5\n  group    mean   q25   q75   IQR\n  <fct>   <dbl> <dbl> <dbl> <dbl>\n1 control  782.  694.  898.  205.\n2 adhd     784.  694.  923.  228.\n\n\n\nd %>% \n  group_by(group) %>% \n  summarise(mean = median(rt),\n            IQR = IQR(rt))\n\n# A tibble: 2 √ó 3\n  group    mean   IQR\n  <fct>   <dbl> <dbl>\n1 control  782.  205.\n2 adhd     784.  228.\n\n\n\nfuns <- list(mean = mean, median = median, \n             sd = sd, IQR = IQR)\n\nby_group <- d |>\n  group_by(group) |>\n  summarise(across(rt, funs, .names = \"{.fn}\")) |>\n  mutate(across(where(is.numeric), ~round(., 2)))\n\n\nby_group\n\n# A tibble: 2 √ó 5\n  group    mean median    sd   IQR\n  <fct>   <dbl>  <dbl> <dbl> <dbl>\n1 control  822.   782.  171.  205.\n2 adhd     839.   784.  214.  228.\n\n\n\np1 +\n  geom_vline(aes(xintercept = mean), \n             data = by_group,\n             color = \"steelblue\",\n             lwd = 1.5) +\n  geom_vline(aes(xintercept = median), \n             data = by_group, \n             color = \"red\",\n             lwd = 1.5)\n\n\n\n\nZentrale Tendenz bei schiefen Verteilungen\nSowohl Mittelwert als auch Median sind jedoch problematisch als Masse der zentralen Tendenz f√ºr asymmetrische Verteilungen. Der Mittelwert kann durch eine hohe Schiefe und Ausreissern verschoben werden, und repr√§sentiert die zentrale Tendenz der Verteilung nicht besonders gut.\nDer Median ist ein besseres Mass f√ºr eine typische Beobachtung aus dieser Verteilung, ist jedoch nicht erwartungstreu, das heisst der Median √ºbersch√§tzt den Populationsmedian. Der Grad der √úbersch√§tzung steigt mit sinkender Anzahl Beobachtungen (d.h. vor allem bei kleinen Stichproben).\nQuantile\n\ndeciles <- seq(0.1, 0.9, length.out = 9)\n\n\nquantile_fun <- function(x, probs = c(0.25, 0.5, 0.75)) {\n  tibble(rt = quantile(x, probs, type = 8), quantile = probs)\n}\n\n\nd_quantiles <- d %>% \n  group_by(group) %>% \n  summarise(quantile_fun(rt, probs = deciles))\n\n\nd_quantiles\n\n# A tibble: 18 √ó 3\n# Groups:   group [2]\n   group      rt quantile\n   <fct>   <dbl>    <dbl>\n 1 control  649.      0.1\n 2 control  684.      0.2\n 3 control  711.      0.3\n 4 control  740.      0.4\n 5 control  782.      0.5\n 6 control  826.      0.6\n 7 control  875.      0.7\n 8 control  949.      0.8\n 9 control 1045.      0.9\n10 adhd     631.      0.1\n11 adhd     674.      0.2\n12 adhd     712.      0.3\n13 adhd     745.      0.4\n14 adhd     784.      0.5\n15 adhd     828.      0.6\n16 adhd     874.      0.7\n17 adhd     980.      0.8\n18 adhd    1108.      0.9\n\n\nShift function\nWir m√ºssen nun den Dataframe mit den Quantilen ins ‚Äúwide‚Äù Format konvertieren, um zwei Spalten f√ºr die control und adhd Gruppen zu erhalten. Danach k√∂nnen wir die Differenzen zwischen den Gruppen f√ºr jedes Quantil berechnen.\n\nd_quantile_differences <- d_quantiles |> \n  pivot_wider(names_from = \"group\", values_from = \"rt\") \n\n\nd_quantile_differences\n\n# A tibble: 9 √ó 3\n  quantile control  adhd\n     <dbl>   <dbl> <dbl>\n1      0.1    649.  631.\n2      0.2    684.  674.\n3      0.3    711.  712.\n4      0.4    740.  745.\n5      0.5    782.  784.\n6      0.6    826.  828.\n7      0.7    875.  874.\n8      0.8    949.  980.\n9      0.9   1045. 1108.\n\n\n\nd_quantile_differences <- d_quantile_differences |> \n    mutate(`control - adhd` = control - adhd)\n\n\nd_quantile_differences\n\n# A tibble: 9 √ó 4\n  quantile control  adhd `control - adhd`\n     <dbl>   <dbl> <dbl>            <dbl>\n1      0.1    649.  631.            18.2 \n2      0.2    684.  674.            10.6 \n3      0.3    711.  712.            -1.45\n4      0.4    740.  745.            -4.99\n5      0.5    782.  784.            -2.16\n6      0.6    826.  828.            -1.57\n7      0.7    875.  874.             1.42\n8      0.8    949.  980.           -30.4 \n9      0.9   1045. 1108.           -62.8 \n\n\nShift function grafisch darstellen\n\nd_quantile_differences %>% \n  ggplot(aes(x = control, y = `control - adhd`)) +\n  geom_hline(yintercept = 0, linetype = 3) +\n  geom_vline(xintercept = d_quantile_differences %>% \n               filter(quantile == \"50%\") %>% \n               select(adhd) %>% \n               pull(), linetype = 3) +\n  geom_line(aes(group = 1), color = \"steelblue\", size = 2) +\n  geom_point(shape = 21, color = \"steelblue\", fill = \"white\", size = 5, stroke = 1) +\n  coord_cartesian(ylim = c(-300, 300))\n\n\n\n\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"GRousselet/rogme\")\n\n\n# library(rogme)\n\n\nout <- rogme::shifthd(d, rt ~ group)\n\n\nrogme::plot_sf(out)\n\n[[1]]"
  },
  {
    "objectID": "pages/chapters/05_signal_detection_i.html",
    "href": "pages/chapters/05_signal_detection_i.html",
    "title": "Signal Detection Theory: I",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden"
  },
  {
    "objectID": "pages/chapters/05_signal_detection_i.html#parameter-recovery-1",
    "href": "pages/chapters/05_signal_detection_i.html#parameter-recovery-1",
    "title": "Signal Detection Theory: I",
    "section": "Parameter recovery",
    "text": "Parameter recovery\nWe can now attempt to recover the known parameters c and d' from the observed hit and false alarm rates.\n\nyes_observer <- yes_observer |>\n    mutate(hit_rate = Hit/(Hit + Miss),\n           fa_rate = FA/(FA + CR))\n\nyes_observer <- yes_observer |>\n    mutate(zhr = qnorm(hit_rate),\n           zfa = qnorm(fa_rate))\n\nyes_observer <- yes_observer |>\n    mutate(dprime = zhr - zfa,\n           k = - zfa,\n           c = -0.5 * (zhr + zfa)) |>\n    mutate(across(c(dprime, c), round, 2))\n\n\nyes_observer \n\n# A tibble: 1 √ó 11\n    Hit  Miss    FA    CR hit_rate fa_rate   zhr   zfa dprime      k     c\n  <int> <dbl> <int> <dbl>    <dbl>   <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n1    92     8    74    26     0.92    0.74  1.41 0.643   0.76 -0.643 -1.02\n\n\nFor the biased observer, the valuues we used were \\(d' = 1\\) and \\(c = -1\\). Are we able to recover these?\n\nyes_observer |> pull(c, dprime)\n\n 0.76 \n-1.02 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy is it seemingly difficult to recover theses parameters?"
  },
  {
    "objectID": "pages/chapters/03_data_cleaning.html",
    "href": "pages/chapters/03_data_cleaning.html",
    "title": "Data cleaning",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nNun wollen wir versuchen, einzelne Trials, zu identifizieren, in denen Versuchpersonen nicht aufgepasst haben, oder einfach geraten wurde.\nAm h√§ufigsten werden die folgenden beiden Kriterien verwendet, um entweder einzelne Datenpunkte, oder Versuchspersonen, auszuschliessen:\nNun ist in Experimenten, in denen ein Bias erzeugt wird, etwas heikel, Trials oder Versuchspersonen aufgrund der Anzahl korrekter Antworten auszuschliessen - wir haben ja die Korrektheit der Antworten experimentell manipuliert.\nDeswegen richten wir hier unseren Fokus auf die Reaktionszeiten. Wir gehen davon aus, dass Reaktionszeiten, die zu schnell oder yu langsam waren, aufgrund von Rateprozessen zustande kamen. Was genau zu schnell oder zu langsam heisst, ist schwierig zu beantworten, und h√§ngt stark vom jeweiligen Task ab. Deshalb ist es wichtig, sich a priori Gedanken dar√ºber zu machen, welche Kriterien angewandt werden sollen."
  },
  {
    "objectID": "pages/chapters/03_data_cleaning.html#eigenschaften-von-reaktionszeiten",
    "href": "pages/chapters/03_data_cleaning.html#eigenschaften-von-reaktionszeiten",
    "title": "Data cleaning",
    "section": "Eigenschaften von Reaktionszeiten",
    "text": "Eigenschaften von Reaktionszeiten\nDie wichtigsten Merkmale von Reaktionszeiten sind\n\nSie sind rechtsschief\nSie sind nicht normalverteilt\nStreuung (Standardabweichung) steigt ungef√§hr linear mit wachsendem Mittelwert (Wagenmakers and Brown 2007)\n\n\nDie Rechtschiefe ist eine nat√ºrliche Konsequenz der Tatsache, dass es viele M√∂glichkeiten gibt, langsamer zu werden, aber nur wenige M√∂glichkeiten, schneller zu werden. Reaktionszeiten k√∂nnen nicht negativ sein Ausserdem gibt es eine Untergrenze, welche durch unsere Physiologie bestimmt ist. Schellere Reaktionszeiten als 200 Millisekunden sind kaum m√∂glich.\nDie Konsequenz daraus ist, dass Reaktionszeiten nicht normalverteilt sind. In folgender Grafik sind zwei Verteilungen dargestellt. Die gelbe Verteilung ist eine Normalverteilung mit \\(\\mu = 1\\) und \\(\\sigma = 0.4\\), w√§hrend die graue Verteilung eine LogNormal Verteilung darstellt.\n\nEine LogNormal-Verteilung bedeutet, dass der Logarithmus einer Zufallsvariablen normalverteilt ist.\n\n\n\n\n\nObwohl die Normalverteilung so aussieht, als k√∂nne sie Reaktionszeiten repr√§sentieren, ist der Wertebereich von \\([-\\Inf, \\Inf]\\) nicht daf√ºr geeignet. Ausserdem erlaubt die Normalverteilung keine extremen Werte, und ist nicht asymmetrisch."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html",
    "href": "pages/chapters/01_psychopy_experiments.html",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "",
    "text": "In dieser Sitzung erstellen wir ein perzeptuelles Entscheidungsexperiment, √§hnlich dem Experiment aus Mulder et al. (2012).\nDas Experiment ist eine Reaktionszeit (RT) Version eines Random-dot Motion Direction Discrimination Task, und wurde im Scanner und ausserhalb durchgef√ºhrt. Die beiden Version unterscheiden sich ganz stark in ihrem Timing. Wir implementieren hier die Scanner Version des Tasks.\nBias (Vorwissen) wurde durch einen Hinweisreiz angezeigt, in Form eines Pfeils oder eines neutralen Stimulus. Der Pfeil zeigte die wahrscheinlichere Bewegungsrichtung an. Vor und nach dem Cue wurde ein Fixationskreuz gezeigt. Alle weiteren Parameter k√∂nnen Sie dem Paper entnehmen (Mulder et al. 2012)."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#trial",
    "href": "pages/chapters/01_psychopy_experiments.html#trial",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "Trial",
    "text": "Trial\nZun√§chst wird ein Fixationskreuz entweder f√ºr 100 ms, 350 ms, 800 ms oder 1200 ms angezeigt. Die tats√§chliche Dauer wird f√ºr jeden Versuch randomisiert. Eine solche Randomisierung kann nicht √ºber die Benutzeroberfl√§che vorgenommen werden, sondern erfordert ein kleines St√ºck Python-Code. Sehen Sie sich den Codeblock der Routine Fixation_pre_cue an, um zu erfahren, wie dies erreicht werden kann.\nAnschlie√üend wird f√ºr 1000 ms ein Hinweis pr√§sentiert. Dabei kann es sich entweder um einen Pfeil handeln, der nach rechts zeigt, einen Pfeil, der nach links zeigt, oder einen einfachen Kreis (f√ºr die Kontrollbedingung). Der Codeblock in der Cue-Routine legt den tats√§chlichen Hinweis f√ºr jeden Versuch auf der Grundlage der Schleifenvariablen cue fest.\nNach dem Cue wird ein weiteres Fixationskreuz pr√§sentiert - dieses Mal f√ºr entweder 3400ms, 4000ms, 4500ms oder 5000ms. Wie beim ersten Fixationskreuz wird die tats√§chliche Dauer zuf√§llig gew√§hlt.\nNach dem zweiten Fixationskreuz wird f√ºr 1500 ms der eigentliche Stimulus angezeigt: ein random dot kinematogram (RDK). Die Punkte bewegen sich entweder nach rechts oder nach links mit einem Koh√§renzniveau von 8%. Die Bewegungsrichtung eines einzelnen Versuchs wird durch die Schleifenvariable direction bestimmt und im Codeblock der Routine Dots festgelegt. Die Teilnehmer m√ºssen entscheiden, welche Richtung sie wahrnehmen, und k√∂nnen ihre Antwort durch Dr√ºcken der linken oder rechten Pfeiltaste auf der Tastatur eingeben.\nSchlie√ülich wird ein Feedback-Bildschirm angezeigt. Wenn der Teilnehmer innerhalb der ersten 100 ms geantwortet hat, wird der Hinweis ‚Äúzu schnell‚Äù angezeigt. Wurde w√§hrend des gesamten Stimulus keine Antwort erfasst, wird das Wort ‚Äúmiss‚Äù angezeigt. War die Antwort richtig, wird ‚Äú+5 Punkte‚Äù angezeigt, war sie falsch, wird ‚Äú+0 Punkte*‚Äù angezeigt."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#main_blocks_loop",
    "href": "pages/chapters/01_psychopy_experiments.html#main_blocks_loop",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "main_blocks_loop",
    "text": "main_blocks_loop\nMit loops in PsychoPy haben wir die M√∂glichkeit, eine oder mehrere Routinen zu wiederholen. In diesem Experiment wird dies genutzt, um denselben Versuch (wie oben beschrieben) mehrfach zu zeigen, aber jedes Mal mit anderen Werten f√ºr die loop variables. Eine Schleife wiederholt also einen Versuch einige Male, wobei die Schleifenvariablen bei jeder Wiederholung ge√§ndert werden. Der Versuch selbst wiederum liest diese Schleifenvariablen aus, um z.B. zu wissen, ob sich die Punkte nach rechts oder nach links bewegen sollen. Hier wird nur die main_blocks_loop erkl√§rt, aber das Prinzip gilt auch f√ºr die practice_block_loop.\nUm die verschiedenen Werte f√ºr die Schleifenvariablen zu definieren, m√ºssen wir eine einfache CSV-Datei erstellen:\ncue,direction\nleft,right\nleft,left\nnone,right\n...\nDiese CSV-Datei (die Bedingungsdatei) definiert die beiden loop Variablen cue und direction. Das Stichwort kann entweder left, right oder none, sein, w√§hrend die Richtung left oder right sein kann.\nIn der Benutzeroberfl√§che k√∂nnen wir die Variablen loopType und nReps f√ºr die Schleife angeben, wenn wir sie anklicken. Mit ersterer k√∂nnen wir steuern, ob wir z.B. die Zeilen in der Bedingungsdatei mischen oder sie sequentiell von oben nach unten ablaufen lassen wollen, w√§hrend die letztere definiert, wie oft jede Zeile der Bedingungsdatei wiederholt werden soll.\nF√ºr die main_blocks_loop haben wir eine Bedingungsdatei mit 80 Zeilen, die 40 neutralen Versuchen und 40 verzerrten Versuchen entsprechen. In der einen H√§lfte der neutralen Trials bewegen sich die Punkte nach rechts, in der anderen H√§lfte nach links. Bei den voreingenommenen Versuchen sind 32 der Hinweise g√ºltig (d.¬†h. sie stimmen mit der Bewegungsrichtung der Punkte √ºberein) und 16 ung√ºltig, wobei sich die Punkte sowohl bei g√ºltigen als auch bei ung√ºltigen Hinweisen in 50 % der Versuche nach rechts und in den anderen 50 % der Versuche nach links bewegen.\nDie Variable nReps wird auf 2 gesetzt, so dass alle diese Reihen zweimal durchlaufen werden (insgesamt 160 Versuche), und die Variable ‚ÄúloopType‚Äù wird auf random gesetzt, so dass die Versuche in zuf√§lliger Reihenfolge durchgef√ºhrt werden."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#daten",
    "href": "pages/chapters/01_psychopy_experiments.html#daten",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "Daten",
    "text": "Daten\nWenn man die default-Einstellungen nicht √§ndert, speichert PsychoPy die Daten automatisch in einem trial-by-trial CSV File. Dieses CSV File erh√§lt einen Namen, der sich aus der Versuchspersonen-ID, dem Namen des Experiments, und dem aktuellen Datum inkl. Uhrzeit zusammensetzt. So ist es m√∂glich, mit derselben Versuchspersonen-ID beliebig oft das Experiment zu wiederholen. Die CSV Files werden in einem Ordner mit dem Name data abgelegt."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#degrees-of-visual-angle",
    "href": "pages/chapters/01_psychopy_experiments.html#degrees-of-visual-angle",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "Degrees of Visual Angle",
    "text": "Degrees of Visual Angle\nOftmals werden Gr√∂ssenangaben von Stimuli noch in Pixel oder Zentimeter, sondern in degrees of visual angle gemacht. Dies hat den Vorteil, dass die Angaben nicht vom Monitor selber oder der Entferung vom Monitor abh√§ngig sind. degrees of visual angle gibt die wahrgenommene Gr√∂sse des Stimulus an, und ber√ºcksichtigt die Gr√∂sse des Monitors und des Stimulus, und die Entfernung der Versuchsperson vom Monitor. Weitere Informationen dazu finden Sie auf der Website von üëâ OpenSesame. √úblicherweise entspricht ein degrees of visual angle etwa einem cm bei einer Entfernung von 57 cm vom Monitor.\nZur Umrechnung zwischen cm und degrees of visual angle finden Sie unter diesem üëâ Link mehr Information.\n\nOpenSesame ist ein weiteres, Python-basierendes Programm f√ºr die Erstellung behaviouraler Experimente."
  },
  {
    "objectID": "pages/admin/03_zulip_forum.html",
    "href": "pages/admin/03_zulip_forum.html",
    "title": "Zulip Forum",
    "section": "",
    "text": "Zulip ist besser geeignet, um Code darzustellen.\nWir benutzen dasselbe Forum f√ºr die Vormittags- und Nachmittagsveranstaltungen.\nDie Diskussion ist f√ºr alle Teilnehmer*innen sichtbar.\nDiskussion kann in Echtzeit (synchron) oder offline (asynchron) stattfinden.\n\nBitte erstellen Sie unter diesem Link einen Account. Sie m√ºssen daf√ºr Ihre Uni Emailadresse verwenden. Account erstellen üëâ zulipchat.com/join/hyuinbg3mtcumccnzt3tpsqb/\n Wenn Sie einen Account erstellt haben, k√∂nnen Sie sich unter folgendem Link einloggen. Zulip Forum üëâ neuroscicomplab2022.zulipchat.com\nAusserdem ist Zulip als Desktop oder Mobile App f√ºr alle g√§ngigen Betriebssysteme erh√§ltlich. Apps üëâ zulip.com/apps\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis,\n  author = {Andrew Ellis},\n  title = {Zulip {Forum}},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/03_zulip_forum.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. n.d. ‚ÄúZulip Forum.‚Äù https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/03_zulip_forum.html."
  },
  {
    "objectID": "pages/admin/01_overview.html",
    "href": "pages/admin/01_overview.html",
    "title": "√úbersicht",
    "section": "",
    "text": "In diesem Kurs besch√§ftigen wir uns im weiteren Sinne mit Model-based Cognitive Neuroscience. Dieses Forschungsgebiet existiert noch nicht sehr lange, und ist aus dem Zusammenschluss von mathematischer Modellierung und neurowissenschaftlichen Methoden entstanden.\nWir widmen uns dem behavioralen/kognitiven Teil dieses Forschungsgebiets. Das bedeutet, wir analysieren Daten aus Verhaltensexperimenten ‚Äî sowohl mit herk√∂mmlichen statistischen Verfahren, als auch mit mathematischen Modellen. Die Resultate dieser Analysen k√∂nnen wiederum in der Analyse bildgebender Verfahren oder EEG benutzt werden.\n\nEs gibt ein sehr gutes Lehrbuch (Forstmann and Wagenmakers 2015) zum Thema Model-based Cognitive Neuroscience; wir werden einzelne Themen daraus aufgreifen. Das Buch ist auf SpringerLink verf√ºgbar: An Introduction to Model-Based Cognitive Neuroscience.\n\nWir werden folgende Themen im Laufe des Semester behandeln:\n\nErstellen von behavioralen Experimenten\nImportieren und Bearbeiten von Daten (z.B. bin√§re Daten, Reaktionszeiten)\nGraphische Darstellung und explorative Datenanalyse\nAuswahl von statistischen Verfahren\nEinf√ºhrung in die Bayesianische Datenanalyse\nAnalyse messwiederholter Daten anhand von Multilevel Modellen\nKognitive Prozessmodelle (mathematische Modelle von Entscheidungsverhalten)"
  },
  {
    "objectID": "pages/admin/01_overview.html#experimente",
    "href": "pages/admin/01_overview.html#experimente",
    "title": "√úbersicht",
    "section": "Experimente",
    "text": "Experimente\nUm ein Experiment zu kreieren benutzen wir PsychoPy. PsychoPy ist ein Python-basiertes Tool, mit dem sich sowohl in einer grafischen Benutzeroberfl√§che (GUI) als auch mit Python Code Experimente programmieren lassen."
  },
  {
    "objectID": "pages/admin/01_overview.html#datenanalyse",
    "href": "pages/admin/01_overview.html#datenanalyse",
    "title": "√úbersicht",
    "section": "Datenanalyse",
    "text": "Datenanalyse\nUm Daten zu verarbeiten (data cleaning), grafisch darzustellen und zu analysieren werden wir R verwenden. Sie sollten daher die aktuelle Version von R installieren (Version 4.2.0), sowie RStudio.\nR üëâ https://cloud.r-project.org/\nRStudio üëâ https://www.rstudio.com/products/rstudio/download/#download\nF√ºr Bayesianische Datenanalyse verwenden wir ausserdem JASP und Stan. JASP ist ein GUI Programm, √§hnlich wie Jamovi, mit dem sich simple Bayesianische Tests durchf√ºhren lassen.\nJASP üëâ https://jasp-stats.org/download/\nStan ist eine probabilistische Programmiersprache, welche wir von R aus benutzen. Die daf√ºr ben√∂tigte Software werden wir im Verlauf des Semesters installieren."
  },
  {
    "objectID": "pages/admin/02_assessments.html",
    "href": "pages/admin/02_assessments.html",
    "title": "Leistungskontrollen",
    "section": "",
    "text": "Der Zweck dieser √úbungen ist, das Gelernte selber anzuwenden, oder dies zumindest zu versuchen. Es gibt f√ºr viele dieser √úbungen nicht eine definitive, richtige Antwort - es geht vor allem darum, es selber zu versuchen. Bei einzureichenden √úbungen gibt es die M√∂glichkeit, diese falls n√∂tig (nach Verbesserung) ein zweites Mal einzureichen.\nDie √úbungen sollen jeweils in dem entsprechenden Ordner auf ILIAS hochgeladen werden, und zwar in Form eines R Scripts, oder als Rmarkdown File.\nILIAS (Vormittag) üëâ 468703-FS2022-0\nILIAS (Nachmittag) üëâ 468703-FS2022-1\n\nEin gute Einf√ºhrung in Rmarkdown finden Sie z.B. hier.\n\nFalls mehrere Files abgegeben werden, sollte unbedingt alles in einem ZIP File komprimiert werden. Sie k√∂nnen auch eine Word/Libreoffice Datei abgeben; bitte f√ºgen Sie aber keinen R Code in ein Word Dokument ein.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis,\n  author = {Andrew Ellis},\n  title = {Leistungskontrollen},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/02_assessments.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. n.d. ‚ÄúLeistungskontrollen.‚Äù https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/02_assessments.html."
  },
  {
    "objectID": "pages/solutions/solution_03.html",
    "href": "pages/solutions/solution_03.html",
    "title": "√úbung 3: L√∂sung",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {√úbung 3: {L√∂sung}},\n  date = {04/03/2022},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/solutions/solution_03.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 4AD‚Äì3AD. ‚Äú√úbung 3: L√∂sung.‚Äù 4AD‚Äì3AD. https://kogpsy.github.io/neuroscicomplabFS22//pages/solutions/solution_03.html."
  },
  {
    "objectID": "pages/exercises/exercise_01.html",
    "href": "pages/exercises/exercise_01.html",
    "title": "√úbung 1",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {√úbung 1},\n  date = {2022-02-22},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 2022. ‚Äú√úbung 1.‚Äù February 22, 2022. https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_01.html."
  },
  {
    "objectID": "pages/exercises/exercise_03.html",
    "href": "pages/exercises/exercise_03.html",
    "title": "√úbung 3",
    "section": "",
    "text": "Note\n\n\n\nDie Aufgaben, die Sie bearbeiten sollen, finden Sie in einem gelben Kasten. Optionale Aufgaben sind in orangen K√§sten.\nIn diesem File finden Sie Beispielscode. Manche Zeilen enthalten ___. Hier m√ºssen Sie den Code vervollst√§ndigen.\nLaden Sie bitte Ihre L√∂sung als ZIP File bis Freitag, 25.03.2022, um 00:00 Uhr, in den Order f√ºr √úbung 3 auf ILIAS. Das ZIP File sollte ein R Skript enthalten, sowie den bereinigten Datensatz.\nNennen Sie Ihr File Matrikelnummer_Nachname_uebung-3.zip."
  },
  {
    "objectID": "pages/exercises/exercise_03.html#aufgaben",
    "href": "pages/exercises/exercise_03.html#aufgaben",
    "title": "√úbung 3",
    "section": "Aufgaben",
    "text": "Aufgaben\n\n\n\n\n\n\nNote\n\n\n\nAufgabe 1\n\nSpeichern Sie das CSV File in Ihren Projektordner.\nLesen Sie das CSV File ein. Per Konvention verwenden wir den Variablennamen d f√ºr den Datensatz.\n√úberpr√ºfen Sie, ob alle Variablen vorhanden sind. Verwenden Sie z.B. die Funktion glimpse().\nKonvertieren Sie die Gruppierungsvariablen subject und condition zu Faktoren.\n\n\n\n\nlibrary(tidyverse)\n\n\nd <- read_csv(\"___\")\n\nSchauen Sie sich die Variablen an:\n\nglimpse(d)\n\nKonvertieren Sie die Gruppierungsvariablen zu Faktoren:\n\nd <- d |>\n    mutate(___ = as_factor(___),\n           ___ = as_factor(___))\n\n\n\n\n\n\n\nNote\n\n\n\nAufgabe 2\nGibt es Versuchspersonen die in einer der Bedingungen Reaktionszeiten hat, welche mehr als zwei Standardabweichungen √ºber dem Bedingungsmittelwert liegen?\n\n\n\n# summary stats (means) for subjects/conditions\nsum_stats_participants <- d |>\n    group_by(___, ___) |>\n    dplyr::summarise(\n        mean_P = mean(___))\n\n\n# summary stats (means and SDs) for conditions\nsum_stats_conditions <- d |>\n    group_by(___) |>\n    dplyr::summarise(\n        mean_C = mean(__),\n        sd_C = sd(___))\n\n\nsum_stats_participants <-\n    full_join(\n        sum_stats_participants,\n        sum_stats_conditions,\n        by = \"condition\") |>\n    mutate(outlier_P = ___)\n\n\n# show outlier participants\nsum_stats_participants |>\n    filter(outlier_P == 1) |>\n    show()\n\n\nexcluded <- sum_stats_participants |>\n    filter(outlier_P == 1)\n\nexcluded\n\n\nd_cleaned <- d |>\n    filter(!(subject %in% excluded$subject)) |>\n    mutate(subject = fct_drop(subject))\n\n\n\n\n\n\n\nNote\n\n\n\nAufgabe 3\n\nGibt es einzelne Trials, in denen Versuchpersonen l√§nger als 4 Standardabweichungen √ºber dem Bedingungsmittelwert gebraucht haben, um zu Antworten?\nGibt es einzelne Trials, in denen Versuchpersonen zu schnell (unter 100 ms) geantwortet haben?\nSpeichern Sie den bearbeiteten Datensatz als CSV File.\n\n\n\n\nd_cleaned <- d_cleaned |>\n    full_join(\n        sum_stats_conditions,\n        by = \"condition\") |>\n    mutate(\n        trial_type = case_when(\n            ___ > ___ ~ \"too slow\",\n            ___ < ___ ~ \"too fast\",\n            TRUE ~ \"OK\") |>\n            factor(levels = c(\"OK\", \"too fast\", \"too slow\")))\n\n\nd_cleaned |>\n    ggplot(aes(x = trial_num, y = rt, color = trial_type, shape = trial_type)) +\n    geom_point(alpha = 0.6) +\n    facet_grid(~condition) +\n    scale_color_manual(values = c(\"gray70\", \"red\", \"steelblue\"))\n\n\nd_cleaned |>\n    filter(trial_type != \"OK\")\n\n\nd_cleaned <- d_cleaned |>\n    filter(trial_type == \"OK\") |>\n    select(subject, trial_num, condition, signal_present, correct, rt)\n\n\nd_cleaned |>\n    ggplot(aes(x = trial_num, y = rt)) +\n    geom_point(alpha = 0.6) +\n    facet_grid(~condition) +\n    scale_color_manual(values = c(\"gray70\", \"red\", \"steelblue\"))\n\n\nd_cleaned |> write_csv(___)\n\n\n\n\n\n\n\nTip\n\n\n\nOptionale Aufgabe\nDie Aufgaben oben bieten lediglich Voschl√§ge, wie man ‚ÄúAusreisser‚Äù identifizieren k√∂nnte. Wenn Sie andere Voschl√§ge haben, k√∂nnen Sie den Code anpassen, oder selber Code schreiben. K√∂nnen Sie Ihr Vorgehen begr√ºnden?"
  },
  {
    "objectID": "pages/exercises/exercise_02.html",
    "href": "pages/exercises/exercise_02.html",
    "title": "√úbung 2",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {√úbung 2},\n  date = {2022-03-01},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 2022. ‚Äú√úbung 2.‚Äù March 1, 2022. https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_02.html."
  },
  {
    "objectID": "pages/exercises/exercise_05.html",
    "href": "pages/exercises/exercise_05.html",
    "title": "√úbung 5",
    "section": "",
    "text": "Note\n\n\n\nDie Daten f√ºr diese √úbung finden Sie hier: üëâ Download Data\nDie Aufgaben, die Sie bearbeiten sollen, finden Sie in einem gelben Kasten. Optionale Aufgaben sind in orangen K√§sten.\nIn diesem File finden Sie Beispielscode. Manche Zeilen enthalten ___. Hier m√ºssen Sie den Code vervollst√§ndigen.\nLaden Sie bitte Ihre L√∂sung als R Skript bis Mittwoch, 27.4.2022, um 00:30 Uhr, in den Order f√ºr √úbung 5 auf ILIAS.\nNennen Sie Ihr File Matrikelnummer_Nachname_uebung-5.R."
  },
  {
    "objectID": "pages/exercises/exercise_05.html#stroop-task",
    "href": "pages/exercises/exercise_05.html#stroop-task",
    "title": "√úbung 5",
    "section": "Stroop Task",
    "text": "Stroop Task\nDer durchgef√ºhre Task ist eine Standardversion eines Stroop Tasks:\n\nStroop task. Participants performed the color-word version of the Stroop task (Botvinick et al.¬†2001; Gratton et al.¬†1992; Macleod 1991; Stroop 1935) comprised of congruent, incongruent, and neutral conditions while in the MR scanner. Participants were instructed to ignore the meaning of the printed word and respond to the ink color in which the word was printed. For example, in the congruent condition the words ‚ÄúRED,‚Äù ‚ÄúGREEN,‚Äù and ‚ÄúBLUE‚Äù were displayed in the ink colors red, green, and blue, respectively. In this condition, attentional demands were low because the ink color matched the prepotent response of reading the word, so response conflict was at a minimum. However, for the incongruent condition the printed words were different from the ink color in which they were printed (e.g., the word ‚ÄúRED‚Äù printed in blue ink). This condition elicited conflict because responding according to the printed word would result in an incorrect response. As a result, attentional demands were high and participants needed to inhibit the prepotent response of reading the word and respond according to the ink color in which the word was printed. On the other hand, the neutral condition consisted of noncolor words presented in an ink color (e.g., the word ‚ÄúCHAIR‚Äù printed in red ink) and had a low level of conflict and low attentional demands.\n\nVersuchspersonen musste mit einer Response Box eine von drei Tasten dr√ºcken. Es wurde festgehalten, ob die Antwort korrekt war.\n\nParticipants were instructed to respond to the ink color in which the text appeared by pressing buttons under the index, middle, and ring fingers on their right hand, each button corresponding to one of the three colors (red, green, and blue, respectively) on an MR-safe response box. The task was briefly practiced in the scanner to acquaint the participant with the task and to ensure understanding of the instructions. The task began with the presentation of a fixation cross hair for 1,000 ms followed by the Stroop stimulus for 2,000 ms, during which participants were instructed to respond as quickly as possible. A total of 120 trials were presented to each participant (42 congruent, 42 neutral, 36 incongruent). A lower number of incongruent trials was used in order to reduce the expectancy of a stimulus conflict relative to the other conditions."
  },
  {
    "objectID": "pages/exercises/exercise_05.html#datenanalyse",
    "href": "pages/exercises/exercise_05.html#datenanalyse",
    "title": "√úbung 5",
    "section": "Datenanalyse",
    "text": "Datenanalyse\nVon Interesse sind die Reaktionszeiten der korrekten Antworten in den Bedingungen kongruent, inkongruent und neutral.\n\nBehavioral analysis. The primary behavioral variable of interest was response time (RT), recorded as the time between cue onset and registered key press (in milliseconds). All first-level analyses were restricted to correct responses. To determine condition-level effects, a one-way repeated-measures ANOVA was used, as were post hoc one-sample t-tests."
  },
  {
    "objectID": "pages/exercises/exercise_05.html#daten-importieren",
    "href": "pages/exercises/exercise_05.html#daten-importieren",
    "title": "√úbung 5",
    "section": "Daten importieren",
    "text": "Daten importieren\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úî ggplot2 3.3.5     ‚úî purrr   0.3.4\n‚úî tibble  3.1.6     ‚úî dplyr   1.0.8\n‚úî tidyr   1.2.0     ‚úî stringr 1.4.0\n‚úî readr   2.1.2     ‚úî forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n\nAngenommen, Sie haben das CSV File in einem Subordner namens data gespeichert:\n\nd <- read_csv(\"data/stroop-data.csv\")\n\nRows: 3337 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): ID, condition, correct\ndbl (1): RT\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nglimpse(d)\n\nRows: 3,337\nColumns: 4\n$ ID        <chr> \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001‚Ä¶\n$ condition <chr> \"neutral\", \"congruent\", \"congruent\", \"neutral\", \"neutral\", \"‚Ä¶\n$ correct   <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", ‚Ä¶\n$ RT        <dbl> 1.186, 0.667, 0.614, 0.696, 0.752, 0.864, 0.793, 0.662, 0.71‚Ä¶\n\n\n\nd <- d |> \n    mutate(across(where(is.character), ~as_factor(.)),\n           condition = fct_relevel(condition, \"congruent\"))\n\nIm Datensatz befinden sich die Daten 28 Personen:\n\nlength(levels(d$ID))\n\n[1] 28\n\n\nDie Variable correct ist mit ‚ÄúY‚Äù und ‚ÄúN‚Äù codiert. Wir bevorzugen die Werte 1 und 0.\n\nd <- d |> \n  mutate(correct = if_else(correct == \"Y\", 1, 0))"
  },
  {
    "objectID": "pages/exercises/exercise_05.html#mittlere-rt-vs-fehlerrate",
    "href": "pages/exercises/exercise_05.html#mittlere-rt-vs-fehlerrate",
    "title": "√úbung 5",
    "section": "Mittlere RT vs Fehlerrate",
    "text": "Mittlere RT vs Fehlerrate\nWir plotten zuerst median RT versus Fehlerrate (beides pro VP/Bedingung), um uns einen √úberblick zu verschaffen.\n\nd_individual_summary <- d |> \n  group_by(ID, condition) |>        \n  summarize(RT = median(RT),\n            error_rate = 1 - mean(correct))\n\n`summarise()` has grouped output by 'ID'. You can override using the `.groups`\nargument.\n\nhead(d_individual_summary)\n\n# A tibble: 6 √ó 4\n# Groups:   ID [2]\n  ID    condition      RT error_rate\n  <fct> <fct>       <dbl>      <dbl>\n1 001   congruent   0.788     0     \n2 001   neutral     0.817     0.0952\n3 001   incongruent 0.953     0.167 \n4 002   congruent   0.774     0.146 \n5 002   neutral     0.839     0.286 \n6 002   incongruent 0.908     0.389 \n\n\n\nd_individual_summary |> \n  ggplot(aes(x = RT, y = error_rate)) +\n  geom_point() +\n  facet_wrap(~condition)\n\n\n\n\nEs sieht aus, als sei die Fehlerrate im Mittel in der inkongruenten Bedingung gr√∂sser als in den kongruenten neutralen Bedingungen. Dies ist zu erwarten.\nWir wollen hier nur die Reaktionszeiten korrekter Antworten analysieren. Filtern Sie den Datensatz, so dass nur noch die korrekten Antworten bleiben.\n\nd <- d |> \n  filter(correct == 1)"
  },
  {
    "objectID": "pages/exercises/exercise_05.html#rt-histogramme",
    "href": "pages/exercises/exercise_05.html#rt-histogramme",
    "title": "√úbung 5",
    "section": "RT Histogramme",
    "text": "RT Histogramme\nPlotten Sie mit folgender Funktion die RT Histogramme zuf√§llig ausgew√§hlter Personen (28 VP sind zuviele, um sie so in einer Grafik darzustellen).\n\nplot_hist <- function(d) {\n  d |> \n  ggplot(aes(x = RT,\n             fill = condition,\n             color = condition,)) + \n    geom_histogram(aes(y = ..density..), \n                   alpha = 0.5, bins = 30) + \n    # facet_grid(condition~ID) +\n      facet_wrap(~ID) +\n    coord_cartesian(xlim=c(0, 1.8)) +\n    scale_fill_viridis(discrete = TRUE, option = \"E\") +\n    scale_color_viridis(discrete = TRUE, option = \"E\")\n}\n\n\nd |> \n  filter(ID %in% sample(levels(ID), 4)) |> \n  plot_hist()"
  },
  {
    "objectID": "pages/exercises/exercise_05.html#aufgaben",
    "href": "pages/exercises/exercise_05.html#aufgaben",
    "title": "√úbung 5",
    "section": "Aufgaben",
    "text": "Aufgaben\nAufgabe 1\n\n\nBerechnen Sie die mittlere RT pro Peron/Bedingung, und stellen Sie die mittlere RT pro Bedingung, gemittelt √ºber Personen, mit Fehlerbalken grafisch dar.\nBeschreiben Sie in 1-2 S√§tzen, was Sie gefunden haben.\n\n\n\nby_subject <- d |> \n  group_by(___) |> \n  summarise(RT = ___)\n\nagg <- Rmisc::summarySEwithin(by_subject,\n                       measurevar = \"RT\",\n                       withinvars = \"condition\",\n                       idvar = \"ID\",\n                       na.rm = FALSE,\n                       conf.interval = .95)\n\n\nagg |> \n  ggplot(aes(condition, RT)) +\n  geom_line(aes(group = 1), linetype = 3) +   \n  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),\n                width = 0.1, size=1, color=\"black\") +\n  geom_point(size = 4) +\n  theme(legend.position = \"none\")\n\nAufgabe 2\n\n\nUntersuchen Sie mittels einer Shift Function den Unterschied zwischen den Bedingungen congruent und incongruent.\nBeschreiben Sie in 1-2 S√§tzen, was Sie gefunden haben.\nPratte et al. (2010) haben festgestellt, dass Stroop Effekte bei l√§ngeren Reaktionszeiten ansteigen. Ist diese Aussage mit Ihren Befunden vereinbar?\n\n\nZuerst entfernen wir die Faktorstufe neutral:\n\ndd <- d |> \n  filter(!(condition %in% \"neutral\")) |> \n  mutate(condition = fct_drop(condition))\n\nNun verwenden wir die Funktion hsf() aus dem rogme Package, um Shift Functions f√ºr jede Person in den beiden Bedingungen zu berechnen, und die gemittelten Differenzen der Dezile zu erhalten.\nDie Formel, welche Sie f√ºr die Funktion brauchen, lautet\nRT ~ condition + ID \nDiese Formel ist so zu lesen: RT Quantile werden durch condition und ID vorhergesagt.\n\nout <- dd |> \n  rogme::hsf(___)\n\nStellen Sie die Shift Function grafisch dar.\n\nrogme::plot_hsf(out)"
  },
  {
    "objectID": "pages/exercises/exercise_04.html",
    "href": "pages/exercises/exercise_04.html",
    "title": "√úbung 4",
    "section": "",
    "text": "In dieser √úbung berechnen aus den Daten von 15 Versuchspersonen aus dem PsychoPy Experiment die Signal Detection Kennzahlen \\(d'\\), \\(k\\) und \\(c\\). Anschliessen berechnen Sie Mittelwerte der drei Bedingungen f√ºr \\(d'\\) und \\(c\\) unter Ber√ºcksichtigung der Messwiederholung."
  },
  {
    "objectID": "pages/exercises/exercise_04.html#variablen-bearbeiten",
    "href": "pages/exercises/exercise_04.html#variablen-bearbeiten",
    "title": "√úbung 4",
    "section": "Variablen bearbeiten",
    "text": "Variablen bearbeiten\nZu factor konvertieren, etc.\n\nd <- d |>\n    select(ID, condition, cue, direction, choice) |>\n    mutate(across(where(is.character), ~as_factor(.)),\n           cue = fct_relevel(cue, \"left\", \"none\", \"right\")) |>\n    drop_na()\n\n\nsdt <- d |>\n    mutate(type = case_when(\n        direction == \"right\" & choice == \"right\" ~ \"Hit\",\n        direction == \"right\" & choice == \"left\" ~ \"Miss\",\n        direction == \"left\" & choice == \"left\" ~ \"CR\",\n        direction == \"left\" & choice == \"right\" ~ \"FA\"))\n\nF√ºr jede Vpn in jeder der drei cue Bedingungen die verschiedenen Antworttypen z√§hlen.\n\nsdt_summary <- sdt |>\n    group_by(ID, cue) |>\n    count(type)\n\n\nsdt_summary\n\n# A tibble: 170 √ó 4\n# Groups:   ID, cue [45]\n   ID     cue   type      n\n   <fct>  <fct> <chr> <int>\n 1 chch04 left  CR       29\n 2 chch04 left  FA        3\n 3 chch04 left  Hit       7\n 4 chch04 left  Miss      1\n 5 chch04 none  CR       38\n 6 chch04 none  FA        2\n 7 chch04 none  Hit      34\n 8 chch04 none  Miss      6\n 9 chch04 right CR        5\n10 chch04 right FA        3\n# ‚Ä¶ with 160 more rows"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#von-wide-zu-long-konvertieren",
    "href": "pages/exercises/exercise_04.html#von-wide-zu-long-konvertieren",
    "title": "√úbung 4",
    "section": "Von wide zu long konvertieren",
    "text": "Von wide zu long konvertieren\n\nsdt_summary <- sdt_summary |>\n    pivot_wider(names_from = type, values_from = n)\n\n\nsdt_summary\n\n# A tibble: 45 √ó 6\n# Groups:   ID, cue [45]\n   ID     cue      CR    FA   Hit  Miss\n   <fct>  <fct> <int> <int> <int> <int>\n 1 chch04 left     29     3     7     1\n 2 chch04 none     38     2    34     6\n 3 chch04 right     5     3    25     7\n 4 chmi14 left     21    10     5     3\n 5 chmi14 none     18    19    29     7\n 6 chmi14 right     3     4    26     4\n 7 J      left     19    12     5     3\n 8 J      none     23    16    33     6\n 9 J      right     6     2    20    12\n10 jh     left     32    NA     5     3\n# ‚Ä¶ with 35 more rows"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#funktionen-definieren",
    "href": "pages/exercises/exercise_04.html#funktionen-definieren",
    "title": "√úbung 4",
    "section": "Funktionen definieren",
    "text": "Funktionen definieren\n\nreplace_NA <- function(x) {\n    x = ifelse(is.na(x), 0, x)\n    x\n}\n\ncorrect_zero_one <- function(x) {\n    if (identical(x, 0)) {\n        x = x + 0.001\n    } else if (identical(x, 1)) {\n        x = x - 0.001\n    }\n    x\n}"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#nas-ersetzen",
    "href": "pages/exercises/exercise_04.html#nas-ersetzen",
    "title": "√úbung 4",
    "section": "NAs ersetzen",
    "text": "NAs ersetzen\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(Hit, Miss, FA, CR), replace_NA))"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#hit-rate-und-false-alarm-rate-berechnen",
    "href": "pages/exercises/exercise_04.html#hit-rate-und-false-alarm-rate-berechnen",
    "title": "√úbung 4",
    "section": "Hit Rate und False Alarm Rate berechnen",
    "text": "Hit Rate und False Alarm Rate berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(hit_rate = ___,\n           fa_rate = ___)"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#werte-0-und-1-korrigieren",
    "href": "pages/exercises/exercise_04.html#werte-0-und-1-korrigieren",
    "title": "√úbung 4",
    "section": "Werte 0 und 1 korrigieren",
    "text": "Werte 0 und 1 korrigieren\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(hit_rate, fa_rate), correct_zero_one))"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#z-transformation",
    "href": "pages/exercises/exercise_04.html#z-transformation",
    "title": "√úbung 4",
    "section": "Z-Transformation",
    "text": "Z-Transformation\n\nsdt_summary <- sdt_summary |>\n    mutate(zhr = qnorm(hit_rate),\n           zfa = qnorm(fa_rate))\n\n\nsdt_summary\n\n# A tibble: 45 √ó 10\n# Groups:   ID, cue [45]\n   ID     cue      CR    FA   Hit  Miss hit_rate fa_rate   zhr     zfa\n   <fct>  <fct> <int> <dbl> <int> <dbl>    <dbl>   <dbl> <dbl>   <dbl>\n 1 chch04 left     29     3     7     1    0.875  0.0938 1.15  -1.32  \n 2 chch04 none     38     2    34     6    0.85   0.05   1.04  -1.64  \n 3 chch04 right     5     3    25     7    0.781  0.375  0.776 -0.319 \n 4 chmi14 left     21    10     5     3    0.625  0.323  0.319 -0.460 \n 5 chmi14 none     18    19    29     7    0.806  0.514  0.862  0.0339\n 6 chmi14 right     3     4    26     4    0.867  0.571  1.11   0.180 \n 7 J      left     19    12     5     3    0.625  0.387  0.319 -0.287 \n 8 J      none     23    16    33     6    0.846  0.410  1.02  -0.227 \n 9 J      right     6     2    20    12    0.625  0.25   0.319 -0.674 \n10 jh     left     32     0     5     3    0.625  0.001  0.319 -3.09  \n# ‚Ä¶ with 35 more rows"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#sdt-kennzahlen-berechnen",
    "href": "pages/exercises/exercise_04.html#sdt-kennzahlen-berechnen",
    "title": "√úbung 4",
    "section": "SDT Kennzahlen berechnen",
    "text": "SDT Kennzahlen berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(dprime = ___,\n           k = ___,\n           c = ___) |>\n    mutate(across(c(dprime, k, c), round, 2))"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#variablen-ausw√§hlen",
    "href": "pages/exercises/exercise_04.html#variablen-ausw√§hlen",
    "title": "√úbung 4",
    "section": "Variablen ausw√§hlen",
    "text": "Variablen ausw√§hlen\n\nsdt_final <- sdt_summary |>\n    select(ID, cue, dprime, k, c)\n\nIm finalen Datensatz haben wir nun d', k und c f√ºr jede Person in jeder Bedingung.\n\nsdt_final\n\n# A tibble: 45 √ó 5\n# Groups:   ID, cue [45]\n   ID     cue   dprime     k     c\n   <fct>  <fct>  <dbl> <dbl> <dbl>\n 1 chch04 left    2.47  1.32  0.08\n 2 chch04 none    2.68  1.64  0.3 \n 3 chch04 right   1.1   0.32 -0.23\n 4 chmi14 left    0.78  0.46  0.07\n 5 chmi14 none    0.83 -0.03 -0.45\n 6 chmi14 right   0.93 -0.18 -0.65\n 7 J      left    0.61  0.29 -0.02\n 8 J      none    1.25  0.23 -0.4 \n 9 J      right   0.99  0.67  0.18\n10 jh     left    3.41  3.09  1.39\n# ‚Ä¶ with 35 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWir erwarten, dass sich d' zwischen den Bedingungen nicht unterscheidet. k und c (bias) sollte sich hingegen zwischen den cue Bedingungen unterscheiden. Uns interessiert hier vor allem c: in der neutralen Bedingung sollte c etwa 0 sein, in der ‚Äòleft‚Äô Bedingung sollte \\(c > 0\\) sein, und in der ‚Äòright‚Äô Bedingung sollte \\(c < 0\\) sein.\nVersuchen Sie die untenstehende Grafiken f√ºr d' und c zu reproduzieren.\n\n\n\n\n\n\n\nSie brauchen zuerst eine (separate) Zusammenfassung der d' und c Werte, welche die Messwiederholung respektiert. Sie k√∂nnen dazu die Funktion summarySEwithin aus dem Rmisc Package verwenden.\nDie Funktion braucht die Argumente measurevar, withinvars und idvar.\n\n\n\n\n\n\nArgument\nBeschreibung\n\n\n\nmeasurevar\nVariable, f√ºr welche eine Messwiederholung vorliegt\n\n\nwithinvars\nMesswiederholung\n\n\nidvar\nIdentit√§t der messwiederholten Einheit\n\n\n\n\ncs <- sdt_final |>\n    select(ID, cue, c) |>\n    ___\n\n\ndprimes <- sdt_final |>\n    select(ID, cue, ___) |>\n    ___\n\nWenn Sie, wie ich, die Datens√§tze mit den Mittelwerten, Standardfehlern und \\(95%\\) Konfidenzintervallen primes und cs genannt haben, k√∂nnen Sie die Plots beispielsweise so erstellen.\n\ncs |>\n    ggplot(aes(x = cue, y = c, group = 1)) + \n    geom_hline(yintercept = 0, \n               linetype = \"dashed\",\n               color = \"grey60\") +\n    geom_line() +\n    geom_errorbar(width = 0.1, aes(ymin = c - ci,\n                                   ymax = c + ci)) +\n    geom_point(shape = 21, size = 3, fill = \"white\") +\n    ggtitle(\"c (bias)\")\n\nFalls Sie wollen, k√∂nnen Sie die individuellen c Sch√§tzungen dem Plot hinzuf√ºgen, mit folgendem Code:\ngeom_jitter(aes(cue, c), data = sdt_final, width = 0.05)"
  }
]