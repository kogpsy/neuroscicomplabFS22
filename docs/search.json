[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neurowissenschaft Computerlab",
    "section": "",
    "text": "Fr√ºhjahrssemester 2022"
  },
  {
    "objectID": "pages/admin/01_overview.html",
    "href": "pages/admin/01_overview.html",
    "title": "√úbersicht",
    "section": "",
    "text": "In diesem Kurs besch√§ftigen wir uns im weiteren Sinne mit Model-based Cognitive Neuroscience. Dieses Forschungsgebiet existiert noch nicht sehr lange, und ist aus dem Zusammenschluss von mathematischer Modellierung und neurowissenschaftlichen Methoden entstanden.\nWir widmen uns dem behavioralen/kognitiven Teil dieses Forschungsgebiets. Das bedeutet, wir analysieren Daten aus Verhaltensexperimenten ‚Äî sowohl mit herk√∂mmlichen statistischen Verfahren, als auch mit mathematischen Modellen. Die Resultate dieser Analysen k√∂nnen wiederum in der Analyse bildgebender Verfahren oder EEG benutzt werden.\n\nEs gibt ein sehr gutes Lehrbuch (Forstmann and Wagenmakers 2015) zum Thema Model-based Cognitive Neuroscience; wir werden einzelne Themen daraus aufgreifen. Das Buch ist auf SpringerLink verf√ºgbar: An Introduction to Model-Based Cognitive Neuroscience.\n\nWir werden folgende Themen im Laufe des Semester behandeln:\n\nErstellen von behavioralen Experimenten\nImportieren und Bearbeiten von Daten (z.B. bin√§re Daten, Reaktionszeiten)\nGraphische Darstellung und explorative Datenanalyse\nAuswahl von statistischen Verfahren\nEinf√ºhrung in die Bayesianische Datenanalyse\nAnalyse messwiederholter Daten anhand von Multilevel Modellen\nKognitive Prozessmodelle (mathematische Modelle von Entscheidungsverhalten)"
  },
  {
    "objectID": "pages/admin/01_overview.html#experimente",
    "href": "pages/admin/01_overview.html#experimente",
    "title": "√úbersicht",
    "section": "Experimente",
    "text": "Experimente\nUm ein Experiment zu kreieren benutzen wir PsychoPy. PsychoPy ist ein Python-basiertes Tool, mit dem sich sowohl in einer grafischen Benutzeroberfl√§che (GUI) als auch mit Python Code Experimente programmieren lassen."
  },
  {
    "objectID": "pages/admin/01_overview.html#datenanalyse",
    "href": "pages/admin/01_overview.html#datenanalyse",
    "title": "√úbersicht",
    "section": "Datenanalyse",
    "text": "Datenanalyse\nUm Daten zu verarbeiten (data cleaning), grafisch darzustellen und zu analysieren werden wir R verwenden. Sie sollten daher die aktuelle Version von R installieren (Version 4.2.0), sowie RStudio.\nR üëâ https://cloud.r-project.org/\nRStudio üëâ https://www.rstudio.com/products/rstudio/download/#download\nF√ºr Bayesianische Datenanalyse verwenden wir ausserdem JASP und Stan. JASP ist ein GUI Programm, √§hnlich wie Jamovi, mit dem sich simple Bayesianische Tests durchf√ºhren lassen.\nJASP üëâ https://jasp-stats.org/download/\nStan ist eine probabilistische Programmiersprache, welche wir von R aus benutzen. Die daf√ºr ben√∂tigte Software werden wir im Verlauf des Semesters installieren."
  },
  {
    "objectID": "pages/admin/02_assessments.html",
    "href": "pages/admin/02_assessments.html",
    "title": "Leistungskontrollen",
    "section": "",
    "text": "Der Zweck dieser √úbungen ist, das Gelernte selber anzuwenden, oder dies zumindest zu versuchen. Es gibt f√ºr viele dieser √úbungen nicht eine definitive, richtige Antwort - es geht vor allem darum, es selber zu versuchen. Bei einzureichenden √úbungen gibt es die M√∂glichkeit, diese falls n√∂tig (nach Verbesserung) ein zweites Mal einzureichen.\nDie √úbungen sollen jeweils in dem entsprechenden Ordner auf ILIAS hochgeladen werden, und zwar in Form eines R Scripts, oder als Rmarkdown File.\nILIAS (Vormittag) üëâ 468703-FS2022-0\nILIAS (Nachmittag) üëâ 468703-FS2022-1\n\nEin gute Einf√ºhrung in Rmarkdown finden Sie z.B. hier.\n\nFalls mehrere Files abgegeben werden, sollte unbedingt alles in einem ZIP File komprimiert werden. Sie k√∂nnen auch eine Word/Libreoffice Datei abgeben; bitte f√ºgen Sie aber keinen R Code in ein Word Dokument ein.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis,\n  author = {Andrew Ellis},\n  title = {Leistungskontrollen},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/02_assessments.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. n.d. ‚ÄúLeistungskontrollen.‚Äù https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/02_assessments.html."
  },
  {
    "objectID": "pages/admin/03_zulip_forum.html",
    "href": "pages/admin/03_zulip_forum.html",
    "title": "Zulip Forum",
    "section": "",
    "text": "Zulip ist besser geeignet, um Code darzustellen.\nWir benutzen dasselbe Forum f√ºr die Vormittags- und Nachmittagsveranstaltungen.\nDie Diskussion ist f√ºr alle Teilnehmer*innen sichtbar.\nDiskussion kann in Echtzeit (synchron) oder offline (asynchron) stattfinden.\n\nBitte erstellen Sie unter diesem Link einen Account. Sie m√ºssen daf√ºr Ihre Uni Emailadresse verwenden. Account erstellen üëâ zulipchat.com/join/hyuinbg3mtcumccnzt3tpsqb/\n Wenn Sie einen Account erstellt haben, k√∂nnen Sie sich unter folgendem Link einloggen. Zulip Forum üëâ neuroscicomplab2022.zulipchat.com\nAusserdem ist Zulip als Desktop oder Mobile App f√ºr alle g√§ngigen Betriebssysteme erh√§ltlich. Apps üëâ zulip.com/apps\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis,\n  author = {Andrew Ellis},\n  title = {Zulip {Forum}},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/03_zulip_forum.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. n.d. ‚ÄúZulip Forum.‚Äù https://kogpsy.github.io/neuroscicomplabFS22//pages/admin/03_zulip_forum.html."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html",
    "href": "pages/chapters/01_psychopy_experiments.html",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "",
    "text": "In dieser Sitzung erstellen wir ein perzeptuelles Entscheidungsexperiment, √§hnlich dem Experiment aus Mulder et al. (2012).\nDas Experiment ist eine Reaktionszeit (RT) Version eines Random-dot Motion Direction Discrimination Task, und wurde im Scanner und ausserhalb durchgef√ºhrt. Die beiden Version unterscheiden sich ganz stark in ihrem Timing. Wir implementieren hier die Scanner Version des Tasks.\nBias (Vorwissen) wurde durch einen Hinweisreiz angezeigt, in Form eines Pfeils oder eines neutralen Stimulus. Der Pfeil zeigte die wahrscheinlichere Bewegungsrichtung an. Vor und nach dem Cue wurde ein Fixationskreuz gezeigt. Alle weiteren Parameter k√∂nnen Sie dem Paper entnehmen (Mulder et al. 2012)."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#trial",
    "href": "pages/chapters/01_psychopy_experiments.html#trial",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "Trial",
    "text": "Trial\nZun√§chst wird ein Fixationskreuz entweder f√ºr 100 ms, 350 ms, 800 ms oder 1200 ms angezeigt. Die tats√§chliche Dauer wird f√ºr jeden Versuch randomisiert. Eine solche Randomisierung kann nicht √ºber die Benutzeroberfl√§che vorgenommen werden, sondern erfordert ein kleines St√ºck Python-Code. Sehen Sie sich den Codeblock der Routine Fixation_pre_cue an, um zu erfahren, wie dies erreicht werden kann.\nAnschlie√üend wird f√ºr 1000 ms ein Hinweis pr√§sentiert. Dabei kann es sich entweder um einen Pfeil handeln, der nach rechts zeigt, einen Pfeil, der nach links zeigt, oder einen einfachen Kreis (f√ºr die Kontrollbedingung). Der Codeblock in der Cue-Routine legt den tats√§chlichen Hinweis f√ºr jeden Versuch auf der Grundlage der Schleifenvariablen cue fest.\nNach dem Cue wird ein weiteres Fixationskreuz pr√§sentiert - dieses Mal f√ºr entweder 3400ms, 4000ms, 4500ms oder 5000ms. Wie beim ersten Fixationskreuz wird die tats√§chliche Dauer zuf√§llig gew√§hlt.\nNach dem zweiten Fixationskreuz wird f√ºr 1500 ms der eigentliche Stimulus angezeigt: ein random dot kinematogram (RDK). Die Punkte bewegen sich entweder nach rechts oder nach links mit einem Koh√§renzniveau von 8%. Die Bewegungsrichtung eines einzelnen Versuchs wird durch die Schleifenvariable direction bestimmt und im Codeblock der Routine Dots festgelegt. Die Teilnehmer m√ºssen entscheiden, welche Richtung sie wahrnehmen, und k√∂nnen ihre Antwort durch Dr√ºcken der linken oder rechten Pfeiltaste auf der Tastatur eingeben.\nSchlie√ülich wird ein Feedback-Bildschirm angezeigt. Wenn der Teilnehmer innerhalb der ersten 100 ms geantwortet hat, wird der Hinweis ‚Äúzu schnell‚Äù angezeigt. Wurde w√§hrend des gesamten Stimulus keine Antwort erfasst, wird das Wort ‚Äúmiss‚Äù angezeigt. War die Antwort richtig, wird ‚Äú+5 Punkte‚Äù angezeigt, war sie falsch, wird ‚Äú+0 Punkte*‚Äù angezeigt."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#main_blocks_loop",
    "href": "pages/chapters/01_psychopy_experiments.html#main_blocks_loop",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "main_blocks_loop",
    "text": "main_blocks_loop\nMit loops in PsychoPy haben wir die M√∂glichkeit, eine oder mehrere Routinen zu wiederholen. In diesem Experiment wird dies genutzt, um denselben Versuch (wie oben beschrieben) mehrfach zu zeigen, aber jedes Mal mit anderen Werten f√ºr die loop variables. Eine Schleife wiederholt also einen Versuch einige Male, wobei die Schleifenvariablen bei jeder Wiederholung ge√§ndert werden. Der Versuch selbst wiederum liest diese Schleifenvariablen aus, um z.B. zu wissen, ob sich die Punkte nach rechts oder nach links bewegen sollen. Hier wird nur die main_blocks_loop erkl√§rt, aber das Prinzip gilt auch f√ºr die practice_block_loop.\nUm die verschiedenen Werte f√ºr die Schleifenvariablen zu definieren, m√ºssen wir eine einfache CSV-Datei erstellen:\ncue,direction\nleft,right\nleft,left\nnone,right\n...\nDiese CSV-Datei (die Bedingungsdatei) definiert die beiden loop Variablen cue und direction. Das Stichwort kann entweder left, right oder none, sein, w√§hrend die Richtung left oder right sein kann.\nIn der Benutzeroberfl√§che k√∂nnen wir die Variablen loopType und nReps f√ºr die Schleife angeben, wenn wir sie anklicken. Mit ersterer k√∂nnen wir steuern, ob wir z.B. die Zeilen in der Bedingungsdatei mischen oder sie sequentiell von oben nach unten ablaufen lassen wollen, w√§hrend die letztere definiert, wie oft jede Zeile der Bedingungsdatei wiederholt werden soll.\nF√ºr die main_blocks_loop haben wir eine Bedingungsdatei mit 80 Zeilen, die 40 neutralen Versuchen und 40 verzerrten Versuchen entsprechen. In der einen H√§lfte der neutralen Trials bewegen sich die Punkte nach rechts, in der anderen H√§lfte nach links. Bei den voreingenommenen Versuchen sind 32 der Hinweise g√ºltig (d.¬†h. sie stimmen mit der Bewegungsrichtung der Punkte √ºberein) und 16 ung√ºltig, wobei sich die Punkte sowohl bei g√ºltigen als auch bei ung√ºltigen Hinweisen in 50 % der Versuche nach rechts und in den anderen 50 % der Versuche nach links bewegen.\nDie Variable nReps wird auf 2 gesetzt, so dass alle diese Reihen zweimal durchlaufen werden (insgesamt 160 Versuche), und die Variable ‚ÄúloopType‚Äù wird auf random gesetzt, so dass die Versuche in zuf√§lliger Reihenfolge durchgef√ºhrt werden."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#daten",
    "href": "pages/chapters/01_psychopy_experiments.html#daten",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "Daten",
    "text": "Daten\nWenn man die default-Einstellungen nicht √§ndert, speichert PsychoPy die Daten automatisch in einem trial-by-trial CSV File. Dieses CSV File erh√§lt einen Namen, der sich aus der Versuchspersonen-ID, dem Namen des Experiments, und dem aktuellen Datum inkl. Uhrzeit zusammensetzt. So ist es m√∂glich, mit derselben Versuchspersonen-ID beliebig oft das Experiment zu wiederholen. Die CSV Files werden in einem Ordner mit dem Name data abgelegt."
  },
  {
    "objectID": "pages/chapters/01_psychopy_experiments.html#degrees-of-visual-angle",
    "href": "pages/chapters/01_psychopy_experiments.html#degrees-of-visual-angle",
    "title": "Verhaltensexperiment mit PsychoPy",
    "section": "Degrees of Visual Angle",
    "text": "Degrees of Visual Angle\nOftmals werden Gr√∂ssenangaben von Stimuli noch in Pixel oder Zentimeter, sondern in degrees of visual angle gemacht. Dies hat den Vorteil, dass die Angaben nicht vom Monitor selber oder der Entferung vom Monitor abh√§ngig sind. degrees of visual angle gibt die wahrgenommene Gr√∂sse des Stimulus an, und ber√ºcksichtigt die Gr√∂sse des Monitors und des Stimulus, und die Entfernung der Versuchsperson vom Monitor. Weitere Informationen dazu finden Sie auf der Website von üëâ OpenSesame. √úblicherweise entspricht ein degrees of visual angle etwa einem cm bei einer Entfernung von 57 cm vom Monitor.\nZur Umrechnung zwischen cm und degrees of visual angle finden Sie unter diesem üëâ Link mehr Information.\n\nOpenSesame ist ein weiteres, Python-basierendes Programm f√ºr die Erstellung behaviouraler Experimente."
  },
  {
    "objectID": "pages/chapters/02_importing_data.html",
    "href": "pages/chapters/02_importing_data.html",
    "title": "Daten importieren",
    "section": "",
    "text": "Nun wollen wir die Datens√§tze aus dem Verhaltensexperiment von der letzten Sitzung in R importieren.\nLaden Sie das RStudio Projekt und √∂ffnen Sie es. Im Projekt ist ein R Script File enthalten (import-data.R).\n\nFalls Sie nur den R Code m√∂chten, k√∂nnen Sie das File hier downloaden: üëâ R Code"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#csv-file-importieren",
    "href": "pages/chapters/02_importing_data.html#csv-file-importieren",
    "title": "Daten importieren",
    "section": "CSV File importieren",
    "text": "CSV File importieren\n\ntestdata <- read_csv(\"testdata/ZZ_rdk-discrimination_2022_Mar_07_1403.csv\") \n\nVariablen √ºberpr√ºfen\n\nglimpse(testdata)\n\nRows: 167\nColumns: 39\n$ cue                                        <chr> \"none\", \"left\", \"right\", \"l‚Ä¶\n$ direction                                  <chr> \"right\", \"right\", \"right\", ‚Ä¶\n$ practice_block_loop.thisRepN               <dbl> 0, 0, 0, 0, 0, 0, NA, NA, N‚Ä¶\n$ practice_block_loop.thisTrialN             <dbl> 0, 1, 2, 3, 4, 5, NA, NA, N‚Ä¶\n$ practice_block_loop.thisN                  <dbl> 0, 1, 2, 3, 4, 5, NA, NA, N‚Ä¶\n$ practice_block_loop.thisIndex              <dbl> 5, 2, 1, 0, 4, 3, NA, NA, N‚Ä¶\n$ main_blocks_loop.thisRepN                  <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ main_blocks_loop.thisTrialN                <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ main_blocks_loop.thisN                     <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ main_blocks_loop.thisIndex                 <dbl> NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ static_isi.started                         <dbl> 0.01033428, 0.03202713, 0.0‚Ä¶\n$ static_isi.stopped                         <dbl> 2.010334, 2.032027, 2.03217‚Ä¶\n$ fixation_pre.started                       <dbl> 26.79425, 36.16522, 44.7852‚Ä¶\n$ fixation_pre.stopped                       <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ image.started                              <dbl> 27.19849, 36.28205, 46.0032‚Ä¶\n$ image.stopped                              <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ fixation_post.started                      <dbl> 28.17814, 37.28240, 47.0037‚Ä¶\n$ fixation_post.stopped                      <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_background.started                    <dbl> 32.18642, 41.30145, 52.0107‚Ä¶\n$ dots_background.stopped                    <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_stimulus.started                      <dbl> 32.18642, 41.30145, 52.0107‚Ä¶\n$ dots_stimulus.stopped                      <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_keyboard_response.keys                <chr> \"None\", \"f\", \"j\", \"f\", \"Non‚Ä¶\n$ dots_keyboard_response.started             <dbl> 32.18642, 41.30145, 52.0107‚Ä¶\n$ dots_keyboard_response.stopped             <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ feedback_text.started                      <dbl> 33.70200, 42.28899, 52.9229‚Ä¶\n$ feedback_text.stopped                      <chr> \"None\", \"None\", \"None\", \"No‚Ä¶\n$ dots_keyboard_response.rt                  <dbl> NA, 0.9339199, 0.8488816, 0‚Ä¶\n$ instruction_main_text.started              <dbl> NA, NA, NA, NA, NA, NA, 81.‚Ä¶\n$ instruction_main_text.stopped              <chr> NA, NA, NA, NA, NA, NA, \"No‚Ä¶\n$ instruction_main_keyboard_response.keys    <chr> NA, NA, NA, NA, NA, NA, \"sp‚Ä¶\n$ instruction_main_keyboard_response.rt      <dbl> NA, NA, NA, NA, NA, NA, 3.1‚Ä¶\n$ instruction_main_keyboard_response.started <dbl> NA, NA, NA, NA, NA, NA, 81.‚Ä¶\n$ instruction_main_keyboard_response.stopped <chr> NA, NA, NA, NA, NA, NA, \"No‚Ä¶\n$ Pseudonym                                  <chr> \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ‚Ä¶\n$ date                                       <chr> \"2022_Mar_07_1403\", \"2022_M‚Ä¶\n$ expName                                    <chr> \"rdk-discrimination\", \"rdk-‚Ä¶\n$ psychopyVersion                            <chr> \"03.02.21\", \"03.02.21\", \"03‚Ä¶\n$ frameRate                                  <dbl> 59.9, 59.9, 59.9, 59.9, 59.‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#practice-trials-l√∂schen",
    "href": "pages/chapters/02_importing_data.html#practice-trials-l√∂schen",
    "title": "Daten importieren",
    "section": "Practice Trials l√∂schen",
    "text": "Practice Trials l√∂schen\n\nlibrary(kableExtra)\n\ntestdata |> \n  slice_head(n = 12) |> \n  kbl() |> \n  kable_paper(\"striped\", full_width = FALSE) |> \n  column_spec(2:7, bold = TRUE) |> \n  row_spec(1:6, bold = TRUE, color = \"white\", background = \"#D7261E\")\n\n\n\n cue \n    direction \n    practice_block_loop.thisRepN \n    practice_block_loop.thisTrialN \n    practice_block_loop.thisN \n    practice_block_loop.thisIndex \n    main_blocks_loop.thisRepN \n    main_blocks_loop.thisTrialN \n    main_blocks_loop.thisN \n    main_blocks_loop.thisIndex \n    static_isi.started \n    static_isi.stopped \n    fixation_pre.started \n    fixation_pre.stopped \n    image.started \n    image.stopped \n    fixation_post.started \n    fixation_post.stopped \n    dots_background.started \n    dots_background.stopped \n    dots_stimulus.started \n    dots_stimulus.stopped \n    dots_keyboard_response.keys \n    dots_keyboard_response.started \n    dots_keyboard_response.stopped \n    feedback_text.started \n    feedback_text.stopped \n    dots_keyboard_response.rt \n    instruction_main_text.started \n    instruction_main_text.stopped \n    instruction_main_keyboard_response.keys \n    instruction_main_keyboard_response.rt \n    instruction_main_keyboard_response.started \n    instruction_main_keyboard_response.stopped \n    Pseudonym \n    date \n    expName \n    psychopyVersion \n    frameRate \n  \n\n\n none \n    right \n    0 \n    0 \n    0 \n    5 \n    NA \n    NA \n    NA \n    NA \n    0.0103343 \n    2.010334 \n    26.79425 \n    None \n    27.19849 \n    None \n    28.17814 \n    None \n    32.18642 \n    None \n    32.18642 \n    None \n    None \n    32.18642 \n    None \n    33.70200 \n    None \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n left \n    right \n    0 \n    1 \n    1 \n    2 \n    NA \n    NA \n    NA \n    NA \n    0.0320271 \n    2.032027 \n    36.16522 \n    None \n    36.28205 \n    None \n    37.28240 \n    None \n    41.30145 \n    None \n    41.30145 \n    None \n    f \n    41.30145 \n    None \n    42.28899 \n    None \n    0.9339199 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    right \n    0 \n    2 \n    2 \n    1 \n    NA \n    NA \n    NA \n    NA \n    0.0321732 \n    2.032173 \n    44.78521 \n    None \n    46.00329 \n    None \n    47.00374 \n    None \n    52.01072 \n    None \n    52.01072 \n    None \n    j \n    52.01072 \n    None \n    52.92295 \n    None \n    0.8488816 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n left \n    left \n    0 \n    3 \n    3 \n    0 \n    NA \n    NA \n    NA \n    NA \n    0.0321533 \n    2.032153 \n    55.39138 \n    None \n    56.19407 \n    None \n    57.22527 \n    None \n    61.23181 \n    None \n    61.23181 \n    None \n    f \n    61.23181 \n    None \n    62.21611 \n    None \n    0.9396018 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n none \n    left \n    0 \n    4 \n    4 \n    4 \n    NA \n    NA \n    NA \n    NA \n    0.0321391 \n    2.032139 \n    64.71204 \n    None \n    64.81315 \n    None \n    65.84603 \n    None \n    69.25240 \n    None \n    69.25240 \n    None \n    None \n    69.25240 \n    None \n    70.78541 \n    None \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    left \n    0 \n    5 \n    5 \n    3 \n    NA \n    NA \n    NA \n    NA \n    0.0323178 \n    2.032318 \n    73.24960 \n    None \n    74.45209 \n    None \n    75.48391 \n    None \n    79.99045 \n    None \n    79.99045 \n    None \n    f \n    79.99045 \n    None \n    80.80311 \n    None \n    0.7490084 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    81.30346 \n    None \n    space \n    3.187924 \n    81.30346 \n    None \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    0 \n    0 \n    18 \n    0.0160001 \n    2.016000 \n    86.52245 \n    None \n    86.89231 \n    None \n    87.92302 \n    None \n    92.92987 \n    None \n    92.92987 \n    None \n    j \n    92.92987 \n    None \n    93.70924 \n    None \n    0.7136441 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n right \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    1 \n    1 \n    31 \n    0.0318162 \n    2.031816 \n    96.17699 \n    None \n    96.54602 \n    None \n    97.57770 \n    None \n    101.58423 \n    None \n    101.58423 \n    None \n    j \n    101.58423 \n    None \n    102.26673 \n    None \n    0.6271285 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n none \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    2 \n    2 \n    66 \n    0.0321148 \n    2.032115 \n    104.76463 \n    None \n    105.13302 \n    None \n    106.16508 \n    None \n    110.67183 \n    None \n    110.67183 \n    None \n    f \n    110.67183 \n    None \n    111.38828 \n    None \n    0.6703410 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n none \n    right \n    NA \n    NA \n    NA \n    NA \n    0 \n    3 \n    3 \n    75 \n    0.0321121 \n    2.032112 \n    113.88535 \n    None \n    115.08794 \n    None \n    116.11989 \n    None \n    119.52612 \n    None \n    119.52612 \n    None \n    j \n    119.52612 \n    None \n    120.15512 \n    None \n    0.5738488 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n left \n    left \n    NA \n    NA \n    NA \n    NA \n    0 \n    4 \n    4 \n    13 \n    0.0321118 \n    2.032112 \n    122.62295 \n    None \n    123.82583 \n    None \n    124.85742 \n    None \n    129.36397 \n    None \n    129.36397 \n    None \n    j \n    129.36397 \n    None \n    130.25975 \n    None \n    0.8405913 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    ZZ \n    2022_Mar_07_1403 \n    rdk-discrimination \n    03.02.21 \n    59.9 \n  \n\n\n\n\n\ntestdata |> \n  slice_head(n = 12) |> \n  select(starts_with(\"main_block\")) |> \n  kbl() |> \n  kable_paper(\"striped\", full_width = FALSE) |> \n  row_spec(1:7, bold = TRUE, color = \"white\", background = \"#D7261E\")\n\n\n\n main_blocks_loop.thisRepN \n    main_blocks_loop.thisTrialN \n    main_blocks_loop.thisN \n    main_blocks_loop.thisIndex \n  \n\n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n NA \n    NA \n    NA \n    NA \n  \n\n 0 \n    0 \n    0 \n    18 \n  \n\n 0 \n    1 \n    1 \n    31 \n  \n\n 0 \n    2 \n    2 \n    66 \n  \n\n 0 \n    3 \n    3 \n    75 \n  \n\n 0 \n    4 \n    4 \n    13 \n  \n\n\n\n\nDie Variable main_blocks_loop.thisN ist die Trialnummer. Diese k√∂nnen wir verwenden, um die Zeilen auszuschliessen, die nicht zum Main Block geh√∂ren.\n\ntestdata |> \n    filter(!is.na(main_blocks_loop.thisN)) |>\n    select(-contains(\"practice_block_loop\"))\n\n# A tibble: 160 √ó 35\n   cue   direction main_blocks_loop.thisRepN main_blocks_loop.‚Ä¶ main_blocks_loo‚Ä¶\n   <chr> <chr>                         <dbl>              <dbl>            <dbl>\n 1 right right                             0                  0                0\n 2 right right                             0                  1                1\n 3 none  right                             0                  2                2\n 4 none  right                             0                  3                3\n 5 left  left                              0                  4                4\n 6 none  right                             0                  5                5\n 7 none  left                              0                  6                6\n 8 left  left                              0                  7                7\n 9 left  right                             0                  8                8\n10 none  right                             0                  9                9\n# ‚Ä¶ with 150 more rows, and 30 more variables:\n#   main_blocks_loop.thisIndex <dbl>, static_isi.started <dbl>,\n#   static_isi.stopped <dbl>, fixation_pre.started <dbl>,\n#   fixation_pre.stopped <chr>, image.started <dbl>, image.stopped <chr>,\n#   fixation_post.started <dbl>, fixation_post.stopped <chr>,\n#   dots_background.started <dbl>, dots_background.stopped <chr>,\n#   dots_stimulus.started <dbl>, dots_stimulus.stopped <chr>, ‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen",
    "href": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen",
    "title": "Daten importieren",
    "section": "Variablen ausw√§hlen",
    "text": "Variablen ausw√§hlen\n\ntestdata |>\n    select(-contains(\"static\"),\n           -contains(\"fixation\"),\n           -contains(\"image\"),\n           -contains(\"instruction\"),\n           -contains(\"feedback\"))\n\n# A tibble: 167 √ó 23\n   cue   direction practice_block_loop.thisRe‚Ä¶ practice_block_‚Ä¶ practice_block_‚Ä¶\n   <chr> <chr>                           <dbl>            <dbl>            <dbl>\n 1 none  right                               0                0                0\n 2 left  right                               0                1                1\n 3 right right                               0                2                2\n 4 left  left                                0                3                3\n 5 none  left                                0                4                4\n 6 right left                                0                5                5\n 7 <NA>  <NA>                               NA               NA               NA\n 8 right right                              NA               NA               NA\n 9 right right                              NA               NA               NA\n10 none  right                              NA               NA               NA\n# ‚Ä¶ with 157 more rows, and 18 more variables:\n#   practice_block_loop.thisIndex <dbl>, main_blocks_loop.thisRepN <dbl>,\n#   main_blocks_loop.thisTrialN <dbl>, main_blocks_loop.thisN <dbl>,\n#   main_blocks_loop.thisIndex <dbl>, dots_background.started <dbl>,\n#   dots_background.stopped <chr>, dots_stimulus.started <dbl>,\n#   dots_stimulus.stopped <chr>, dots_keyboard_response.keys <chr>,\n#   dots_keyboard_response.started <dbl>, ‚Ä¶\n\n\n\ntestdata <- testdata |>\n    select(-contains(\"static\"),\n           -contains(\"fixation\"),\n           -contains(\"image\"),\n           -contains(\"instruction\"),\n           -contains(\"feedback\"))\n\n\ntestdata\n\n# A tibble: 167 √ó 23\n   cue   direction practice_block_loop.thisRe‚Ä¶ practice_block_‚Ä¶ practice_block_‚Ä¶\n   <chr> <chr>                           <dbl>            <dbl>            <dbl>\n 1 none  right                               0                0                0\n 2 left  right                               0                1                1\n 3 right right                               0                2                2\n 4 left  left                                0                3                3\n 5 none  left                                0                4                4\n 6 right left                                0                5                5\n 7 <NA>  <NA>                               NA               NA               NA\n 8 right right                              NA               NA               NA\n 9 right right                              NA               NA               NA\n10 none  right                              NA               NA               NA\n# ‚Ä¶ with 157 more rows, and 18 more variables:\n#   practice_block_loop.thisIndex <dbl>, main_blocks_loop.thisRepN <dbl>,\n#   main_blocks_loop.thisTrialN <dbl>, main_blocks_loop.thisN <dbl>,\n#   main_blocks_loop.thisIndex <dbl>, dots_background.started <dbl>,\n#   dots_background.stopped <chr>, dots_stimulus.started <dbl>,\n#   dots_stimulus.stopped <chr>, dots_keyboard_response.keys <chr>,\n#   dots_keyboard_response.started <dbl>, ‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#variablen-umbennen",
    "href": "pages/chapters/02_importing_data.html#variablen-umbennen",
    "title": "Daten importieren",
    "section": "Variablen umbennen",
    "text": "Variablen umbennen\n\ntestdata <- testdata |>\n    select(trial = main_blocks_loop.thisN,\n           ID = Pseudonym,\n           cue,\n           direction,\n           response = dots_keyboard_response.keys,\n           rt = dots_keyboard_response.rt)\n\n\ntestdata\n\n# A tibble: 167 √ó 6\n   trial ID    cue   direction response     rt\n   <dbl> <chr> <chr> <chr>     <chr>     <dbl>\n 1    NA ZZ    none  right     None     NA    \n 2    NA ZZ    left  right     f         0.934\n 3    NA ZZ    right right     j         0.849\n 4    NA ZZ    left  left      f         0.940\n 5    NA ZZ    none  left      None     NA    \n 6    NA ZZ    right left      f         0.749\n 7    NA ZZ    <NA>  <NA>      <NA>     NA    \n 8     0 ZZ    right right     j         0.714\n 9     1 ZZ    right right     j         0.627\n10     2 ZZ    none  right     f         0.670\n# ‚Ä¶ with 157 more rows"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#neue-variablen-definieren",
    "href": "pages/chapters/02_importing_data.html#neue-variablen-definieren",
    "title": "Daten importieren",
    "section": "Neue Variablen definieren",
    "text": "Neue Variablen definieren\n\ntestdata <- testdata |>\n    mutate(choice = if_else(response == \"j\", \"right\", \"left\"),\n           response = if_else(choice == \"right\", 1, 0))\n\nAlternative:\n\ntestdata <- testdata |>\n    mutate(choice = if_else(response == \"j\", \"right\", \"left\"),\n           response = as.numeric(choice == \"right\"))\n\nWir erstellen ausserdem hier eine Variable, welche angibt, ob der Cue valid, invalid oder neutral war. Ein Cue ist genau dann valide, wenn er dieselbe Richtung hat wie der RDK Stimulus, d.h. cue == direction.\n\ntestdata <- testdata |>\n    mutate(condition = case_when(cue == \"none\" ~ \"neutral\",\n                                 cue == direction ~ \"valid\",\n                                 cue != direction ~ \"invalid\"))\n\n\ntestdata <- testdata |>\n    mutate(correct = as.numeric(choice == direction))"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#gruppierungsvariablen",
    "href": "pages/chapters/02_importing_data.html#gruppierungsvariablen",
    "title": "Daten importieren",
    "section": "Gruppierungsvariablen",
    "text": "Gruppierungsvariablen\n\nglimpse(testdata)\n\nRows: 167\nColumns: 9\n$ trial     <dbl> NA, NA, NA, NA, NA, NA, NA, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10‚Ä¶\n$ ID        <chr> \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", \"ZZ\", ‚Ä¶\n$ cue       <chr> \"none\", \"left\", \"right\", \"left\", \"none\", \"right\", NA, \"right‚Ä¶\n$ direction <chr> \"right\", \"right\", \"right\", \"left\", \"left\", \"left\", NA, \"righ‚Ä¶\n$ response  <dbl> 0, 0, 1, 0, 0, 0, NA, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,‚Ä¶\n$ rt        <dbl> NA, 0.9339199, 0.8488816, 0.9396018, NA, 0.7490084, NA, 0.71‚Ä¶\n$ choice    <chr> \"left\", \"left\", \"right\", \"left\", \"left\", \"left\", NA, \"right\"‚Ä¶\n$ condition <chr> \"neutral\", \"invalid\", \"valid\", \"valid\", \"neutral\", \"invalid\"‚Ä¶\n$ correct   <dbl> 0, 0, 1, 1, 1, 1, NA, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,‚Ä¶\n\n\n\ntestdata <- testdata |>\n    mutate_if(is.character, as.factor)\n\n\nglimpse(testdata)\n\nRows: 167\nColumns: 9\n$ trial     <dbl> NA, NA, NA, NA, NA, NA, NA, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10‚Ä¶\n$ ID        <fct> ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ZZ, ‚Ä¶\n$ cue       <fct> none, left, right, left, none, right, NA, right, right, none‚Ä¶\n$ direction <fct> right, right, right, left, left, left, NA, right, right, rig‚Ä¶\n$ response  <dbl> 0, 0, 1, 0, 0, 0, NA, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,‚Ä¶\n$ rt        <dbl> NA, 0.9339199, 0.8488816, 0.9396018, NA, 0.7490084, NA, 0.71‚Ä¶\n$ choice    <fct> left, left, right, left, left, left, NA, right, right, left,‚Ä¶\n$ condition <fct> neutral, invalid, valid, valid, neutral, invalid, NA, valid,‚Ä¶\n$ correct   <dbl> 0, 0, 1, 1, 1, 1, NA, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#accuracy-pro-bedingung",
    "href": "pages/chapters/02_importing_data.html#accuracy-pro-bedingung",
    "title": "Daten importieren",
    "section": "Accuracy pro Bedingung",
    "text": "Accuracy pro Bedingung\nWir k√∂nnen nun die accuracy in jeder Cue-Bedingung berechnen. Es gibt hier zwei M√∂glichkeiten: wir berechen die Anzahl Trials (N), und die Anzahl korrekter Antworten (ncorrect) separat. Der Anteil korrekter Antworten ist dann einfach ncorrect/N. Dasselbe Ergebnis erhalten wir, wenn wir einfach den Mittelwert der korrekten Antworten nehmen.\n\ntestaccuracy <- testdata |>\n    group_by(condition) |>\n    summarise(N = n(),\n              ncorrect = sum(correct),\n              accuracy = ncorrect/N,\n              accuracy2 = mean(correct))\n\ntestaccuracy\n\n# A tibble: 4 √ó 5\n  condition     N ncorrect accuracy accuracy2\n  <fct>     <int>    <dbl>    <dbl>     <dbl>\n1 invalid      18       14    0.778     0.778\n2 neutral      82       67    0.817     0.817\n3 valid        66       62    0.939     0.939\n4 <NA>          1       NA   NA        NA"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#funktion-definieren",
    "href": "pages/chapters/02_importing_data.html#funktion-definieren",
    "title": "Daten importieren",
    "section": "Funktion definieren",
    "text": "Funktion definieren\nNun wollen wir die ersten paar Schritte gleichzeitig auf mehrere Files anwenden:\n\n\nCSV File einlesen\nFilename hinzuf√ºgen\nPractice Trials l√∂schen\nPractice Variablen l√∂schen\n\nDieser Vorgang ist in R ziemlich elegant. Anstatt dass wir manuell √ºber alle Files iterieren m√ºssen, k√∂nnen wir eine Funktion definieren, die wir auf ein File anwenden k√∂nnen, und dann wenden wir diese Funktion auf alle Files an.\n\nMit map_* Funktionen k√∂nnen wir eine Funktion auf alle Elemente einer Liste anwenden. map_dfr macht genau das, und gibt einen Dataframe als Output, in welchem die einzelnen Elemente row-wise zusamengesetzt werden.\nDie Funktion, welche wir auf ein einzelnes .csv File anweden m√∂chten, ist diese:\n\nimport_function <- function(filename) {\n    read_csv(filename) |>\n        mutate(filename = basename(filename)) |>\n        filter(!is.na(main_blocks_loop.thisN)) |>\n        select(-contains(\"practice_block_loop\"))\n}\n\n\nProbieren Sie die Funktion mit dem einzelnen .csv File von oben."
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#alle-files-in-einem-ordner-auflisten",
    "href": "pages/chapters/02_importing_data.html#alle-files-in-einem-ordner-auflisten",
    "title": "Daten importieren",
    "section": "Alle Files in einem Ordner auflisten",
    "text": "Alle Files in einem Ordner auflisten\n\ndatadir <- \"data/\"\nlist_of_files <- datadir |>\n    list.files(pattern = \"csv\", recursive = TRUE, full.names = TRUE)\n\n\nlist_of_files\n\n[1] \"data//JH_rdk-discrimination_2022_Mar_07_1403.csv\"   \n[2] \"data//NS_rdk-discrimination_2022_Mar_07_1331.csv\"   \n[3] \"data//SS91_rdk-discrimination_2022_Mar_06_0953.csv\" \n[4] \"data//VP1_rdk-discrimination_2022_Mar_07_1237.csv\"  \n[5] \"data//VP2_rdk-discrimination_2022_Mar_07_1302.csv\"  \n[6] \"data//VPN01_rdk-discrimination_2022_Mar_01_2142.csv\"\n[7] \"data//VPN02_rdk-discrimination_2022_Mar_01_2208.csv\"\n[8] \"data//rh_rdk-discrimination_2022_Mar_02_1105.csv\"   \n[9] \"data//sb_rdk-discrimination_2022_Mar_06_0746.csv\""
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#funktion-auf-liste-anwenden",
    "href": "pages/chapters/02_importing_data.html#funktion-auf-liste-anwenden",
    "title": "Daten importieren",
    "section": "Funktion auf Liste anwenden",
    "text": "Funktion auf Liste anwenden\n\ndata <- list_of_files |> \n    map_dfr(~ import_function(.x))"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen-und-umbennen",
    "href": "pages/chapters/02_importing_data.html#variablen-ausw√§hlen-und-umbennen",
    "title": "Daten importieren",
    "section": "Variablen ausw√§hlen und umbennen",
    "text": "Variablen ausw√§hlen und umbennen\n\ndata <- data |>\n    select(-contains(\"static\"),\n           -contains(\"fixation\"),\n           -contains(\"image\"),\n           -contains(\"instruction\"),\n           -contains(\"feedback\"))\n\n\ndata <- data |>\n    select(trial = main_blocks_loop.thisN,\n           ID = Pseudonym,\n           cue,\n           direction,\n           response = dots_keyboard_response.keys,\n           rt = dots_keyboard_response.rt)"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#neue-variablen-definieren-1",
    "href": "pages/chapters/02_importing_data.html#neue-variablen-definieren-1",
    "title": "Daten importieren",
    "section": "Neue Variablen definieren",
    "text": "Neue Variablen definieren\nKorrekte Antworten\n\ndata <- data |>\n    mutate(choice = if_else(response == \"j\", \"right\", \"left\"),\n           response = if_else(choice == \"right\", 1, 0))\n\n\ndata <- data |>\n    mutate(correct = as.numeric(choice == direction))\n\n\nglimpse(data)\n\nRows: 1,440\nColumns: 8\n$ trial     <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ ID        <chr> \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", \"JH\", ‚Ä¶\n$ cue       <chr> \"right\", \"right\", \"none\", \"none\", \"left\", \"none\", \"none\", \"l‚Ä¶\n$ direction <chr> \"right\", \"right\", \"right\", \"right\", \"left\", \"right\", \"left\",‚Ä¶\n$ response  <dbl> 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, ‚Ä¶\n$ rt        <dbl> 0.7136441, 0.6271285, 0.6703410, 0.5738488, 0.8405913, 0.667‚Ä¶\n$ choice    <chr> \"right\", \"right\", \"left\", \"right\", \"right\", \"right\", \"right\"‚Ä¶\n$ correct   <dbl> 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n\n\n\ndata |> \n  slice_head(n = 20)\n\n# A tibble: 20 √ó 8\n   trial ID    cue   direction response    rt choice correct\n   <dbl> <chr> <chr> <chr>        <dbl> <dbl> <chr>    <dbl>\n 1     0 JH    right right            1 0.714 right        1\n 2     1 JH    right right            1 0.627 right        1\n 3     2 JH    none  right            0 0.670 left         0\n 4     3 JH    none  right            1 0.574 right        1\n 5     4 JH    left  left             1 0.841 right        0\n 6     5 JH    none  right            1 0.668 right        1\n 7     6 JH    none  left             1 1.12  right        0\n 8     7 JH    left  left             0 0.640 left         1\n 9     8 JH    left  right            0 1.13  left         0\n10     9 JH    none  right            1 1.03  right        1\n11    10 JH    none  left             0 1.35  left         1\n12    11 JH    left  left             0 0.688 left         1\n13    12 JH    left  left             0 0.721 left         1\n14    13 JH    none  left             0 0.655 left         1\n15    14 JH    right right            1 1.02  right        1\n16    15 JH    none  right            1 1.12  right        1\n17    16 JH    left  left             0 1.08  left         1\n18    17 JH    right left             0 0.643 left         1\n19    18 JH    right right            1 0.716 right        1\n20    19 JH    left  left             0 0.578 left         1\n\n\nCue-Bedingungsvariable\n\ndata <- data |>\n    mutate(condition = case_when(cue == \"none\" ~ \"neutral\",\n                                 cue == direction ~ \"valid\",\n                                 cue != direction ~ \"invalid\"))\n\nDaten als CSV speichern\nAn dieser Stelle speichern wir den neu kreierten Datensatz als .csv File. Somit k√∂nnen wir die Daten einfach importieren, ohne die ganzen Schritte wiederholen zu m√ºssen.\n\ndata |> write_csv(file = \"data_clean/rdkdata.csv\")\n\n\ndata |> \n  slice_head(n = 20)\n\n# A tibble: 20 √ó 9\n   trial ID    cue   direction response    rt choice correct condition\n   <dbl> <chr> <chr> <chr>        <dbl> <dbl> <chr>    <dbl> <chr>    \n 1     0 JH    right right            1 0.714 right        1 valid    \n 2     1 JH    right right            1 0.627 right        1 valid    \n 3     2 JH    none  right            0 0.670 left         0 neutral  \n 4     3 JH    none  right            1 0.574 right        1 neutral  \n 5     4 JH    left  left             1 0.841 right        0 valid    \n 6     5 JH    none  right            1 0.668 right        1 neutral  \n 7     6 JH    none  left             1 1.12  right        0 neutral  \n 8     7 JH    left  left             0 0.640 left         1 valid    \n 9     8 JH    left  right            0 1.13  left         0 invalid  \n10     9 JH    none  right            1 1.03  right        1 neutral  \n11    10 JH    none  left             0 1.35  left         1 neutral  \n12    11 JH    left  left             0 0.688 left         1 valid    \n13    12 JH    left  left             0 0.721 left         1 valid    \n14    13 JH    none  left             0 0.655 left         1 neutral  \n15    14 JH    right right            1 1.02  right        1 valid    \n16    15 JH    none  right            1 1.12  right        1 neutral  \n17    16 JH    left  left             0 1.08  left         1 valid    \n18    17 JH    right left             0 0.643 left         1 invalid  \n19    18 JH    right right            1 0.716 right        1 valid    \n20    19 JH    left  left             0 0.578 left         1 valid"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#gruppierungsvariablen-1",
    "href": "pages/chapters/02_importing_data.html#gruppierungsvariablen-1",
    "title": "Daten importieren",
    "section": "Gruppierungsvariablen",
    "text": "Gruppierungsvariablen\n\ndata <- data |>\n    mutate_if(is.character, as.factor)\n\n\nglimpse(data)\n\nRows: 1,440\nColumns: 9\n$ trial     <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17‚Ä¶\n$ ID        <fct> JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, JH, ‚Ä¶\n$ cue       <fct> right, right, none, none, left, none, none, left, left, none‚Ä¶\n$ direction <fct> right, right, right, right, left, right, left, left, right, ‚Ä¶\n$ response  <dbl> 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, ‚Ä¶\n$ rt        <dbl> 0.7136441, 0.6271285, 0.6703410, 0.5738488, 0.8405913, 0.667‚Ä¶\n$ choice    <fct> right, right, left, right, right, right, right, left, left, ‚Ä¶\n$ correct   <dbl> 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ condition <fct> valid, valid, neutral, neutral, valid, neutral, neutral, val‚Ä¶"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#accuracy-pro-personbedingung",
    "href": "pages/chapters/02_importing_data.html#accuracy-pro-personbedingung",
    "title": "Daten importieren",
    "section": "Accuracy pro Person/Bedingung",
    "text": "Accuracy pro Person/Bedingung\nAccuracy pro Person und pro Bedingung berechnen.\n\naccuracy <- data |>\n    group_by(ID, condition) |>\n    summarise(N = n(),\n              ncorrect = sum(correct),\n              accuracy = mean(correct))\n\n`summarise()` has grouped output by 'ID'. You can override using the `.groups`\nargument.\n\n\n\naccuracy\n\n# A tibble: 27 √ó 5\n# Groups:   ID [9]\n   ID    condition     N ncorrect accuracy\n   <fct> <fct>     <int>    <dbl>    <dbl>\n 1 JH    invalid      16       13    0.812\n 2 JH    neutral      80       66    0.825\n 3 JH    valid        64       60    0.938\n 4 NS    invalid      16       11    0.688\n 5 NS    neutral      80       56    0.7  \n 6 NS    valid        64       58    0.906\n 7 SS91  invalid      16        0    0    \n 8 SS91  neutral      80       39    0.488\n 9 SS91  valid        64       54    0.844\n10 VP1   invalid      16       11    0.688\n# ‚Ä¶ with 17 more rows"
  },
  {
    "objectID": "pages/chapters/02_importing_data.html#visualisieren",
    "href": "pages/chapters/02_importing_data.html#visualisieren",
    "title": "Daten importieren",
    "section": "Visualisieren",
    "text": "Visualisieren\n\naccuracy |> \n  ggplot(aes(x = condition, y = accuracy, fill = condition)) +\n  geom_col() +\n  scale_fill_manual(\n    values = c(invalid = \"#9E0142\",\n    neutral = \"#C4C4B7\",\n    valid = \"#2EC762\")\n  ) +\n  labs(\n    x = \"Cue\",\n    y = \"Proportion correct\",\n    title = \"Accuracy per person/condition\"\n  ) +\n  theme_linedraw(base_size = 28) +\n  facet_wrap(~ID)"
  },
  {
    "objectID": "pages/chapters/03_data_cleaning.html",
    "href": "pages/chapters/03_data_cleaning.html",
    "title": "Data cleaning",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nNun wollen wir versuchen, einzelne Trials, zu identifizieren, in denen Versuchpersonen nicht aufgepasst haben, oder einfach geraten wurde.\nAm h√§ufigsten werden die folgenden beiden Kriterien verwendet, um entweder einzelne Datenpunkte, oder Versuchspersonen, auszuschliessen:\nNun ist in Experimenten, in denen ein Bias erzeugt wird, etwas heikel, Trials oder Versuchspersonen aufgrund der Anzahl korrekter Antworten auszuschliessen - wir haben ja die Korrektheit der Antworten experimentell manipuliert.\nDeswegen richten wir hier unseren Fokus auf die Reaktionszeiten. Wir gehen davon aus, dass Reaktionszeiten, die zu schnell oder yu langsam waren, aufgrund von Rateprozessen zustande kamen. Was genau zu schnell oder zu langsam heisst, ist schwierig zu beantworten, und h√§ngt stark vom jeweiligen Task ab. Deshalb ist es wichtig, sich a priori Gedanken dar√ºber zu machen, welche Kriterien angewandt werden sollen."
  },
  {
    "objectID": "pages/chapters/03_data_cleaning.html#eigenschaften-von-reaktionszeiten",
    "href": "pages/chapters/03_data_cleaning.html#eigenschaften-von-reaktionszeiten",
    "title": "Data cleaning",
    "section": "Eigenschaften von Reaktionszeiten",
    "text": "Eigenschaften von Reaktionszeiten\nDie wichtigsten Merkmale von Reaktionszeiten sind\n\nSie sind rechtsschief\nSie sind nicht normalverteilt\nStreuung (Standardabweichung) steigt ungef√§hr linear mit wachsendem Mittelwert (Wagenmakers and Brown 2007)\n\n\nDie Rechtschiefe ist eine nat√ºrliche Konsequenz der Tatsache, dass es viele M√∂glichkeiten gibt, langsamer zu werden, aber nur wenige M√∂glichkeiten, schneller zu werden. Reaktionszeiten k√∂nnen nicht negativ sein Ausserdem gibt es eine Untergrenze, welche durch unsere Physiologie bestimmt ist. Schellere Reaktionszeiten als 200 Millisekunden sind kaum m√∂glich.\nDie Konsequenz daraus ist, dass Reaktionszeiten nicht normalverteilt sind. In folgender Grafik sind zwei Verteilungen dargestellt. Die gelbe Verteilung ist eine Normalverteilung mit \\(\\mu = 1\\) und \\(\\sigma = 0.4\\), w√§hrend die graue Verteilung eine LogNormal Verteilung darstellt.\n\nEine LogNormal-Verteilung bedeutet, dass der Logarithmus einer Zufallsvariablen normalverteilt ist.\n\n\n\n\n\nObwohl die Normalverteilung so aussieht, als k√∂nne sie Reaktionszeiten repr√§sentieren, ist der Wertebereich von \\([-\\Inf, \\Inf]\\) nicht daf√ºr geeignet. Ausserdem erlaubt die Normalverteilung keine extremen Werte, und ist nicht asymmetrisch."
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html",
    "href": "pages/chapters/04_summarizing_data.html",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nOb eine Variable als factor definiert ist, wird als Attribut gespeichert. Attribute werden aber in einem .csv. File nicht mitgespeichert; deshalb m√ºssen wir die Gruppierungsvariablen wieder als factor definieren."
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#pro-versuchsperson",
    "href": "pages/chapters/04_summarizing_data.html#pro-versuchsperson",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Pro Versuchsperson",
    "text": "Pro Versuchsperson\n\ndata\n\n# A tibble: 1,440 √ó 9\n   trial ID    cue   direction response    rt choice correct condition\n   <dbl> <fct> <fct> <fct>        <dbl> <dbl> <fct>    <dbl> <fct>    \n 1     0 JH    right right            1 0.714 right        1 valid    \n 2     1 JH    right right            1 0.627 right        1 valid    \n 3     2 JH    none  right            0 0.670 left         0 neutral  \n 4     3 JH    none  right            1 0.574 right        1 neutral  \n 5     4 JH    left  left             1 0.841 right        0 valid    \n 6     5 JH    none  right            1 0.668 right        1 neutral  \n 7     6 JH    none  left             1 1.12  right        0 neutral  \n 8     7 JH    left  left             0 0.640 left         1 valid    \n 9     8 JH    left  right            0 1.13  left         0 invalid  \n10     9 JH    none  right            1 1.03  right        1 neutral  \n# ‚Ä¶ with 1,430 more rows\n\n\n\ndata |> \n  group_by(ID, condition)\n\n# A tibble: 1,440 √ó 9\n# Groups:   ID, condition [27]\n   trial ID    cue   direction response    rt choice correct condition\n   <dbl> <fct> <fct> <fct>        <dbl> <dbl> <fct>    <dbl> <fct>    \n 1     0 JH    right right            1 0.714 right        1 valid    \n 2     1 JH    right right            1 0.627 right        1 valid    \n 3     2 JH    none  right            0 0.670 left         0 neutral  \n 4     3 JH    none  right            1 0.574 right        1 neutral  \n 5     4 JH    left  left             1 0.841 right        0 valid    \n 6     5 JH    none  right            1 0.668 right        1 neutral  \n 7     6 JH    none  left             1 1.12  right        0 neutral  \n 8     7 JH    left  left             0 0.640 left         1 valid    \n 9     8 JH    left  right            0 1.13  left         0 invalid  \n10     9 JH    none  right            1 1.03  right        1 neutral  \n# ‚Ä¶ with 1,430 more rows\n\n\n\naccuracy <- data |>\n    group_by(ID, condition) |>\n    summarise(N = n(),\n              ncorrect = sum(correct),\n              accuracy = mean(correct))\n\n\naccuracy\n\n# A tibble: 27 √ó 5\n# Groups:   ID [9]\n   ID    condition     N ncorrect accuracy\n   <fct> <fct>     <int>    <dbl>    <dbl>\n 1 JH    invalid      16       13    0.812\n 2 JH    neutral      80       66    0.825\n 3 JH    valid        64       60    0.938\n 4 NS    invalid      16       11    0.688\n 5 NS    neutral      80       56    0.7  \n 6 NS    valid        64       58    0.906\n 7 SS91  invalid      16        0    0    \n 8 SS91  neutral      80       39    0.488\n 9 SS91  valid        64       54    0.844\n10 VP1   invalid      16       11    0.688\n# ‚Ä¶ with 17 more rows"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#visualisieren",
    "href": "pages/chapters/04_summarizing_data.html#visualisieren",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Visualisieren",
    "text": "Visualisieren\n\naccuracy |> \n  ggplot(aes(x = condition, y = accuracy, fill = condition)) +\n  geom_col() +\n  geom_line(aes(group = ID), size = 2) +\n  geom_point(size = 8) +\n  scale_fill_manual(\n    values = c(invalid = \"#9E0142\",\n    neutral = \"#C4C4B7\",\n    valid = \"#2EC762\")\n  ) +\n  labs(\n    x = \"Cue\",\n    y = \"Proportion correct\",\n    title = \"Accuracy per person/condition\"\n  ) +\n  theme_linedraw(base_size = 28) +\n  facet_wrap(~ID)"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren",
    "href": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "√úber Versuchsperson aggregieren",
    "text": "√úber Versuchsperson aggregieren"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#ein-exkurs-√ºber-within-person-standardfehler",
    "href": "pages/chapters/04_summarizing_data.html#ein-exkurs-√ºber-within-person-standardfehler",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Ein Exkurs √ºber Within-person Standardfehler",
    "text": "Ein Exkurs √ºber Within-person Standardfehler\n\nlibrary(tidyverse)\n\ndfw <- tribble(\n ~subject, ~pretest, ~posttest,\n       1,   59.4,     64.5,\n       2,   46.4,     52.4,\n       3,   46.0,     49.7,\n       4,   49.0,     48.7,\n       5,   32.5,     37.4,\n       6,   45.2,     49.5,\n       7,   60.3,     59.9,\n       8,   54.3,     54.1,\n       9,   45.4,     49.6,\n      10,   38.9,     48.5) |>\n    mutate(subject = as.factor(subject))\n\n\ndfl <- dfw |>\n    pivot_longer(contains(\"test\"),\n                 names_to = \"condition\",\n                 values_to = \"value\") |>\n    mutate(condition = as_factor(condition))\n\n\ndflsum <- dfl |>\n    Rmisc::summarySEwithin(measurevar = \"value\",\n                               withinvars = \"condition\",\n                               idvar = \"subject\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\n\n\ndflsum |>\n    ggplot(aes(x = condition, y = value, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = 0.1, aes(ymin = value-ci, ymax = value+ci)) +\n    geom_point(shape = 21, size = 3, fill = \"white\") +\n    ylim(40,60)\n\n\n\n\n\n# Use a consistent y range\nymax <- max(dfl$value)\nymin <- min(dfl$value)\n\n\n# Plot the individuals\ndfl |>\n    ggplot(aes(x=condition, y=value, colour=subject, group=subject)) +\n    geom_line() + geom_point(shape=21, fill=\"white\") +\n    ylim(ymin,ymax)\n\n\n\n\n\ndfNorm_long <- Rmisc::normDataWithin(data=dfl, idvar=\"subject\", measurevar=\"value\")\n?Rmisc::normDataWithin\n\ndfNorm_long |>\n    ggplot(aes(x=condition, y=valueNormed, colour=subject, group=subject)) +\n    geom_line() + geom_point(shape=21, fill=\"white\") +\n    ylim(ymin,ymax)\n\n\n\n\n\n# Instead of summarySEwithin, use summarySE, which treats condition as though it were a between-subjects variable\ndflsum_between <- Rmisc::summarySE(data = dfl, \n                                   measurevar = \"value\", \n                                   groupvars = \"condition\", \n                                   na.rm = FALSE, \n                                   conf.interval = .95)\ndflsum_between\n\n  condition  N value       sd       se       ci\n1   pretest 10 47.74 8.598992 2.719240 6.151348\n2  posttest 10 51.43 7.253972 2.293907 5.189179\n\n\n\n# Show the between-S CI's in red, and the within-S CI's in black\ndflsum_between |>\n    ggplot(aes(x=condition, y=value, group=1)) +\n    geom_line() +\n    geom_errorbar(width=.1, aes(ymin=value-ci, ymax=value+ci), colour=\"red\") +\n    geom_errorbar(width=.1, aes(ymin=value-ci, ymax=value+ci), data=dflsum) +\n    geom_point(shape=21, size=3, fill=\"white\") +\n    ylim(ymin,ymax)"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#within-person-standardfehler",
    "href": "pages/chapters/04_summarizing_data.html#within-person-standardfehler",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Within-person Standardfehler",
    "text": "Within-person Standardfehler\n\naccuracy |> \n  ggplot(aes(x = condition, y = accuracy, colour = ID, group = ID)) +\n    geom_line() + \n  geom_point(shape=21, fill=\"white\")\n\n\n\n\nDer Standardfehler is definiert als: \\[SE = sd/ \\sqrt{n}\\]\nLeider gibt es in R keine Funktion, welche den Standardfehler berechnet (sch√§tzt); wir k√∂nnen aber ganz einfach selber eine Funktion definieren.\n\nse <- function(x) sd(x)/sqrt(length(x))\n\n\ndatasum <- data |>\n   group_by(condition) |> \n   summarise(N = n(),\n             ccuracy = mean(correct),\n             sd = sd(correct),\n             se = se(correct))\ndatasum\n\n# A tibble: 3 √ó 5\n  condition     N ccuracy    sd     se\n  <fct>     <int>   <dbl> <dbl>  <dbl>\n1 invalid     144   0.389 0.489 0.0408\n2 neutral     720   0.629 0.483 0.0180\n3 valid       576   0.825 0.381 0.0159\n\n\n\ndatasum_2 <- data |>\n    Rmisc::summarySE(measurevar = \"correct\",\n                              groupvars = \"condition\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\ndatasum_2\n\n  condition   N   correct        sd         se         ci\n1   invalid 144 0.3888889 0.4891996 0.04076663 0.08058308\n2   neutral 720 0.6291667 0.4833637 0.01801390 0.03536613\n3     valid 576 0.8246528 0.3805943 0.01585810 0.03114686\n\n\n\ndatasum_3 <- data |>\n    Rmisc::summarySEwithin(measurevar = \"correct\",\n                               withinvars = \"condition\",\n                               idvar = \"ID\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\ndatasum_3\n\n  condition   N   correct        sd         se         ci\n1   invalid 144 0.3888889 0.5773528 0.04811273 0.09510406\n2   neutral 720 0.6291667 0.5726512 0.02134145 0.04189901\n3     valid 576 0.8246528 0.4523391 0.01884746 0.03701827\n\n\n\np_accuracy <- datasum_3 |>\n    ggplot(aes(x = condition, y = correct, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = correct-se, ymax = correct+se), colour=\"red\") +\n    geom_point(shape=21, size=3, fill=\"white\")\np_accuracy"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#pro-versuchsperson-1",
    "href": "pages/chapters/04_summarizing_data.html#pro-versuchsperson-1",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "Pro Versuchsperson",
    "text": "Pro Versuchsperson\nWir fassen die Daten pro Person pro Block mit Mittelwert, Median und Standarabweichung zusammen.\n\nfuns <- list(mean = mean, median = median, sd = sd)\n\nby_subj <- data %>%\n  drop_na(rt) |> \n  group_by(ID, condition) %>% \n  dplyr::summarise(across(rt, funs, .names = \"{.fn}\"))\n\n\nby_subj \n\n# A tibble: 27 √ó 5\n# Groups:   ID [9]\n   ID    condition  mean median    sd\n   <fct> <fct>     <dbl>  <dbl> <dbl>\n 1 JH    invalid   0.775  0.739 0.163\n 2 JH    neutral   0.799  0.733 0.202\n 3 JH    valid     0.696  0.658 0.190\n 4 NS    invalid   0.894  0.913 0.207\n 5 NS    neutral   0.885  0.844 0.201\n 6 NS    valid     0.738  0.715 0.191\n 7 SS91  invalid   0.489  0.459 0.144\n 8 SS91  neutral   0.542  0.467 0.242\n 9 SS91  valid     0.475  0.418 0.170\n10 VP1   invalid   1.09   1.13  0.209\n# ‚Ä¶ with 17 more rows\n\n\nEinfachere Version:\n\nby_subj <- data |> \n  drop_na(rt) |> \n  group_by(ID, condition) |>  \n  dplyr::summarise(mean = mean(rt),\n                   median = median(rt),\n                   sd = sd(rt))\n\n\nby_subj |> \n  ggplot(aes(x = condition, y = mean, fill = condition)) +\n  geom_col() +\n  geom_line(aes(group = ID), size = 2) +\n  geom_point(size = 8) +\n  scale_fill_manual(\n    values = c(invalid = \"#9E0142\",\n    neutral = \"#C4C4B7\",\n    valid = \"#2EC762\")\n  ) +\n  labs(\n    x = \"Cue\",\n    y = \"Response time\") +\n  theme_linedraw(base_size = 28) +\n  facet_wrap(~ID)\n\n\n\n\n\nse <- function(x, ...) sd(x, ...)/sqrt(length(x))\n\nby_subj <- data %>% \n  group_by(ID, condition) %>% \n  summarise(mean = mean(rt, na.rm = TRUE), \n            median = median(rt, na.rm = TRUE), \n            sd = sd(rt, na.rm = TRUE), \n            se = se(rt, na.rm = TRUE))\n\n\nby_subj |> \n  ggplot(aes(condition, mean)) +\n  geom_line(aes(group = 1), linetype = 3) +    \n  geom_errorbar(aes(ymin = mean-se, ymax = mean+se),\n                width = 0.2, size=1, color=\"blue\") +\n  geom_point(size = 2) +\n  facet_wrap(~ID, scales = \"free_y\")"
  },
  {
    "objectID": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren-1",
    "href": "pages/chapters/04_summarizing_data.html#√ºber-versuchsperson-aggregieren-1",
    "title": "Daten bearbeiten und zusammenfassen",
    "section": "√úber Versuchsperson aggregieren",
    "text": "√úber Versuchsperson aggregieren\n\nrtsum <- data |>\n  drop_na(rt) |> \n    Rmisc::summarySEwithin(measurevar = \"rt\",\n                               withinvars = \"condition\",\n                               idvar = \"ID\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\nrtsum\n\n  condition   N        rt        sd         se         ci\n1   invalid 141 0.7055247 0.2204498 0.01856522 0.03670444\n2   neutral 710 0.7238269 0.2449543 0.00919297 0.01804870\n3     valid 568 0.6716487 0.2482698 0.01041717 0.02046095\n\n\n\np_rt <- rtsum |>\n    ggplot(aes(x = condition, y = rt, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = rt-se, ymax = rt+se), colour=\"red\") +\n    geom_point(shape=21, size=3, fill=\"white\")\n\n\np_rt\n\n\n\n\n\nlibrary(patchwork)\n\n\np_accuracy / p_rt"
  },
  {
    "objectID": "pages/chapters/05_signal_detection_i.html",
    "href": "pages/chapters/05_signal_detection_i.html",
    "title": "Signal Detection Theory: I",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden"
  },
  {
    "objectID": "pages/chapters/05_signal_detection_i.html#parameter-recovery-1",
    "href": "pages/chapters/05_signal_detection_i.html#parameter-recovery-1",
    "title": "Signal Detection Theory: I",
    "section": "Parameter recovery",
    "text": "Parameter recovery\nWe can now attempt to recover the known parameters c and d' from the observed hit and false alarm rates.\n\nyes_observer <- yes_observer |>\n    mutate(hit_rate = Hit/(Hit + Miss),\n           fa_rate = FA/(FA + CR))\n\nyes_observer <- yes_observer |>\n    mutate(zhr = qnorm(hit_rate),\n           zfa = qnorm(fa_rate))\n\nyes_observer <- yes_observer |>\n    mutate(dprime = zhr - zfa,\n           k = - zfa,\n           c = -0.5 * (zhr + zfa)) |>\n    mutate(across(c(dprime, c), round, 2))\n\n\nyes_observer \n\n# A tibble: 1 √ó 11\n    Hit  Miss    FA    CR hit_rate fa_rate   zhr   zfa dprime      k     c\n  <int> <dbl> <int> <dbl>    <dbl>   <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n1    92     8    74    26     0.92    0.74  1.41 0.643   0.76 -0.643 -1.02\n\n\nFor the biased observer, the valuues we used were \\(d' = 1\\) and \\(c = -1\\). Are we able to recover these?\n\nyes_observer |> pull(c, dprime)\n\n 0.76 \n-1.02 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy is it seemingly difficult to recover theses parameters?"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html",
    "href": "pages/chapters/06_signal_detection_ii.html",
    "title": "Signal Detection Theory: II",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nüëâ Daten downloaden"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#daten-importieren",
    "href": "pages/chapters/06_signal_detection_ii.html#daten-importieren",
    "title": "Signal Detection Theory: II",
    "section": "Daten importieren",
    "text": "Daten importieren\nZuerst die Daten downloaden, und speichern.\n\nlibrary(tidyverse)\nd <- read_csv(\"data/session-6.csv\")"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#variablen-bearbeiten",
    "href": "pages/chapters/06_signal_detection_ii.html#variablen-bearbeiten",
    "title": "Signal Detection Theory: II",
    "section": "Variablen bearbeiten",
    "text": "Variablen bearbeiten\nZu factor konvertieren, etc.\n\nd <- d |>\n    select(ID, condition, cue, direction, choice) |>\n    mutate(across(where(is.character), ~as_factor(.)),\n           cue = fct_relevel(cue, \"left\", \"none\", \"right\")) |>\n    drop_na()"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#trials-klassifizieren",
    "href": "pages/chapters/06_signal_detection_ii.html#trials-klassifizieren",
    "title": "Signal Detection Theory: II",
    "section": "Trials klassifizieren",
    "text": "Trials klassifizieren\nAls Hit, Miss, CR und FA.\n\nsdt <- d |>\n    mutate(type = case_when(\n        direction == \"___\" & choice == \"___\" ~ \"___\"),\n        ___,\n        ___,\n        ___)\n\n\nsdt\n\n# A tibble: 2,362 √ó 6\n   ID     condition cue   direction choice type \n   <fct>  <fct>     <fct> <fct>     <fct>  <chr>\n 1 chch04 valid     left  left      left   CR   \n 2 chch04 valid     left  left      left   CR   \n 3 chch04 valid     left  left      right  FA   \n 4 chch04 invalid   right left      left   CR   \n 5 chch04 neutral   none  left      left   CR   \n 6 chch04 valid     left  left      left   CR   \n 7 chch04 invalid   right left      left   CR   \n 8 chch04 valid     left  left      left   CR   \n 9 chch04 neutral   none  left      left   CR   \n10 chch04 neutral   none  right     left   Miss \n# ‚Ä¶ with 2,352 more rows"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-zusammenz√§hlen",
    "href": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-zusammenz√§hlen",
    "title": "Signal Detection Theory: II",
    "section": "SDT Kennzahlen zusammenz√§hlen",
    "text": "SDT Kennzahlen zusammenz√§hlen\n\nsdt_summary <- sdt |>\n    group_by(ID, cue) |>\n    count(type)\n\n\nsdt_summary\n\n# A tibble: 170 √ó 4\n# Groups:   ID, cue [45]\n   ID     cue   type      n\n   <fct>  <fct> <chr> <int>\n 1 chch04 left  CR       29\n 2 chch04 left  FA        3\n 3 chch04 left  Hit       7\n 4 chch04 left  Miss      1\n 5 chch04 none  CR       38\n 6 chch04 none  FA        2\n 7 chch04 none  Hit      34\n 8 chch04 none  Miss      6\n 9 chch04 right CR        5\n10 chch04 right FA        3\n# ‚Ä¶ with 160 more rows"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#von-wide-zu-long-konvertieren",
    "href": "pages/chapters/06_signal_detection_ii.html#von-wide-zu-long-konvertieren",
    "title": "Signal Detection Theory: II",
    "section": "Von wide zu long konvertieren",
    "text": "Von wide zu long konvertieren\n\nsdt_summary <- sdt_summary |>\n    pivot_wider(names_from = type, values_from = n)\n\n\nsdt_summary\n\n# A tibble: 45 √ó 6\n# Groups:   ID, cue [45]\n   ID     cue      CR    FA   Hit  Miss\n   <fct>  <fct> <int> <int> <int> <int>\n 1 chch04 left     29     3     7     1\n 2 chch04 none     38     2    34     6\n 3 chch04 right     5     3    25     7\n 4 chmi14 left     21    10     5     3\n 5 chmi14 none     18    19    29     7\n 6 chmi14 right     3     4    26     4\n 7 J      left     19    12     5     3\n 8 J      none     23    16    33     6\n 9 J      right     6     2    20    12\n10 jh     left     32    NA     5     3\n# ‚Ä¶ with 35 more rows"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#funktionen-definieren",
    "href": "pages/chapters/06_signal_detection_ii.html#funktionen-definieren",
    "title": "Signal Detection Theory: II",
    "section": "Funktionen definieren",
    "text": "Funktionen definieren\n\nreplace_NA <- function(x) {\n    x = ifelse(is.na(x), 0, x)\n    x\n}\n\ncorrect_zero_one <- function(x) {\n    if (identical(x, 0)) {\n        x = x + 0.001\n    } else if (identical(x, 1)) {\n        x = x - 0.001\n    }\n    x\n}"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#nas-ersetzen",
    "href": "pages/chapters/06_signal_detection_ii.html#nas-ersetzen",
    "title": "Signal Detection Theory: II",
    "section": "NAs ersetzen",
    "text": "NAs ersetzen\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(Hit, Miss, FA, CR), replace_NA))"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#hit-rate-und-false-alarm-rate-berechnen",
    "href": "pages/chapters/06_signal_detection_ii.html#hit-rate-und-false-alarm-rate-berechnen",
    "title": "Signal Detection Theory: II",
    "section": "Hit Rate und False Alarm Rate berechnen",
    "text": "Hit Rate und False Alarm Rate berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(hit_rate = ___,\n           fa_rate = ___)"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#werte-0-und-1-korrigieren",
    "href": "pages/chapters/06_signal_detection_ii.html#werte-0-und-1-korrigieren",
    "title": "Signal Detection Theory: II",
    "section": "Werte 0 und 1 korrigieren",
    "text": "Werte 0 und 1 korrigieren\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(hit_rate, fa_rate), correct_zero_one))"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#z-transformation",
    "href": "pages/chapters/06_signal_detection_ii.html#z-transformation",
    "title": "Signal Detection Theory: II",
    "section": "Z-Transformation",
    "text": "Z-Transformation\n\nsdt_summary <- sdt_summary |>\n    mutate(zhr = ___,\n           zfa = ___)"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-berechnen",
    "href": "pages/chapters/06_signal_detection_ii.html#sdt-kennzahlen-berechnen",
    "title": "Signal Detection Theory: II",
    "section": "SDT Kennzahlen berechnen",
    "text": "SDT Kennzahlen berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(dprime = ___,\n           k = ___,\n           c = ___) |>\n    mutate(across(c(dprime, k, c), round, 2))"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#variablen-ausw√§hlen",
    "href": "pages/chapters/06_signal_detection_ii.html#variablen-ausw√§hlen",
    "title": "Signal Detection Theory: II",
    "section": "Variablen ausw√§hlen",
    "text": "Variablen ausw√§hlen\n\nsdt_final <- sdt_summary |>\n    select(ID, cue, dprime, k, c)"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#eine-person-ausw√§hlen.",
    "href": "pages/chapters/06_signal_detection_ii.html#eine-person-ausw√§hlen.",
    "title": "Signal Detection Theory: II",
    "section": "Eine Person ausw√§hlen.",
    "text": "Eine Person ausw√§hlen.\n\nSU6460 <- d |>\n    filter(ID %in% \"SU6460\")\n\nSU6460_sdt <- sdt_final |>\n    filter(ID %in% \"SU6460\")"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#visualisieren",
    "href": "pages/chapters/06_signal_detection_ii.html#visualisieren",
    "title": "Signal Detection Theory: II",
    "section": "Visualisieren",
    "text": "Visualisieren\n\nSU6460_sdt\n\n# A tibble: 3 √ó 5\n# Groups:   ID, cue [3]\n  ID     cue   dprime     k     c\n  <fct>  <fct>  <dbl> <dbl> <dbl>\n1 SU6460 left    0.32  0    -0.16\n2 SU6460 none    0.3  -0.23 -0.38\n3 SU6460 right  -0.27 -0.67 -0.54\n\n\n\nSU6460_sdt |>\n    ggplot(aes(x = cue, y = dprime, group = 1)) +\n    geom_line() +\n    geom_point(shape = 21, size = 3, fill = \"white\")\n\n\n\n\n\nSU6460_sdt |>\n    ggplot(aes(x = cue, y = c, group = 1)) +\n    geom_line() +\n    geom_point(shape = 21, size = 3, fill = \"white\")"
  },
  {
    "objectID": "pages/chapters/06_signal_detection_ii.html#generalized-linear-model",
    "href": "pages/chapters/06_signal_detection_ii.html#generalized-linear-model",
    "title": "Signal Detection Theory: II",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nCheck levels: right muss die zweite Faktorstufe sein!\n\nlevels(SU6460$choice)\n\n[1] \"left\"  \"right\"\n\n\n\nSU6460_glm_k_left <- glm(choice ~ direction,\n                      family = binomial(link = \"probit\"),\n                      data = SU6460 |> filter(cue == \"left\"))\n\nsummary(SU6460_glm_k_left)\n\n\nCall:\nglm(formula = choice ~ direction, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"left\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4006  -1.1774   0.9695   1.1774   1.1774  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)\n(Intercept)    -1.250e-16  2.216e-01   0.000    1.000\ndirectionright  3.186e-01  5.028e-01   0.634    0.526\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.352  on 39  degrees of freedom\nResidual deviance: 54.946  on 38  degrees of freedom\nAIC: 58.946\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nSU6460_glm_k_right <- glm(choice ~ direction,\n                       family = binomial(link = \"probit\"),\n                       data = SU6460 |> filter(cue == \"right\"))\n\nsummary(SU6460_glm_k_right)\n\n\nCall:\nglm(formula = choice ~ direction, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"right\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6651  -1.4614   0.9178   0.9178   0.9178  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)\n(Intercept)      0.6745     0.4818   1.400    0.162\ndirectionright  -0.2722     0.5331  -0.511    0.610\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 50.446  on 39  degrees of freedom\nResidual deviance: 50.181  on 38  degrees of freedom\nAIC: 54.181\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nSU6460 <- SU6460 |>\n    mutate(dir = if_else(direction == \"left\", -1/2, 1/2))\n\n\nSU6460_glm_c_left <- glm(choice ~ dir,\n                       family = binomial(link = \"probit\"),\n                       data = SU6460 |> filter(cue == \"left\"))\nsummary(SU6460_glm_c_left)\n\n\nCall:\nglm(formula = choice ~ dir, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"left\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4006  -1.1774   0.9695   1.1774   1.1774  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   0.1593     0.2514   0.634    0.526\ndir           0.3186     0.5028   0.634    0.526\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.352  on 39  degrees of freedom\nResidual deviance: 54.946  on 38  degrees of freedom\nAIC: 58.946\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nSU6460_glm_c_right <- glm(choice ~ dir,\n                        family = binomial(link = \"probit\"),\n                        data = SU6460 |> filter(cue == \"right\"))\n\nsummary(SU6460_glm_c_right)\n\n\nCall:\nglm(formula = choice ~ dir, family = binomial(link = \"probit\"), \n    data = filter(SU6460, cue == \"right\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6651  -1.4614   0.9178   0.9178   0.9178  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)   0.5384     0.2665   2.020   0.0434 *\ndir          -0.2722     0.5331  -0.511   0.6096  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 50.446  on 39  degrees of freedom\nResidual deviance: 50.181  on 38  degrees of freedom\nAIC: 54.181\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "pages/chapters/07_response_times_i.html",
    "href": "pages/chapters/07_response_times_i.html",
    "title": "Reaktionszeiten: I",
    "section": "",
    "text": "Note\n\n\n\nüëâ R Code f√ºr dieses Kapitel downloaden\nüëâ Daten downloaden"
  },
  {
    "objectID": "pages/chapters/07_response_times_i.html#zusammenfassen-zentrale-tendenz-und-dispersion",
    "href": "pages/chapters/07_response_times_i.html#zusammenfassen-zentrale-tendenz-und-dispersion",
    "title": "Reaktionszeiten: I",
    "section": "Zusammenfassen: zentrale Tendenz und Dispersion",
    "text": "Zusammenfassen: zentrale Tendenz und Dispersion\nMittelwert und Standardabweichung\n\nd %>% \n  group_by(group) %>% \n  summarise(mean = mean(rt),\n            sd = sd(rt))\n\n# A tibble: 2 √ó 3\n  group    mean    sd\n  <fct>   <dbl> <dbl>\n1 control  822.  171.\n2 adhd     839.  214.\n\n\nMedian und Interquartilsbereich\nDer Interquartilsbereich repr√§ntiert den Unterschied zwischen dem ersten (25. Perzentil) und dritten (75. Perzentil) Quartil. In diesem Bereich befinden sich 50% der Datenpunkte.\n\nd %>% \n  group_by(group) %>% \n  summarise(mean = median(rt),\n            q25 = quantile(rt, probs = 0.25),\n            q75 = quantile(rt, probs = 0.75)) |> \n  mutate(IQR = q75 - q25)\n\n# A tibble: 2 √ó 5\n  group    mean   q25   q75   IQR\n  <fct>   <dbl> <dbl> <dbl> <dbl>\n1 control  782.  694.  898.  205.\n2 adhd     784.  694.  923.  228.\n\n\n\nd %>% \n  group_by(group) %>% \n  summarise(mean = median(rt),\n            IQR = IQR(rt))\n\n# A tibble: 2 √ó 3\n  group    mean   IQR\n  <fct>   <dbl> <dbl>\n1 control  782.  205.\n2 adhd     784.  228.\n\n\n\nfuns <- list(mean = mean, median = median, \n             sd = sd, IQR = IQR)\n\nby_group <- d |>\n  group_by(group) |>\n  summarise(across(rt, funs, .names = \"{.fn}\")) |>\n  mutate(across(where(is.numeric), ~round(., 2)))\n\n\nby_group\n\n# A tibble: 2 √ó 5\n  group    mean median    sd   IQR\n  <fct>   <dbl>  <dbl> <dbl> <dbl>\n1 control  822.   782.  171.  205.\n2 adhd     839.   784.  214.  228.\n\n\n\np1 +\n  geom_vline(aes(xintercept = mean), \n             data = by_group,\n             color = \"steelblue\",\n             lwd = 1.5) +\n  geom_vline(aes(xintercept = median), \n             data = by_group, \n             color = \"red\",\n             lwd = 1.5)\n\n\n\n\nZentrale Tendenz bei schiefen Verteilungen\nSowohl Mittelwert als auch Median sind jedoch problematisch als Masse der zentralen Tendenz f√ºr asymmetrische Verteilungen. Der Mittelwert kann durch eine hohe Schiefe und Ausreissern verschoben werden, und repr√§sentiert die zentrale Tendenz der Verteilung nicht besonders gut.\nDer Median ist ein besseres Mass f√ºr eine typische Beobachtung aus dieser Verteilung, ist jedoch nicht erwartungstreu, das heisst der Median √ºbersch√§tzt den Populationsmedian. Der Grad der √úbersch√§tzung steigt mit sinkender Anzahl Beobachtungen (d.h. vor allem bei kleinen Stichproben).\nQuantile\n\ndeciles <- seq(0.1, 0.9, length.out = 9)\n\n\nquantile_fun <- function(x, probs = c(0.25, 0.5, 0.75)) {\n  tibble(rt = quantile(x, probs, type = 8), quantile = probs)\n}\n\n\nd_quantiles <- d %>% \n  group_by(group) %>% \n  summarise(quantile_fun(rt, probs = deciles))\n\n\nd_quantiles\n\n# A tibble: 18 √ó 3\n# Groups:   group [2]\n   group      rt quantile\n   <fct>   <dbl>    <dbl>\n 1 control  649.      0.1\n 2 control  684.      0.2\n 3 control  711.      0.3\n 4 control  740.      0.4\n 5 control  782.      0.5\n 6 control  826.      0.6\n 7 control  875.      0.7\n 8 control  949.      0.8\n 9 control 1045.      0.9\n10 adhd     631.      0.1\n11 adhd     674.      0.2\n12 adhd     712.      0.3\n13 adhd     745.      0.4\n14 adhd     784.      0.5\n15 adhd     828.      0.6\n16 adhd     874.      0.7\n17 adhd     980.      0.8\n18 adhd    1108.      0.9\n\n\nShift function\nWir m√ºssen nun den Dataframe mit den Quantilen ins ‚Äúwide‚Äù Format konvertieren, um zwei Spalten f√ºr die control und adhd Gruppen zu erhalten. Danach k√∂nnen wir die Differenzen zwischen den Gruppen f√ºr jedes Quantil berechnen.\n\nd_quantile_differences <- d_quantiles |> \n  pivot_wider(names_from = \"group\", values_from = \"rt\") \n\n\nd_quantile_differences\n\n# A tibble: 9 √ó 3\n  quantile control  adhd\n     <dbl>   <dbl> <dbl>\n1      0.1    649.  631.\n2      0.2    684.  674.\n3      0.3    711.  712.\n4      0.4    740.  745.\n5      0.5    782.  784.\n6      0.6    826.  828.\n7      0.7    875.  874.\n8      0.8    949.  980.\n9      0.9   1045. 1108.\n\n\n\nd_quantile_differences <- d_quantile_differences |> \n    mutate(`control - adhd` = control - adhd)\n\n\nd_quantile_differences\n\n# A tibble: 9 √ó 4\n  quantile control  adhd `control - adhd`\n     <dbl>   <dbl> <dbl>            <dbl>\n1      0.1    649.  631.            18.2 \n2      0.2    684.  674.            10.6 \n3      0.3    711.  712.            -1.45\n4      0.4    740.  745.            -4.99\n5      0.5    782.  784.            -2.16\n6      0.6    826.  828.            -1.57\n7      0.7    875.  874.             1.42\n8      0.8    949.  980.           -30.4 \n9      0.9   1045. 1108.           -62.8 \n\n\nShift function grafisch darstellen\n\nd_quantile_differences %>% \n  ggplot(aes(x = control, y = `control - adhd`)) +\n  geom_hline(yintercept = 0, linetype = 3) +\n  geom_vline(xintercept = d_quantile_differences %>% \n               filter(quantile == \"50%\") %>% \n               select(adhd) %>% \n               pull(), linetype = 3) +\n  geom_line(aes(group = 1), color = \"steelblue\", size = 2) +\n  geom_point(shape = 21, color = \"steelblue\", fill = \"white\", size = 5, stroke = 1) +\n  coord_cartesian(ylim = c(-300, 300))\n\n\n\n\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"GRousselet/rogme\")\n\n\n# library(rogme)\n\n\nout <- rogme::shifthd(d, rt ~ group)\n\n\nrogme::plot_sf(out)\n\n[[1]]"
  },
  {
    "objectID": "pages/chapters/08_response_times_ii.html",
    "href": "pages/chapters/08_response_times_ii.html",
    "title": "Reaktionszeiten: II",
    "section": "",
    "text": "Hierarchical Shift Function\nWir schauen uns Daten aus einem Lexical Decision Task (Wagenmakers and Brown 2007) an, bei dem Versuchspersonen W√∂rter als entweder word oder non-word klassifizieren mussten. Es ist bekannt, dass W√∂rter welche h√§ufiger vorkommen schneller klassifiziert werden k√∂nnen, als seltene W√∂rter. In diesem Experiment mussten Versuchspersonen diesen Task unter zwei Bedingungen durchf√ºhren. In der speed Bedingung mussten sie sich so schnell wie m√∂glich entscheiden, in der accuracy Bedingung mit so wenig Fehler wie m√∂glich.\nHier untersuchen wir also den Unterschied in der Reaktionszeit zwischen zwei ‚Äúwithin‚Äù Bedingungen. Die Daten befinden sich im Package rtdists, welches zuerst installiert werden sollte.\n\nlibrary(tidyverse)\nlibrary(rtdists)\nlibrary(viridis)\n\ndata(speed_acc) \n\nspeed_acc <- speed_acc |>\n  as_tibble()\n\n\ndf_speed_acc <- speed_acc |> \n   # zwischen 180 ms and 3000 ms\n  filter(rt > 0.18, rt < 3) |> \n   # zu Character konvertieren (damit filter funktioniert)\n  mutate(across(c(stim_cat, response), as.character)) |> \n  # Korrekte Antworten\n  filter(response != 'error', stim_cat == response) |> \n  # wieder zu Factor konvertieren\n  mutate(across(c(stim_cat, response), as_factor))\n\n\ndf_speed_acc\n\n# A tibble: 27,936 √ó 9\n   id    block condition stim  stim_cat frequency   response    rt censor\n   <fct> <fct> <fct>     <fct> <fct>    <fct>       <fct>    <dbl> <lgl> \n 1 1     1     speed     5015  nonword  nw_low      nonword  0.7   FALSE \n 2 1     1     speed     6481  nonword  nw_very_low nonword  0.46  FALSE \n 3 1     1     speed     3305  word     very_low    word     0.455 FALSE \n 4 1     1     speed     4468  nonword  nw_high     nonword  0.773 FALSE \n 5 1     1     speed     1047  word     high        word     0.39  FALSE \n 6 1     1     speed     5036  nonword  nw_low      nonword  0.603 FALSE \n 7 1     1     speed     1111  word     high        word     0.435 FALSE \n 8 1     1     speed     6561  nonword  nw_very_low nonword  0.524 FALSE \n 9 1     1     speed     1670  word     high        word     0.427 FALSE \n10 1     1     speed     6207  nonword  nw_very_low nonword  0.456 FALSE \n# ‚Ä¶ with 27,926 more rows\n\n\nWir schauen uns vier Versuchspersonen grafisch an:\n\ndata_plot <- df_speed_acc |> \n  filter(id %in% c(1, 8, 11, 15))\n\ndata_plot |> \n  ggplot(aes(x = rt)) + \n    geom_histogram(aes(fill = condition), alpha = 0.5, bins = 60) + \n    facet_wrap(~id) +\n    coord_cartesian(xlim=c(0, 1.6)) +\n    scale_fill_viridis(discrete = TRUE, option = \"E\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSchauen Sie sich alle Vpn an.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWas w√ºrden Sie anhand der Histogramme erwarten?\n\n\n\n\n\n\n\n\nNote\n\n\n\nBerechnen Sie nun die Differenzen der Dezile zwischen den Bedingungen f√ºr jede Versuchsperson.\n\n\n\nout_speed_acc <- rogme::hsf_pb(df_speed_acc, rt ~ condition + id)\n\n\np_speed_acc <- rogme::plot_hsf_pb(out_speed_acc, interv = \"ci\")\np_speed_acc\n\n\n\n\nIn dieser Grafik sehen wir auf der X-Achse die Dezile der accuracy Bedingung und auf der Y-Achse die Differenz accuracy - speed. Die Differenz ist bei jedem Dezil positiv und scheint steig gr√∂sser zu werden. Die accuracy Bedingung f√ºhrt also zu l√§ngeren und variableren Reaktionszeiten. Die Bedingungen unterscheiden sich im Median, aber wenn wir nur das ber√ºcksichtigt h√§tten, w√ºrden wir verpassen, dass sich die Verteilungen sehr stark am rechten Ende der Verteilung unterscheiden.\nZum Vergleich berechnen wir noch Bedingungsmittelwerte der Median Reaktionszeiten.\n\nby_subject <- df_speed_acc |> \n  group_by(id, condition) |> \n  summarise(mean = median(rt))\n\nagg <- Rmisc::summarySEwithin(by_subject,\n                       measurevar = \"mean\",\n                       withinvars = \"condition\",\n                       idvar = \"id\",\n                       na.rm = FALSE,\n                       conf.interval = .95)\n\n\nagg |> \n  ggplot(aes(condition, mean, fill = condition)) +\n  geom_col(alpha = 0.8) +\n  geom_line(aes(group = 1), linetype = 3) +   \n  geom_errorbar(aes(ymin = mean-se, ymax = mean+se),\n                width = 0.1, size=1, color=\"black\") +\n  scale_fill_viridis(discrete=TRUE, option=\"cividis\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nReferences\n\nWagenmakers, Eric-Jan, and Scott Brown. 2007. ‚ÄúOn the Linear Relation Between the Mean and the Standard Deviation of a Response Time Distribution.‚Äù Psychological Review 114 (3): 830‚Äì41. https://doi.org/10.1037/0033-295X.114.3.830.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {Reaktionszeiten: {II}},\n  date = {2022-04-12},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/08_response_times_ii.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 2022. ‚ÄúReaktionszeiten: II.‚Äù April 12, 2022.\nhttps://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/08_response_times_ii.html."
  },
  {
    "objectID": "pages/chapters/09_evidence_accumulation_1.html#random-walk-simulieren",
    "href": "pages/chapters/09_evidence_accumulation_1.html#random-walk-simulieren",
    "title": "Evidence Accumulation Models: I",
    "section": "Random walk simulieren",
    "text": "Random walk simulieren\nEin random walk ist das Resultat der Aufsummierung von Zufallszahlen. Probieren Sie es selber aus; simulieren Sie einen random walk mit 100 Zeitschritten. Fangen Sie bei \\(0\\) an, ziehen Sie 99 normalverteilte Zufallszahlen und berechnen Sie die kumulierte Summe. Plotten Sie das Resultat.\nDieser random walk hat keinen Trend, weil wir immer aus einer Normalverteilung mit Mittelwert \\(\\mu=0\\) ziehen. Wenn wir stattdessen aus einer Verteilung mit \\(\\mu=0.1\\) ziehen, erhalten wir einen positiven Trend.\n\nset.seed(546)\n\n# hier z.B> standardnormalverteilte Zahlen\nzufallszahlen_1 <- c(0, rnorm(99, 0, 1))\nrandom_walk_1 <- cumsum(zufallszahlen_1)\n\nplot(1:100, random_walk_1, type = \"s\", col = \"#7fc97f\", \n     ylim=c(-10,30), lwd = 2, \n     xlab = \"Zeit\", ylab=\"Random Walk\")\n\n\n\n\n\nzufallszahlen_2 <- c(0, rnorm(99, 0.3, 1))\nrandom_walk_2 <- cumsum(zufallszahlen_2)\n\nplot(1:100, random_walk_1, type = \"s\", col = \"#7fc97f\", \n     ylim=c(-10,30), lwd = 2, \n     xlab = \"Zeit\", ylab=\"Random Walk\")\nlines(1:100, random_walk_2, pch = 18, col = \"#beaed4\", \n      type = \"s\", lwd = 2)\n\nlegend(\"topleft\", legend=c(\"Ohne Trend\", \"Mit Trend\"),\n       col=c(\"#7fc97f\", \"#beaed4\"), lty = c(1, 1))"
  },
  {
    "objectID": "pages/chapters/09_evidence_accumulation_1.html#evidenzakkumulierung",
    "href": "pages/chapters/09_evidence_accumulation_1.html#evidenzakkumulierung",
    "title": "Evidence Accumulation Models: I",
    "section": "Evidenzakkumulierung",
    "text": "Evidenzakkumulierung\nDie Evidenzakkumulierung wird analog modelliert. Wenn wir explizit die Zeitschritte als Iterationen aufschreiben, k√∂nnen wir dies in R mit einer for Loop machen.\n\ndriftrate <- 0.5\nsd <- 0.1\n\nn_steps <- 10\nevidence <- rep(NA, n_steps)\n\ndv <- rep(NA, n_steps)\n\ntime_steps <- 1:n_steps\n\n# Wir ziehen den ersten Wert aus der Verteilung\nevidence[1] <- rnorm(1, mean = driftrate, sd = sd)\ndv[1] <- evidence[1]\n\n# f√ºr jeden weitern Zeitpunkt ziehen wir wieder eine Zufallszahl und addieren zur kumulierten DV\nfor (t in 2:n_steps) {\n    evidence[t] <- rnorm(1, mean = driftrate, sd = sd)\n    dv[t] <- dv[t-1] + evidence[t]\n}\n\n\ntibble(time_steps, evidence, dv) |> \n    pivot_longer(c(evidence, dv), names_to = \"type\", values_to = \"value\") |> \n    ggplot(aes(time_steps, value, linetype = type, color = type)) +\n    geom_line() +\n    geom_point(size = 4) +\n    scale_color_viridis_d(begin = 0.2, end = 0.5)\n\n\n\n\nDie Decision Variable dv repr√§sentiert nun die kumulierten Evidenz, aufgrund dessen das Gehirn eine Entscheiung treffen kann. Wenn die Decision Variable entweder gr√∂sser als die ober Grenze ist, oder kleiner als die untere Grenze, wird die Evidenzakkumulierung abgebrochen, und eine Antwort wird ausgel√∂st. Wir k√∂nnen nun noch die ‚Äúnon-decision time‚Äù hinzuf√ºgen, und den Anfangspunkt der Evidenzakkumulierung. Dieser Anfangspunkt ist ein sehr wichtiger Parameter, denn wenn der Anfagnspunkt nicht genau in der Mitte zwischen den beiden Grenzen liegt, dann braucht es nat√ºrlich weniger Evindenz, um die Grenze zu erreichen, welche n√§her beim Anfangspunkt liegt.\nAnhand der folgenden Funktion l√§sst sich ein simpler Entscheidungsprozess simulieren, welcher alle wesentlichen Komponenten enth√§lt: die drift rate, boundary separation, bias und die non-decision time ndt.\n\n\n\n\n\n\n\n\n\nParameter\nBedeutung\nAnwendung\n\n\n\ndrift rate\nQualit√§t der Evidenz pro Zeiteinheit\nTask Schwierigkeit, F√§higkeit\n\n\nbias\nAnfangspunkt der Evidenzakkumulierung\nA priori Pr√§ferenz f√ºr eine der beiden Alternativen\n\n\nboundary separation\nVorsicht (caution)\nSpeed-Accuracy Trade-off\n\n\nnon-decision time\nVerz√∂gerung\nPeriphere Prozesse"
  },
  {
    "objectID": "pages/chapters/09_evidence_accumulation_1.html#ddm-function",
    "href": "pages/chapters/09_evidence_accumulation_1.html#ddm-function",
    "title": "Evidence Accumulation Models: I",
    "section": "DDM Function",
    "text": "DDM Function\nUm den Code zu verstehen, brauchen wir ein paar neue R Funktionen.\n\nseq_along(c(\"a\", \"b\", \"c\"))\n\n[1] 1 2 3\n\n\n\nmin(1, -2)\n\n[1] -2\n\n\n\nmax(3, 4)\n\n[1] 4\n\n\n\nas.numeric(2)\n\n[1] 2\n\n\n\narray(dim = 2)\n\n[1] NA NA\n\n\nAus einer for Loop ausbrechen:\n\nbreak()\n\nZeitangaben in Sekunden:\n\nbias <- 0.5\ndriftrate <- 0.8\ndecision_boundary <- 2\nndt <- 0.5\ndiffvar <- 0.1\ndt <- 0.001\nmax_time <- 6\n\nBias muss zwischen den decisioon boundaries liegen.\n\n# rescale bias so that 0.5 lies halfway between upper and lower bound\nbias <- as.numeric(2 * decision_boundary * bias - decision_boundary)\n\nZeitschritte:\n\n# initialize time_steps and dv\ntime_steps <- max_time/dt\ndv <- array(dim = time_steps)\n\nEvidenzakkumulierung, ausgehend vom bias:\n\n# start accumulating from bias (starting point)\ndv[1] <- rnorm(1, mean = bias, sd = sqrt(dt))\n\n\nfor (j in 2:time_steps) {\n\n    # non-decision time\n    if (j <= ndt/dt) {\n        # dv bleibt gleich\n        dv[j] <- dv[j-1]\n    }\n    else {\n        # Akkumulierung f√§ngt an\n        error <- rnorm(1, 0, sqrt(diffvar * dt))\n        # dv ist alte dv plus drift plus noise\n        dv[j] <- dv[j-1] + driftrate * dt + error  # Cobb & Zacks (1985), Eq. 1.14\n        \n        # decision\n        if (abs(dv[j]) > decision_boundary) {\n            dv[j] <- dplyr::if_else(dv[j] > 0,\n                             min(dv[j], decision_boundary),\n                             max(dv[j], -decision_boundary))\n            break()\n        }\n    }\n}\n\n\nd <- dplyr::tibble(time = round(seq_along(dv) * dt, 3),\n                     dv = dv,\n                     steps = seq_along(dv),\n                     driftrate = driftrate,\n                     decision_boundary = decision_boundary,\n                     bias = bias,\n                     ndt = ndt)\n\n\nd |> \n    ggplot(aes(time, dv)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    geom_hline(yintercept = c(-2, 2), color = \"black\", size = 1) \n\nWarning: Removed 2782 row(s) containing missing values (geom_path).\n\n\n\n\n\nFunction\n\ndrift_diffusion <- function(bias = 0.5,\n                            driftrate = 0.8,\n                            decision_boundary = 2,\n                            ndt = 0.5,\n                            diffvar = 0.1,\n                            dt = 0.001,\n                            max_time = 6) {\n\n    assertthat::assert_that(diffvar > 0)\n\n    # rescale bias so that 0.5 lies halfway between upper and lower bound\n    bias <- as.numeric(2 * decision_boundary * bias - decision_boundary)\n\n    # initialize time_steps and dv\n    time_steps <- max_time/dt\n    dv <- array(dim = time_steps)\n\n    # start acumulating from bias (starting point)\n    dv[1] <- rnorm(1, mean = bias, sd = sqrt(dt))\n\n    for (j in 2:time_steps) {\n\n        # non-decision time\n        if (j <= ndt/dt) {\n            dv[j] <- dv[j-1]\n        }\n        else {\n            error <- rnorm(1, 0, sqrt(diffvar * dt))\n            dv[j] <- dv[j-1] + driftrate * dt + error  # Cobb & Zacks (1985), Eq. 1.14\n            if (abs(dv[j]) > decision_boundary) {\n                dv[j] <- dplyr::if_else(dv[j] > 0,\n                                 min(dv[j], decision_boundary),\n                                 max(dv[j], -decision_boundary))\n                break()\n            }\n        }\n    }\n    d <- dplyr::tibble(time = round(seq_along(dv) * dt, 3),\n                         dv = dv,\n                         steps = seq_along(dv),\n                         driftrate = driftrate,\n                         decision_boundary = decision_boundary,\n                         bias = bias,\n                         ndt = ndt)\n    return(d)\n}"
  },
  {
    "objectID": "pages/chapters/09_evidence_accumulation_1.html#auswirkungen-der-parameter",
    "href": "pages/chapters/09_evidence_accumulation_1.html#auswirkungen-der-parameter",
    "title": "Evidence Accumulation Models: I",
    "section": "Auswirkungen der Parameter",
    "text": "Auswirkungen der Parameter\nWir k√∂nnen nun einige Trials plotten, um den Effekt dieser Parameter zu visualisieren.\nDrift rate\nWir fangen an mit der drift rate. Wenn diese \\(>> 0\\) ist, wird die Obergrenze schnell erreicht, und es wir wenige Fehler geben. Ist die drift rate kleiner, aber immer noch \\(> 0\\), wird die durschnittliche Zeit l√§nger, um eine korrekte Antwort zu geben.\n\nset.seed(829)\n\nslow <- drift_diffusion(driftrate = 0.8) |> mutate(type = \"slow\")\nfast <- drift_diffusion(driftrate = 1.2) |> mutate(type = \"fast\")\n\nfastslow <- bind_rows(fast, slow) \n\nfastslow |> \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    geom_hline(yintercept = c(-2, 2), color = \"black\", size = 1) +\n    ggtitle(\"Grosse vs. kleine Drift Rate\")\n\n\n\n\nBias\nWenn der bias \\(>0.5\\) ist, wird die Obergrenze schneller erreicht. Hier gibt es nun eine Interaktion mit der drift rate‚Äîist diese klein, und der bias \\(<0.5\\), ist die Chance, schnelle Fehler zu machen erh√∂ht.\n\nset.seed(29)\n\nunbiased <- drift_diffusion(bias = 0.5) |> mutate(type = \"unbiased\")\nupbiased <- drift_diffusion(bias = 0.7) |> mutate(type = \"upbiased\")\ndownbiased <- drift_diffusion(bias = 0.3) |> mutate(type = \"downbiased\")\n\n\n\nbias <- bind_rows(unbiased, upbiased, downbiased) \n\nbias |> \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    geom_hline(yintercept = c(-2, 2), color = \"black\", size = 1) +\n    ggtitle(\"Anfangspunkte\")\n\nWarning: Removed 8605 row(s) containing missing values (geom_path).\n\n\n\n\n\nBoundary separation\nLiegen die Grenzen weiter auseinander, braucht es mehr akkumulierte Evidenz, um eine der Grenzen zu erreichen. Dies f√ºhrt dazu, dass weniger Fehler gemacht werden, da die zuf√§llige Fluktuation √ºber l√§ngere Zeit hinweg einen weniger starken Einfluss hat. Deshalb kann eine Verschiebung der Grenzen den Speed-Accuracy Trade-off erkl√§ren.\n\nset.seed(84)\n\ncarefree <- drift_diffusion(decision_boundary = 1.6) |> mutate(type = \"carefree\")\ncautious <- drift_diffusion(decision_boundary = 2.1) |> mutate(type = \"cautious\")\n\ncautiouscareless <- bind_rows(carefree, cautious) \n\ndecision_boundaries <- tribble(~type, ~decision_boundary,\n                               \"carefree\", 1.6,\n                               \"cautious\", 2.1)\ncautiouscareless |> \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    geom_hline(aes(yintercept = decision_boundary, color = type), data = decision_boundaries) +\n    geom_hline(aes(yintercept = -decision_boundary, color = type), data = decision_boundaries) +\n    ggtitle(\"Unterschiede im Abstand zwischen den Grenzen\")\n\nWarning: Removed 7157 row(s) containing missing values (geom_path).\n\n\n\n\n\nNon-decision time\nEine Ver√§nderung der non-decision time hat eine Auswirkung auf die durschnittliche Reaktionszeit, hat aber keinen Einfluss auf die Fehlerrate.\n\nset.seed(4534)\n\nlongndt <- drift_diffusion(ndt = 0.7) |> mutate(type = \"longndt\")\nshortndt <- drift_diffusion(ndt = 0.2) |> mutate(type = \"shortndt\")\n\nndt <- bind_rows(longndt, shortndt) \n\nndts <- tribble(~type, ~ndt,\n                \"longndt\", 0.7,\n                \"shortndt\", 0.2)\n\nndt |> \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_viridis_d(end = 0.8) +\n    geom_vline(aes(xintercept = ndt, color = type), data = ndts) +\n    geom_hline(yintercept = c(-2, 2), color = \"black\", size = 1) +\n    ggtitle(\"Unterschiede in der Non-Decision Time\")\n\nWarning: Removed 6407 row(s) containing missing values (geom_path)."
  },
  {
    "objectID": "pages/chapters/09_evidence_accumulation_1.html#simulationen",
    "href": "pages/chapters/09_evidence_accumulation_1.html#simulationen",
    "title": "Evidence Accumulation Models: I",
    "section": "Simulationen",
    "text": "Simulationen\nDie Verteilungsfunktion sind im R Package rtdists enthalten. Damit k√∂nnen zum Beispiel Zufallszahlen aus der DDM Verteilung ziehen, ohne dass wir den Prozess wie oben Schritt f√ºr Schritt modellieren m√ºssen.\n\nlibrary(rtdists)\n\nWir k√∂nnen so ein Experiment simulieren, bei dem die Fehler im Schnitt schneller als die korrekten Antworten sind, indem wir eine A Priori Pr√§ferenz f√ºr die Untergrenze definieren (z = 0.2).\nDie 5 wichtigsten Argumente der Funktion sind:\nn: Anzahl Zufallszahlen\na: boundary separation\nv: drift rate\nt0: non-decision time\nz: bias\n\nrts <- rdiffusion(500, a = 1, v = 2, t0 = 0.5, z = 0.2)\n\nglimpse(rts)\n\nRows: 500\nColumns: 2\n$ rt       <dbl> 0.9946603, 0.6598096, 0.5840942, 0.8145105, 0.7448141, 0.6305‚Ä¶\n$ response <fct> upper, upper, lower, upper, upper, upper, upper, upper, upper‚Ä¶\n\n\n\nhead(rts)\n\n         rt response\n1 0.9946603    upper\n2 0.6598096    upper\n3 0.5840942    lower\n4 0.8145105    upper\n5 0.7448141    upper\n6 0.6305719    upper\n\n\n\nrts |> \n  ggplot(aes(rt, response, fill = response)) +\n  geom_violin() +\n  geom_jitter(height = 0.1, alpha = 0.5) +\n  scale_fill_viridis_d(option = \"B\", direction = -1, \n                       begin = 1/3, end = 3/3) +\n  xlim(c(0, 2))\n\n\n\n\n\nrts |> \n    group_by(response) |> \n    summarise(mean = mean(rt),\n              median = median(rt),\n              sd = sd(rt))\n\n# A tibble: 2 √ó 4\n  response  mean median    sd\n  <fct>    <dbl>  <dbl> <dbl>\n1 lower    0.585  0.550 0.106\n2 upper    0.767  0.724 0.159"
  },
  {
    "objectID": "pages/chapters/10_evidence_accumulation_2.html#fitting-models-to-data-parameter-estimation",
    "href": "pages/chapters/10_evidence_accumulation_2.html#fitting-models-to-data-parameter-estimation",
    "title": "Evidence Accumulation Models: II",
    "section": "Fitting models to data: Parameter estimation",
    "text": "Fitting models to data: Parameter estimation\nThe goal of fitting a model to data is to find the best-fitting parameter values. In other words, we want to find parameters of the model for which the probability of observing the data is maximized. i.e.¬†we want to estimate parameters from the data. This is done by minimizing the error between the model‚Äôs predictions and the data.\nHere, we will look at maximum likelihood estimation (MLE). The likelihood function is a function that represents how likely it is to obtain a certain set of observations from a given model. We‚Äôre considering the set of observations as fixed, and now we‚Äôre considering under which set of model parameters we would be most likely to observe those data.\nDue to the fact that computers have numerical issues when calculating with very small numbers, i.e.¬†probabilities, we don‚Äôt work with probabilities, but instead the natural logarithm of the probabilities (in R this is the log() function).\n\nvery_small_number <- 1e-6\nvery_small_number\n\n[1] 1e-06\n\n\n\nlog(very_small_number)\n\n[1] -13.81551\n\n\nA further point is that, by convention, most existing routines for optimizing functions perform minimization, instead of maximization. Therefore, if we want to use functions, such as e.g.¬†optim() in R, we need to minimize the negative log likelihood. This is equivalent to maximizing the likelihood; minimizing the negative log likelihood results in finding the set of parameters for which the probability of the data is maximal.\n\ntibble(x = seq(0, 1, length = 1e3)) |> \n  ggplot(aes(x))+ \n  stat_function(fun=function(x) -log(x),\n                size = 1.5) +\n  xlab(\"Probability\") + ylab(\"Negative log likelihood\")"
  },
  {
    "objectID": "pages/chapters/10_evidence_accumulation_2.html#generate-some-data-with-known-parameters",
    "href": "pages/chapters/10_evidence_accumulation_2.html#generate-some-data-with-known-parameters",
    "title": "Evidence Accumulation Models: II",
    "section": "Generate some data with known parameters",
    "text": "Generate some data with known parameters\nGenerate RTs from the diffusion model as data.\n\nlibrary(rtdists)\n\ndrift <- 0.2\nboundary <- 0.1\nbias <- 0.5\nndt <- 0.05\n\ngenparams <- c(boundary, \n               drift, \n               bias*boundary,\n               ndt)           \nnames(genparams) <- c(\"a\", \"v\", \"z\", \"t0\") \n\nIt is important to note that the *diffusion() functions take the absolute starting as an argument, instead of the relative starting point. For this reason, we need to compute the absolute starting as \\(z = bias * a\\). If the relative bias is 0.5, meaning that it is halfway between the boundaries, then the absolute starting point is z = 0.5 * a.\n\nset.seed(54)\nrts <- rdiffusion(n = 500,\n                  a=genparams[\"a\"],\n                  v=genparams[\"v\"], \n                  t0=genparams[\"t0\"], \n                  z=genparams[\"z\"],\n                  s=0.1)  \n\n\nrts |>\n  ggplot(aes(rt, response, fill = response)) +\n  geom_violin() +\n  geom_jitter(height = 0.1, alpha = 0.5) +\n  scale_fill_viridis_d(option = \"B\", direction = -1,\n                       begin = 1/3, end = 3/3) +\n  xlim(c(0, 1.5))"
  },
  {
    "objectID": "pages/chapters/10_evidence_accumulation_2.html#define-log-likelihood-function",
    "href": "pages/chapters/10_evidence_accumulation_2.html#define-log-likelihood-function",
    "title": "Evidence Accumulation Models: II",
    "section": "Define log likelihood function",
    "text": "Define log likelihood function\nThis function returns the negative log probability of the data, given the parameters.\n\ndiffusionloglik <- function(pars, rt, response) \n{\n  likelihoods <- ddiffusion(rt, response=response,                 \n                          a=pars[\"a\"], \n                          v=pars[\"v\"], \n                          t0=pars[\"t0\"], \n                          z=pars[\"z\"], \n                          s=0.1)      \n  return(-sum(log(likelihoods)))\n}"
  },
  {
    "objectID": "pages/chapters/10_evidence_accumulation_2.html#generate-starting-values-for-parameters",
    "href": "pages/chapters/10_evidence_accumulation_2.html#generate-starting-values-for-parameters",
    "title": "Evidence Accumulation Models: II",
    "section": "Generate starting values for parameters",
    "text": "Generate starting values for parameters\n\nset.seed(342)\ninit_params <- c(runif(1, 0.01, 0.4),\n                runif(1, 0.01, 0.1),\n                runif(1, 0, 0.1),\n                runif(1, 0, 0.1))\n\nnames(init_params) <- c(\"a\", \"v\", \"z\", \"t0\")"
  },
  {
    "objectID": "pages/chapters/10_evidence_accumulation_2.html#estimate-parameters",
    "href": "pages/chapters/10_evidence_accumulation_2.html#estimate-parameters",
    "title": "Evidence Accumulation Models: II",
    "section": "Estimate parameters",
    "text": "Estimate parameters\nNow, we can evaluate the density function for the DDM, given, the initial parameters:\n\npars <- init_params\n\nlik <- ddiffusion(rt = rts$rt, \n           response = rts$response,                 \n           a = pars[\"a\"], \n           v = pars[\"v\"], \n           t0 = pars[\"t0\"], \n           z = pars[\"z\"], \n           s = 0.1) \n\nNext, we can compute the negative log likelihood. If you look at this vector, you will see that there a number of data points for which the log likelihood is -Inf, meaning that the probability of that data point given the initial parameters is \\(0\\).\n\n\nThis means that we should try to make our function robust; we will do so in the next exercise.\n\nloglik <- log(lik)\n\nWe can also use our function to calculate the negative log likelihood.\n\ndiffusionloglik(init_params, rt = rts$rt, \n           response = rts$response)\n\n[1] 503.2724\n\n\nNow, we can repeatedly evaluate the log likelihood, and adjust the parameters so that the negative log likelihood becomes successively smaller. To do this, we use the R function optim().\n\nfit <- optim(init_params, \n             diffusionloglik, \n             gr = NULL, \n             rt = rts$rt, \n             response = rts$response)\n\n\nfit\n\n$par\n         a          v          z         t0 \n0.09861885 0.18515304 0.04884865 0.05056087 \n\n$value\n[1] -200.4542\n\n$counts\nfunction gradient \n     255       NA \n\n$convergence\n[1] 0\n\n$message\nNULL"
  },
  {
    "objectID": "pages/chapters/10_evidence_accumulation_2.html#compare-estimated-parameters-to-true-parameters",
    "href": "pages/chapters/10_evidence_accumulation_2.html#compare-estimated-parameters-to-true-parameters",
    "title": "Evidence Accumulation Models: II",
    "section": "Compare estimated parameters to true parameters",
    "text": "Compare estimated parameters to true parameters\n\nround(fit$par, 3)\n\n    a     v     z    t0 \n0.099 0.185 0.049 0.051 \n\n\n\ngenparams\n\n   a    v    z   t0 \n0.10 0.20 0.05 0.05"
  },
  {
    "objectID": "pages/chapters/10_evidence_accumulation_2.html#example-full-ddm",
    "href": "pages/chapters/10_evidence_accumulation_2.html#example-full-ddm",
    "title": "Evidence Accumulation Models: II",
    "section": "Example: full DDM",
    "text": "Example: full DDM\n\ndiffusionloglik <- function(pars, rt, response) \n{\n  likelihoods <- tryCatch(ddiffusion(rt, response=response,                 \n                          a=pars[\"a\"], \n                          v=pars[\"v\"], \n                          t0=pars[\"t0\"], \n                          z=0.5*pars[\"a\"], \n                          sz=pars[\"sz\"], \n                          st0=pars[\"st0\"], \n                          sv=pars[\"sv\"],s=.1,precision=1),\n                        error = function(e) 0)  \n  \n  if (any(likelihoods==0)) return(1e6) \n  return(-sum(log(likelihoods)))\n}  \n\n\ngenparms <- c(.1, .2, .5, .05, .2, .05)           \nnames(genparms) <- c(\"a\", \"v\", \"t0\", \"sz\", \"st0\", \"sv\") \n\n\nrts <- rdiffusion(500, a=genparms[\"a\"],\n                       v=genparms[\"v\"], \n                       t0=genparms[\"t0\"], \n                       z=0.5*genparms[\"a\"],\n                       d=0, \n                       sz=genparms[\"sz\"], \n                       sv=genparms[\"sv\"],  \n                       st0=genparms[\"st0\"],\n                       s=.1)   \n\n\n#generate starting values for parameters     \nsparms <- c(runif(1, 0.01, 0.4), \n           runif(1, 0.01, 0.5),\n           0.3, \n           runif(1, 0.02, 0.08),\n           runif(1, .1, .3),\n           runif(1, 0, 0.1))\nnames(sparms) <- c(\"a\", \"v\", \"t0\", \"sz\", \"st0\", \"sv\") \n\n\n#now estimate parameters\nfit2 <- optim(sparms, diffusionloglik, gr = NULL, \n               rt=rts$rt, response=rts$response)\n\n\nround(fit2$par, 3)\n\n    a     v    t0    sz   st0    sv \n0.107 0.230 0.500 0.066 0.189 0.073 \n\n\n\ngenparms\n\n   a    v   t0   sz  st0   sv \n0.10 0.20 0.50 0.05 0.20 0.05"
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#data-analysis",
    "href": "pages/chapters/11_data-analysis-1.html#data-analysis",
    "title": "Data analysis: I",
    "section": "Data analysis",
    "text": "Data analysis\nIn this chapter, we will tackle two important topics:\n\nWhat methods should we use when modelling and summarizing data from multiple participants, and\nWhich alternative approaches are available when methods for null hypothesis significance testing (NHST) are not suitable.\n\nThe latter is often the case in neuroscience, where low power due to small sample sizes is a well-known problem (Button et al. 2013). The alternative approach we will discuss here is Bayesian data analysis, which also has the benefit is allowing us to provide evidence for or against hypotheses, something which is not possible using NHST.\n\nButton, Katherine S., John P. A. Ioannidis, Claire Mokrysz, Brian A. Nosek, Jonathan Flint, Emma S. J. Robinson, and Marcus R. Munaf√≤. 2013. ‚ÄúPower Failure: Why Small Sample Size Undermines the Reliability of Neuroscience.‚Äù Nature Reviews Neuroscience 14 (5): 365‚Äì76. https://doi.org/10.1038/nrn3475."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#modelling-and-summarizing-data-from-multiple-participants",
    "href": "pages/chapters/11_data-analysis-1.html#modelling-and-summarizing-data-from-multiple-participants",
    "title": "Data analysis: I",
    "section": "Modelling and summarizing data from multiple participants",
    "text": "Modelling and summarizing data from multiple participants\nIn most neuroscience experiments, we are interested in effects of manipulations at the individual level, but we also need to be sure that theses effects hold at the group level, and are not particular to certain individuals. However, reporting effects at the group level is not straightforward.In short, there are three possiblities when combining informatin from multiple participants:\nComplete pooling\nWe can pretend that all our data come from 1 participant, by averaging over participants. In statistics, this is know as complete pooling, meaning that all data are pooled and we are no longer interested in individual participants. The pooled data are then used to estimate parameters, and to perfom hypothesis testing. While there are cases in which it is possible to analyze averaged data, in general this approach can lead to errors, and needs to be treated with care. A classic example is shown in Figure¬†1. The thin solid lines represent individual learning curves. Each indivdial appears to learn very rapidly (and linearly), but there is a huge variation in the onset of learning. The filled circles represent a naive attempt at averaging over all individuals. The result suggests that learning is gradual and starts from the outset; however the average learning curve looks nothing like the individual learning curves. This type of analysis can lead to erroneous conclusions.\n\n\nFigure 1: Learning curves"
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#no-pooling",
    "href": "pages/chapters/11_data-analysis-1.html#no-pooling",
    "title": "Data analysis: I",
    "section": "No pooling",
    "text": "No pooling\nThis means that data we estimate parameters of our models for each participant separately. While this appears to be a more sensible approach, there is the danger of over-fitting. This means that we give up the ability to generalize because our models are too sensitive to noise inherent in our data. Estimating parameters at the individual level is usually followed by a group-level statistical analysis. We might estimate the parameters of a signal detection or diffusion decision model for each participant individually, and then perform hypothesis tests on those parameters at the group level. For example, in a within-person design, we estimate individual bias parameters for each person in two conditions, and then perform a t-test at the group level to discover whether there is a difference between conditions. A problem that is associated with this approach is that we have to treat the estimated parameters as ‚Äúdata‚Äù at the group level, thus ignoring any uncertainty associated with those parameters; this can lead to over-confidence in our results. Nevertheless, this two-stage analysis is by far the most common approach in neuroscience."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#partial-pooling",
    "href": "pages/chapters/11_data-analysis-1.html#partial-pooling",
    "title": "Data analysis: I",
    "section": "Partial pooling",
    "text": "Partial pooling\nA principled solution to the problems associated with individual and group level analyses is to use multilevel modelling. This allows us to simultaneously estimate group level parameters (fixed effects) and individual parameters (random effects) as deviations from the group average. This leads to the phenomenon known as shrinkage, which means that individual estimates are ‚Äúpulled‚Äù towards the group average, and are less susceptible to noise in the data. An intuitive way of thinking about this is that we are assuming that participants are all individuals, but they do share some common characteristics, and we can use information obtained from other participants to inform our estimates. This is what partial pooling refers to.\nWhile multilevel models are outside the scope of this course, we will look some basic models in our final session on Baysian data analysis. The basic idea is that each individual participant‚Äôs parameters are treated as random draws from a group-level distribution, and the average effect is then simply the mean of that distribution.\n\nMultilevel models do not have to be Bayesian - the can just as well be estimated using maximum likelihood estimation."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#frequentist-statistics",
    "href": "pages/chapters/11_data-analysis-1.html#frequentist-statistics",
    "title": "Data analysis: I",
    "section": "Frequentist statistics",
    "text": "Frequentist statistics\nThe traditional approach to statistics is know as frequentist statistics. We will first go through the basic principles, then discuss some of the limitations associated with this approach, and then contrast this with the Bayesian approach.\nIn a traditional approach to statistics, students are usually taught a variety of different approaches (far too many to discuss here - there are enough enough different models to fill entire text books). In a nutshell, most of these approachs perform the following steps (I will use an equal-variance independent samples t-test as a running example, as well as the problem of estimating the probability of sucess parameter in a model of Bernoulli trials):\n\nA statistical model is assumed. In the case of the t-test all observations within both groups are assumed to be normally distributed, with the groups differing in the means of the distributions (assuming that the standard deviations of both groups are equal to \\(\\sigma\\)).\n\n\\[\ny_{ij} \\sim \\mathcal{N}(\\mu_j, \\sigma^2)\n\\] \\(y_{ij}\\) refers to the \\(i_{th}\\) observation in group \\(j \\in \\{1, 2\\}\\). This observations are assumed to be independent random variates from a normal distribution (i.i.d)1, with the distribution‚Äôs mean \\(\\mu_j\\) depending on the group. In a t-test, we are interested in the difference between the group means, \\(\\mu_2 - \\mu_1\\).1¬†independent and identically distributed\n\nThe group means are estimated. This can be done, e.g.¬†using maximum likelihood estimation. You will be more familiar with the technique of ‚Äúsimply‚Äù using the sample means to estimate \\(\\mu_j\\), which actually corresponds to the maximum likelihood estimate. The third parameter that has to be estimated is \\(\\sigma\\). This is the parameter estimation step. \\(\\sigma\\) can be estimated using maximum likelihood, or simply ‚Äúcalculated‚Äù as the pooled standard deviation \\(s_p\\). In either case, the parameter is estimated.\nA test statistic is calculated. In this case, the difference \\(\\mu_1 - \\mu_2\\) is of interest, and using this difference, a test statistic is computed. The test statistic can be computed as\n\n\\[\nt = \\frac{\\bar{x_1} - \\bar{x_2}}{s_p \\sqrt{2/n}}\n\\]\nwhere \\(s_p \\sqrt{2/n}\\) is the standard error of the difference. \\(t\\) is assumed to follow a Student-t distribution (under the null hypothesis), and we can compute the probability of observing a test statistic (estimated from the data) that is at least as extreme (depending on the hypothesis) as the observed one (or rather the t value estimated from the actually observed data), under the assumption that the null hypothesis is true. This tail probability is known as a p-value.\nWhat can we discover with NHST?\nIt is important to think about what exactly this approach can tell us. For instance, can we discover how probable the null hypothesis is? Can we ask how probable it is that the null hypothesis is false, and that therefore the alternative is true?\nUnfortunately, the answer to both of these questions is no, we cannot.\nThis approach cannot tell us anything about probabilities of hypotheses - all that we can discover is how likely the data (as summarized by the parameters) are to have occured under the assumption that the null is true. In this case, how likely is the t value, given that there is no difference between the groups, or mathematically, \\(\\mu_1 - \\mu_2 = 0\\). A small p-value can tell us that the data are unlikely, but that is not really the question we are asking.\nWhat we would like to know (intuitively) is how likely are our hypotheses (Wagenmakers 2007). But this is something that frequentist statistics cannot tell us, by design. In the frequentist interpretation, a probability is the relative frequency of occurence of an event. Under this interpretation, a parameter does not have an associated probability distribution, and the question of how probable certain parameter values are is meaningless. In contrast, in Bayesian statistics, a probability is assumed to represent the degree of belief (either subjective or objective) that a certain parameter value is true. As an example, for a frequentist, talking about the probability that it will rain tomorrow is meaningless, because this is not an experiment that can be repeated infinitely many times. For a Bayesian, on the other hand, the probability of rain is merely an expression of belief, a summary of our state of knowledge.\n\nWagenmakers, Eric-Jan. 2007. ‚ÄúA Practical Solution to the Pervasive Problems of p Values.‚Äù Psychonomic Bulletin & Review 14 (5): 779‚Äì804. https://doi.org/10.3758/BF03194105."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#problems-with-nhst",
    "href": "pages/chapters/11_data-analysis-1.html#problems-with-nhst",
    "title": "Data analysis: I",
    "section": "Problems with NHST",
    "text": "Problems with NHST\nEven scientists routinely mistake p-values for probabilities of hypotheses. This is actually one of the most common misconceptions, and lead Wasserstein and Lazar (2016) to write a Statement on Statistical Significance and P-Values in the American Statistical Association. They clarify:\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. ‚ÄúThe ASA Statement on p-Values: Context, Process, and Purpose.‚Äù The American Statistician 70 (2): 129‚Äì33. https://doi.org/10.1080/00031305.2016.1154108.\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n\nA further complication is that the commonly used threshold \\(p = 0.05\\) is completely arbitrary, and based on a combination of ideas by Fisher and Pearson/Neyman. The modern usage actually reflects neither approaches, and is often considered to be a bit of a mess (Amrhein, Greenland, and McShane 2019). Gigerenzer (2004);Gigerenzer (2018) provides an interesting comment on modern usage of hypothesis testing.\n\nAmrhein, Valentin, Sander Greenland, and Blake McShane. 2019. ‚ÄúScientists Rise up Against Statistical Significance.‚Äù Nature 567 (7748, 7748): 305‚Äì7. https://doi.org/10.1038/d41586-019-00857-9.\n\nGigerenzer, Gerd. 2004. ‚ÄúMindless Statistics.‚Äù The Journal of Socio-Economics 33 (5): 587‚Äì606. https://doi.org/10.1016/j.socec.2004.09.033.\n\n‚Äî‚Äî‚Äî. 2018. ‚ÄúStatistical Rituals: The Replication Delusion and How We Got There.‚Äù Advances in Methods and Practices in Psychological Science 1 (2): 198‚Äì218. https://doi.org/10.1177/2515245918771329.\nApart from not being particularly intuitive and not allowing us to adress the questions we would like to answer, NHST also has the problem that incentive structures in scientific publishing can often lead to misapplications of NHST procedures. Various questionable research practices are associated with this, including p-hacking. This refers to the practice of performing multiple significance tests, and selecting only those that yield significant results for reporting. In other words - NHST, if used correclty, can be a very useful tool. A big problem is that it os often not used correctly, and this can lead to misleading results."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#case-studies",
    "href": "pages/chapters/11_data-analysis-1.html#case-studies",
    "title": "Data analysis: I",
    "section": "Case studies",
    "text": "Case studies\nWe will briefly explore the frequentist approach to data analysis using two examples:\n\nan independent samples t-test, and\nthe card game from the previous chapter, and\n\nT-test\nThis example uses simulated data, but is based on an actual study. In the code below, we simulate creativity scores from two groups. One group was instructed to wear a fancy hat, the other, woth no head gear, serves as a control group. We would like to know whether the fancy hat group and the control group differ in their mean creativity.\n\nlibrary(tidyverse)\n\nset.seed(12)\n\n# Number of people wearing fancy hats\nN_fancyhats <- 50 \n\n# Number of people not wearing fancy hats\nN_nofancyhats <- 50\n\n# Population mean of creativity for people wearing fancy hats\nmu_fancyhats <- 103 \n\n# Population mean of creativity for people wearing no fancy hats\nmu_nofancyhats <- 98 \n\n# Average population standard deviation of both groups\nsigma <- 15 \n\n# Generate data\nfancyhats = tibble(Creativity = rnorm(N_fancyhats, mu_fancyhats, sigma),\n               Group = \"Fancy Hat\")\n\nnofancyhats = tibble(Creativity = rnorm(N_nofancyhats, mu_nofancyhats, sigma),\n                 Group = \"No Fancy Hat\")\n\n\nFancyHat <- bind_rows(fancyhats, nofancyhats)  |>\n    mutate(Group = fct_relevel(as.factor(Group), \"No Fancy Hat\"))\n\nWe have created a dataframe called FancyHat.\n\nFancyHat\n\n# A tibble: 100 √ó 2\n   Creativity Group    \n        <dbl> <fct>    \n 1       80.8 Fancy Hat\n 2      127.  Fancy Hat\n 3       88.6 Fancy Hat\n 4       89.2 Fancy Hat\n 5       73.0 Fancy Hat\n 6       98.9 Fancy Hat\n 7       98.3 Fancy Hat\n 8       93.6 Fancy Hat\n 9      101.  Fancy Hat\n10      109.  Fancy Hat\n# ‚Ä¶ with 90 more rows\n\n\nWe can now pretend that we don‚Äôt know how the data were generated, and treat them as data from an experiment.\n\nFancyHat |> \n    ggplot() +\n    geom_boxplot(aes(y = Creativity, x = Group)) +\n    labs(title= \"Box Plot of Creativity Values\") +\n    theme_bw()\n\n\n\n\nAssuming that both groups are normally distributed with the same variance, we can use the t-test to determine whether the mean of the two groups is different.\n\nfancyhat_ttest <- t.test(Creativity ~ Group,\n       var.equal = TRUE,\n       data = FancyHat)\n\n\nfancyhat_ttest_tab <- broom::tidy(fancyhat_ttest)\n\n\nfancyhat_ttest_tab |>\n    select(estimate, estimate1, estimate2, statistic, p.value, conf.low, conf.high) |>\n    round(3)\n\n# A tibble: 1 √ó 7\n  estimate estimate1 estimate2 statistic p.value conf.low conf.high\n     <dbl>     <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1    -1.65      99.2      101.    -0.637   0.526    -6.78      3.49\n\n\nIt might not be obvious at first glance what we have done here:\n\nWe assumed that the data are conditionally normally distributed, given the means, with the same variance.\n\n\\[\ny_{ij} \\sim \\mathcal{N}(\\mu_j, \\sigma^2)\n\\]\n\nwe estimated three parameters: \\(\\mu_1\\), \\(\\mu_2\\) 2, and \\(\\sigma\\).\nWe computed the difference between groups as \\(\\mu_1 - \\mu_2\\). This gives us an estimate of the difference between the means 3.\nWe compute a test statistic (empirical t value)4.\nWe computed the probability of observing the observed difference divided by the standard error, under the null hypothesis that the means are the same (\\(\\mu_1 = \\mu_2\\))5. This is a two-sided test, i.e.¬†we have no hypothesis as to which group has the larger mean.\n\n2¬†estimate1 und estimate23¬†estimate4¬†statistic5¬†p.value\n\n\n\n\n\nTip\n\n\n\nWhat does the p-value tell us? Does it tell you what you want to know? Do you know what the confidence interval tells you 6?6¬†conf.low and conf.high\nThe p-value is 0.53 This is larger that the conventional threshold \\(\\alpha=0.05\\). What does this mean?\n\n\nCard game\nTwo players are playing a game of cards. You observe that they play 9 games, and that player A wins 6 of those. Now you would like to estimate the probability that player A will win the next game. Another way of putting it that you would to estimate the ability of player A to beat player B at this particular game.\nYou know that the probability of success must lie in the range \\([0, 1]\\). What you might not be aware of is that you are assuming a certain probability model, and the probability of success is a parameter of that model. Let‚Äôs take a closer look:\nWe know that the number of \\(k\\) successes in \\(n\\) games follows a binomial distribution with parameters \\(n\\) and \\(\\theta\\). To make things simpler, we also know that each individual game is independent of the others, and the probability of success \\(\\theta\\) is the same for each game. Each success therefore follows a Bernoulli distribution with parameter \\(\\theta\\).\n\\[\ny_i \\sim \\mathcal{Bernoulli}(\\theta)\n\\]\n\nI will generally use the notation \\(y\\) for a variable that is observed, i.e.¬†the data.\n\n\\(y_i\\) is the \\(i\\) th observation in the data, meaning that it tells us whether player A won the game or not on trial \\(i\\). \\(\\theta\\) is the probability of success for each individual game; this is a parameter of our model (\\(\\mathcal{M}\\)).\nPreviously, we used maximum likelihood to estimate \\(\\theta\\) - we will repeat this briefly here.\n\nwins <- 6\ngames <- 9\n\nThe goal is to figure out the ‚Äúbest‚Äù value of \\(\\theta\\), i.e.¬†the value that maximizes the likelihood of observing the data. To do this, we need to consider a range of possible values of \\(\\theta\\) (we already know that this range is \\([0, 1]\\), so that part is easy). We will consider 101 values of \\(\\theta\\) between 0 and 1, and compute the likelihood of observing the data for each value of \\(\\theta\\).\n\nn_points <- 101\ntheta_grid <- seq( from=0 , to=1 , length.out = n_points )\n\nAssuming that both players have an equal chance of winning, the parameter should be \\(\\theta = 0.5\\). The probability of the data given \\(\\theta = 0.5\\) is:\n\ndbinom(x = wins, size = games, prob = 0.5)\n\n[1] 0.1640625\n\n\nThe probability of winning 6 out of 9 games, given that both players are equally likely to win, is 0.1640625.\nWe can also compute the probability of A winning 6, 7, 8 or 9 games, using the cumulative distribution function of the binomial distribution.\n\n1 - pbinom(q = 5, size = games, prob = 0.5)\n\n[1] 0.2539063\n\n\nor\n\npbinom(q = 5, size = games, prob = 0.5, lower.tail = FALSE)\n\n[1] 0.2539063\n\n\n\npbinom() gives us the lower tail probability, which is the probability that the number of successes is less than or equal to the given value, by default.\n\n\n\n\n\n\n\np-value\n\n\n\nDoes this seem familiar?\nIf we want to quantify our null hypothsis that both players are equally likely to win, we would assume that \\(\\theta=0.5\\). Computing the probability of the data under the null is exactly what we have just done. We then plug in the actual data, i.e.¬†6 out of 9, and the upper tail probability is the p-value. In this case, the p-value is approximately \\(0.25\\). Using cut-off of 0.05, we would not reject the null hypothesis, and conclude that there is not enough evidence that player A is better than player B (this is a one-sided test).\n\n\nNow we compute the probability of the data under all parameter values under consideration. In R, this is very simple, since all functions are vectorized.\n\nlikelihood <- dbinom(wins , size = games , prob = theta_grid)\n\n\nplot(likelihood)\n\n\n\n\nWe can see in the above figure that the probability of observing the data is small for a lot of values of \\(\\theta\\). The probability of observing the data, or the likelihood, is maximal for the value 0.6666667:\n\ntheta_grid[which.max(likelihood)]\n\n[1] 0.67\n\n\nWhat we have done so far highlights the distinction between parameter estimation and hypothesis testing. Computing the tail probability under the null (\\(\\theta=0.5\\)) is a hypothesis test, and estimating \\(\\theta\\) is parameter estimation."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#an-introduction-to-bayesian-inference",
    "href": "pages/chapters/11_data-analysis-1.html#an-introduction-to-bayesian-inference",
    "title": "Data analysis: I",
    "section": "An introduction to Bayesian inference",
    "text": "An introduction to Bayesian inference\nSo far, we haven‚Äôt considered any prior knowledge we might have had about which parameters are the most likely a priori. In fact, as we will see a bit further down, we have implicitly assumed that all parameters are equally likely. We will now introduce a new concept: a prior distribution for the parameter(s) we are trying to estimate7.7¬†In frequentist statistics, the concept is meaningless - parameters cannot have distribution. In Bayesian statistics, a prior distribution should reflect everything we know about the parameter, before we consider the data. The prior reflects our belief, which can be subjective, or objective.\nWe will then use that prior belief in order to obtain a posterior belief over the possible parameter values. To do this, we need to multiply the prior probability of each parameter value by the likelihood of the data, i.e.¬†by the probability of observing the data given that parameter value. This is an application of Bayes theorem:\n\\[\np(\\theta|y) = \\frac{ p(y|\\theta) * p(\\theta) } {p(y)}\n\\]\nThis states that the posterior probability of \\(\\theta\\) given the observed data \\(y\\) is equal to the probability of the data, multiplied by how probable each value of \\(\\theta\\) is a priori. You can think of it like this: each parameter value is weighted according to how well it predicts the data. The product \\(p(y|\\theta) * p(\\theta)\\) is then divided by the probability of the data, which in this case is summed over all possible parameter values. This step serves to normalize the posterior, so that it sums to \\(1\\). This essentially turns the unnormalized posterior into a proper probability distribution.\n\\[\np(y) = \\sum_{\\theta}p(y|\\theta) * p(\\theta)\n\\]\nWhen we are interested in estimating the parameters of a given model, we can often neglect the (constant for a model) normalizing term \\(p(y)\\). This term, often called the evidence, reflects the prbability of the data, averaged over all parameter values. Written without the normalizing constant, Bayes rule is often written as:\n\\[\np(\\theta|y) \\propto  p(y|\\theta) * p(\\theta)\n\\]"
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#recap",
    "href": "pages/chapters/11_data-analysis-1.html#recap",
    "title": "Data analysis: I",
    "section": "Recap",
    "text": "Recap\nBayesian inference, in a nutshell, consists of:\n\nRepresent your prior belief by a probability distribution over the possible parameter values. This is a principled way of dealing with uncertainty.\nUse the likelihood to weight the prior belief.\nObtain a posterior belief over the possible parameter values.\n\n\nThe term belief is used synonymously with probability distribution."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#bayesian-inference-in-the-card-game-a-numerical-example",
    "href": "pages/chapters/11_data-analysis-1.html#bayesian-inference-in-the-card-game-a-numerical-example",
    "title": "Data analysis: I",
    "section": "Bayesian inference in the card game: a numerical example",
    "text": "Bayesian inference in the card game: a numerical example\nRecall that we defined a sequence of 101 points between 0 and 1, which represented the possible \\(\\theta\\) values.\n\nn_points <- 101\ntheta_grid <- seq( from=0 , to=1 , length.out = n_points )\n\nFor each of these, we computed the likelihood, that is the probability of observing the (fixed) data, given the parameter. Now, we can make our knowledge about the probability of each parameter value explicit. At first, we will assume that all parameters are equally likely. We will assign the probability of 1 to each parameter value. This is our prior distribution.\n\nprior_1 <- rep(1, length(theta_grid))\n\n\nplot(theta_grid, prior_1, \"type\" = \"l\")\n\n\n\n\nWe could also express the belief that player is at least as good as player B, i.e.¬†they are equally good or A is better than B. One way of doing this is to assign a probability of \\(1\\) to parameter values greater than or equal to \\(0.5\\), and the value \\(0\\) to parameter values less than \\(0.5\\).\n\nprior_2 <- ifelse(theta_grid < 0.5, 0, 1)\n\n\nplot(theta_grid, prior_2, type = \"l\")\n\n\n\n\nA more systematic way of doing this is to use a parameterized probability distribution that expresses our beliefs about the parameter.\n\n\n\n\n\n\nNote\n\n\n\nA family of probability distributions that are suitable for parameters that lie in the interval \\([0,1]\\) is the Beta distribution. This distribution has \\(2\\) parameters \\(\\alpha\\) und \\(\\beta\\), which can be interpreted as the prior number of successes and the number of failures, respectively. The number of trials is therefore \\(\\alpha + \\beta\\). (ref?)(fig:betadists) shows a number of possible Beta distributions for various settings of \\(\\alpha\\) and \\(\\beta\\).\n\nlibrary(tidyverse)\n\nlength <- 1e4\nd <- crossing(shape1 = c(.1, 1:4),\n           shape2 = c(.1, 1:4)) |>\n  tidyr::expand(nesting(shape1, shape2),\n         x = seq(from = 0, to = 1, length.out = length)) |> \n  mutate(a = str_c(\"a = \", shape1),\n         b = str_c(\"b = \", shape2),\n         group = rep(1:length, each = 25))\n\n\nd |> \n  ggplot(aes(x = x, group = group)) +\n  \n  geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)),\n            color = \"steelblue4\", size = 1.1) +\n  scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +\n  coord_cartesian(ylim = c(0, 3)) +\n  labs(title = \"Beta distributions\",\n       y = expression(p(theta*\"|\"*a*\", \"*b))) +\n  theme(panel.grid = element_blank()) +\n  facet_grid(b~a)\n\n\n\n\n\n\nIf we want to use a Beta distribution to express the belief that all values of \\(\\theta\\) are equally likely (uniform prior), we can a Beta distribution with \\(\\alpha = 1\\) and \\(\\beta = 1\\).\n\nprior_3 <- dbeta(x = theta_grid, shape1 = 1, shape2 = 1)\n\n\nplot(theta_grid, prior_3, type = \"l\")\n\n\n\n\nFinally, we could express the following prior information as a Beta distribution: imagine you had previously observed \\(100\\) games between A and B, and each won half of the games. You can set \\(\\alpha\\) and \\(\\beta\\) to the number of games won by A, and the number of games won by B, respectively. A won \\(50\\), and B won \\(50\\):\n\nprior <- dbeta(x = theta_grid, shape1 = 50, shape2 = 50)\n\n\nplot(theta_grid, prior, type = \"l\")\n\n\n\n\nNow, we can combine the prior and the likelihood by multiplying them element-wise, i.e.¬†we need to multiply each parameter value with the probability of the data given that parameter.\n\n\n\n\n\n\nNote\n\n\n\nRecall Bayes theorem:\n\\[\np(\\theta|y) = \\frac{ p(y|\\theta) * p(\\theta) } {p(y)}\n\\]\n\n\nIn R, this is simply:\n\nwins <- 6\ngames <- 9\n\n\nprior <- dbeta(x = theta_grid, shape1 = 4, shape2 = 4)\nlikelihood <- dbinom(wins , size = games , prob = theta_grid)\n\n\nunstandardized_posterior <- likelihood * prior\n\nThis gives us the unnormalized posterior:\n\\[\np(\\theta|y) \\propto  p(y|\\theta) * p(\\theta)\n\\]\nWe can then normalize the posterior distribution by dividing it by \\(p(y) = \\sum_{\\theta}p(y|\\theta) * p(\\theta)\\).\nIn R we can use the sum() function: sum(unstandardized_posterior).\n\nposterior <- unstandardized_posterior / sum(unstandardized_posterior)\n\nWe can now plot the resulting normalized posterior distribution.\n\nplot(theta_grid, posterior, type = \"l\", yaxt = 'n', ylab = 'Probability', \n        main = \"Posterior\", cex.lab = 1.5, cex.main = 3)\n\n\n\n\nTo make this repeatable, we will write two functions. The first, compute_posterior(), will compute the posterior, given the prior and likelihood, and return a dataframe containg prior, likelihood and posterior. The second, plot_posterior(), will plot all three side-by-side. You can also pass in the maximum likelihood estimate, e.g.¬†6/9, and this will be plotted as well.\n\ncompute_posterior = function(likelihood, prior){\n  # compute product of likelihood and prior\n  unstandardized_posterior <- likelihood * prior\n  \n  # standardize the posterior, so it sums to 1\n  posterior <- unstandardized_posterior / sum(unstandardized_posterior)\n  \n  out <- tibble(prior, likelihood, posterior)\n  out\n}\n\n\nplot_posterior <- function(df, mle = 6/9){\nwith(df, {\n    par(mfrow=c(1, 3))\n    plot(theta_grid , prior, type=\"l\", main=\"Prior\", col = \"dodgerblue3\", \n            lwd = 4, yaxt = 'n', ylab = 'Probability', cex.lab = 1.5, cex.main = 3)\n    plot(theta_grid , likelihood, type = \"l\", main = \"Likelihood\", col = \"firebrick3\", \n            lwd = 4, yaxt = 'n', ylab = '', cex.lab = 1.5, cex.main = 3)\n    plot(theta_grid , posterior , type = \"l\", main = \"Posterior\", col = \"darkorchid3\", \n            lwd = 4, yaxt = 'n', ylab = '', cex.lab = 1.5, cex.main = 3)\n    abline(v = mle, col = 4, lty = 2, lwd = 2)\n  } )\n}\n\nYou can now try out various prior distributions, and observe the effect on the posterior.\nLet‚Äôs try out a uniform prior first:\n\nprior <- dbeta(x = theta_grid, shape1 = 1, shape2 = 1)\nlikelihood <- dbinom(wins , size = games , prob = theta_grid)\n\n\ndf <- compute_posterior(likelihood, prior)\n\n\nplot_posterior(df)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this case, the maximum likelihood estimate coincides with the value that maximizes the posterior probability. This is the case because maximum likelihood estimate uses only the likelihood and does not consider prior knowledge. If we are are using a uniform prior, that amounts to saying that all values of \\(\\theta\\) are equally likely. In other words, we have no information as which values are more probable.z\n\n\nNow let‚Äôs try out our prior expressing the belief that player A cannot be worse than player B:\n\nprior <- ifelse(theta_grid < 0.5, 0, 1)\nlikelihood <- dbinom(wins , size = games , prob = theta_grid)\n\n\ndf <- compute_posterior(likelihood, prior)\n\n\nplot_posterior(df)\n\n\n\n\nThe resulting posterior distribution is zero for all values of \\(\\theta\\) that are less than 0.5. This is because our prior allocates zero probability those values.\n\n\n\n\n\n\nTip\n\n\n\nTry out various priors. How do they affect the posterior?\n\nA prior exspressing the belief that B is better than A.\nA prior expressing that belief that either A is a lot better, or B is a lot better.\nA prior expressing the belief that the players are probably as good as each other, and that it is unlikely that either is much better than the other.\n\n\n\nThe posterior represents our belief about the the possible parameter values, after having observed the data. You can therefore think of Bayesian inference as a method for updating your beliefs, conditional on observed data. You are re-allocating probabilities over parameter values, depending on how well those parameter values predicted the data."
  },
  {
    "objectID": "pages/chapters/11_data-analysis-1.html#summarizing-the-posterior",
    "href": "pages/chapters/11_data-analysis-1.html#summarizing-the-posterior",
    "title": "Data analysis: I",
    "section": "Summarizing the posterior",
    "text": "Summarizing the posterior\nWe now need one last step: we need to summarize the posterior distribution. We can do this, for example by computing the mean and standard deviation of the posterior distribution. However, the method we have looked at so far is very simple, and only works for single parameters with a well-defined range. For real-world inference problems, we will numerical methods to approximate the posterior distribution. These methods are know collectively as Monte Carlo sampling, or Markov Chain Monte Carlo (MCMC). Using MCMC, we will not obtain an analytical description of posterior distributions, but instead a collection of random numbers (samples), that are drawn from the posterior distribution. We will use these samples to represent the posterior distribution.\nTo show how this works, we can draw a thousand samples from the posterior distribution. First, we‚Äôll create a posterior.\n\nprior <- dbeta(x = theta_grid, shape1 = 1, shape2 = 1)\nlikelihood <- dbinom(wins, size = games , prob = theta_grid)\n\n\ndf <- compute_posterior(likelihood, prior)\n\n\nplot_posterior(df)\n\n\n\n\nNow we‚Äôll draw 1000 random numbers from the posterior.\n\nn_samples <- 1e3\n\nsamples <- theta_grid |> sample(size = n_samples, replace = TRUE, prob = df$posterior)\n\n\nhead(samples, 10)\n\n [1] 0.33 0.71 0.56 0.84 0.35 0.53 0.72 0.66 0.77 0.72\n\n\nNow we can summarize the samples, e.g.¬†by computing the mean or quantiles.\n\nmean(samples)\n\n[1] 0.63476\n\n\nThe following will give us the median, and a 50% credible interval, i.e.¬†an interval that contains 50% of the mass of the distribution.\n\nquantile(samples, c(0.25, 0.5, 0.75))\n\n 25%  50%  75% \n0.54 0.64 0.74 \n\n\nWe can also use this approach to compute a 95% credible interval.\n\nalpha <- 0.05\n\nquantile(samples, c(alpha/2, 1-alpha/2))\n\n 2.5% 97.5% \n 0.36  0.87 \n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis should not be confused with the 95% confidence interval. Can you remember how a confidence interval is defined? What is the difference between a confidence interval and a credible interval?\n\n\nWhat we have done so far is to look at Bayesian inference for parameter estimation (in a very simple model). Let‚Äôs call this model \\(\\mathcal{M}\\). We have only considered one model at a time, but in the next session we will consider two or models $, which differ in the prior distributions they use. We will then look at methods for comparing models, in order to perform hypothesis testing in a Bayesian framework.\nIn particular, we will look at how Bayesian model comparison can help, in the case that we have no significant results (see Section¬†7.1)."
  },
  {
    "objectID": "pages/chapters/14-Bayesian-models-of-perception.html",
    "href": "pages/chapters/14-Bayesian-models-of-perception.html",
    "title": "Bayesian Models of Perception",
    "section": "",
    "text": "References\n\nVincent, Benjamin T. 2015. ‚ÄúA Tutorial on Bayesian Models of Perception.‚Äù Journal of Mathematical Psychology 66 (June): 103‚Äì14. https://doi.org/10.1016/j.jmp.2015.02.001.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {Bayesian {Models} of {Perception}},\n  date = {2022-05-31},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/14-Bayesian-models-of-perception.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 2022. ‚ÄúBayesian Models of Perception.‚Äù May\n31, 2022. https://kogpsy.github.io/neuroscicomplabFS22//pages/chapters/14-Bayesian-models-of-perception.html."
  },
  {
    "objectID": "slides/01_introduction.html#model-based-cognitive-neuroscience",
    "href": "slides/01_introduction.html#model-based-cognitive-neuroscience",
    "title": "1. Sitzung",
    "section": "(Model-based) Cognitive Neuroscience",
    "text": "(Model-based) Cognitive Neuroscience\n\n\nWas heisst Model-based Neuroscience?\nWelche Kenntnisse brauchen wir, um Experiment durchzuf√ºhren und Daten auszuwerten?\nWelche Programmiertechniken/sprachen brauchen wir?"
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience-beispiel",
    "href": "slides/01_introduction.html#model-based-neuroscience-beispiel",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience: Beispiel",
    "text": "Model-based Neuroscience: Beispiel\nMulder, M. J., Wagenmakers, E.-J., Ratcliff, R., Boekel, W., & Forstmann, B. U. (2012). Bias in the Brain: A Diffusion Model Analysis of Prior Probability and Potential Payoff. Journal of Neuroscience, 32(7), 2335‚Äì2343.\nüëâ https://www.jneurosci.org/content/32/7/2335\nIn dieser Studie geht es darum, den Einfluss von Vorwissen (prior knowledge) auf eine simple perzeptuelle Entscheidung zu untersuchen.\n\nAls Task haben die Autoren ein Random Dot Motion Experiment benutzt.\nF√ºr die Datenanalyse wurde unter anderem ein Diffusion Decision Model verwendet."
  },
  {
    "objectID": "slides/01_introduction.html#diffusion-decision-model",
    "href": "slides/01_introduction.html#diffusion-decision-model",
    "title": "1. Sitzung",
    "section": "Diffusion Decision Model",
    "text": "Diffusion Decision Model"
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience",
    "href": "slides/01_introduction.html#model-based-neuroscience",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience",
    "text": "Model-based Neuroscience\n\n√úberfliegen Sie das Paper, und achten Sie dabei darauf, welche Skills Sie ben√∂tigen, um eine solche Studie durchzuf√ºhren.\n\nWelches theoretische Wissen brauchen Sie?\nWelche Programmierkenntnisse brauchen Sie?\n\nf√ºr das Experiment\nf√ºr die Datenanalyse\n\nWelche statistischen Verfahen brauchen Sie, um die Daten auszuwerten?\nWarum wurde das Experiment im Scanner und ausserhalb des Scanners durchgef√ºhrt?\nWas kann man mit einer solchen Studie herausfinden?"
  },
  {
    "objectID": "slides/01_introduction.html#vorwissen",
    "href": "slides/01_introduction.html#vorwissen",
    "title": "1. Sitzung",
    "section": "Vorwissen",
    "text": "Vorwissen\nEs wurden zwei verschiedene Typen von Vorwissen benutzt.\n\nA-Priori Wahrscheinlichkeit, dass die Punktwolke sich nach rechts oder nach links bewegte.\nAsymmetrische Belohnung f√ºr korrekte links/rechts Entscheidungen."
  },
  {
    "objectID": "slides/01_introduction.html#diffusion-decision-model-1",
    "href": "slides/01_introduction.html#diffusion-decision-model-1",
    "title": "1. Sitzung",
    "section": "Diffusion Decision Model",
    "text": "Diffusion Decision Model"
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience-1",
    "href": "slides/01_introduction.html#model-based-neuroscience-1",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience",
    "text": "Model-based Neuroscience\n\n\nSchematische Darstellung der erwarteten Resultate.\n\nStarting point: korrekte und inkorrekte RTs unterschieden sich.\nDrift rate: korrekte und inkorrekte RTs sind sich √§hnlich.\n\n\n\nTats√§chliche Resultate: Quantifizierung des Bias anhand des DDM."
  },
  {
    "objectID": "slides/01_introduction.html#model-based-neuroscience-2",
    "href": "slides/01_introduction.html#model-based-neuroscience-2",
    "title": "1. Sitzung",
    "section": "Model-based Neuroscience",
    "text": "Model-based Neuroscience\nBOLD Responses der Areale welche besonder stark sowohl auf die ‚Äúprior probability‚Äù als auch auf die ‚Äúpayoff‚Äù Manipulation reagierten.\n\n\n\nright MedFG (right medial frontal gyrus)\nACG (anterior cingulate cortex)\nSFG (superior frontal gyrus)\nleft middle temporal gyrus\nIPS (intra-parietal sulcus).\n\n\n\n\n\nDiese Areale sollen eine besondere Rolle in der Verarbeitung von Bias im Entscheidungsverhalten haben."
  },
  {
    "objectID": "slides/01_introduction.html#wichtige-skills",
    "href": "slides/01_introduction.html#wichtige-skills",
    "title": "1. Sitzung",
    "section": "Wichtige Skills",
    "text": "Wichtige Skills\n\n\n\nTheorien √ºber Entscheidungsverhalten\nExperimente programmieren\n\nTiming (inside/outside scanner)\n\nData cleaning and manipulation (data wrangling)\nStatistische Verfahren f√ºr messwiederholte Daten\n\nPsychometric curve\nBinary choices / Reaktionszeiten\nrepeated-measures ANOVA\n\n\n\n\nGrafische Darstellung der Resultate\nKognitive Prozessmodelle\n\nfit Diffusion Decision Model (DDM)\n\nAuswertung von fRMI Daten\n\n\n\n\nMit diesen Themen (ausser der Analyse von fMRI Daten) besch√§ftigen wir uns in diesem Kurs.\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/02_psychopy.html#bias-rdk-experiment",
    "href": "slides/02_psychopy.html#bias-rdk-experiment",
    "title": "2. Sitzung",
    "section": "Bias RDK Experiment",
    "text": "Bias RDK Experiment\n\n\n\nRandom-dot motion direction-discrimination task\nInside/outside scanner (timing)\nBias: cue (probability left/right/unbiased)\nFixation cross\nRDK: 3x3 pixels, coherence\n40 bias trials, 40 neutral trials\n32 valid, 8 invalid trials"
  },
  {
    "objectID": "slides/02_psychopy.html#psychopy",
    "href": "slides/02_psychopy.html#psychopy",
    "title": "2. Sitzung",
    "section": "PsychoPy",
    "text": "PsychoPy\n\n\n\nPsychoPy Website\nRessourcen\nWalk-through: Builder\nDiskussionsforum\nKapitel: Verhaltensexperiment mit PsychoPy"
  },
  {
    "objectID": "slides/02_psychopy.html#pavlovia",
    "href": "slides/02_psychopy.html#pavlovia",
    "title": "2. Sitzung",
    "section": "Pavlovia",
    "text": "Pavlovia\n\nPavlovia:\n\n\nPavlovia is a place for the wide community of researchers in the behavioural sciences to run, share, and explore experiments online.\n\n\nExperimente suchen.\nZum Beispiel ChoiceRTT ausprobieren und den Code anschauen."
  },
  {
    "objectID": "slides/02_psychopy.html#understanding-your-computer",
    "href": "slides/02_psychopy.html#understanding-your-computer",
    "title": "2. Sitzung",
    "section": "Understanding your Computer",
    "text": "Understanding your Computer\n\nRefresh rate: 60 Hz. Ein Frame dauert 1/60 Sekunde, oder 16.667 ms.\n\nfrom psychopy import visual\n\nwin = visual.Window()\nwin.getActualFrameRate()\n\nKeyboard timing: Variabilit√§t ~15 ms.\nScreen refresh f√§ngt oben an und endet (~10 ms sp√§ter) unten."
  },
  {
    "objectID": "slides/02_psychopy.html#probieren-sie-es-selber",
    "href": "slides/02_psychopy.html#probieren-sie-es-selber",
    "title": "2. Sitzung",
    "section": "Probieren Sie es selber!",
    "text": "Probieren Sie es selber!\n\nVersuchen Sie selber, Teile des Experiments in PsychoPy zu implementieren\n\n\nWenn Sie eine Starthilfe ben√∂tigen, downloaden Sie ein Beipiel: üëâ Practice Trials\nEine Einf√ºhrung finden Sie hier: üëâ Verhaltensexperiment mit PsychoPy\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#set-up-rrstudio",
    "href": "slides/03_import_and_process_data.html#set-up-rrstudio",
    "title": "3. Sitzung",
    "section": "Set up R/RStudio",
    "text": "Set up R/RStudio\n\n\nüëâ Download R\nüëâ Download RStudio\n\n\nRStudio √∂ffnen\nRStudio einrichten\nPackages installieren\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#r-kenntnisse",
    "href": "slides/03_import_and_process_data.html#r-kenntnisse",
    "title": "3. Sitzung",
    "section": "R Kenntnisse",
    "text": "R Kenntnisse\n\n\nüëâ Einf√ºhrung in R\n\nEinf√ºhrung in RStudio\nR Sprache\nControl flow / Funktionen\nDaten importieren / tidy data / visualisieren\nDeskriptive Statistik ]\n\n\nüëâ Data Skills for Reproducible Research\n\nReproducible Workflows\nData visualization\nData wrangling\nIteration & Functions"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#datensatz-importieren",
    "href": "slides/03_import_and_process_data.html#datensatz-importieren",
    "title": "3. Sitzung",
    "section": "Datensatz importieren",
    "text": "Datensatz importieren\nüëâ Download Rstudio Projekt\n\nZZ_rdk-discrimination_2022_Mar_07_1403 aus dem testdata Ordner importieren.\nPractice Trials l√∂schen.\nVariablen ausw√§hlen und umbennen.\n\n    - Trial Index\n    - ID\n    - Cue\n    - Direction\n    - Response / RT\n\nAntworten rekodieren."
  },
  {
    "objectID": "slides/03_import_and_process_data.html#mehrere-datens√§tze-importieren",
    "href": "slides/03_import_and_process_data.html#mehrere-datens√§tze-importieren",
    "title": "3. Sitzung",
    "section": "Mehrere Datens√§tze importieren",
    "text": "Mehrere Datens√§tze importieren\n\n\n\nAlle .csv Files aus dem data Ordner importieren.\nDieselben Schritte wie oben auf alle Datens√§tze anwenden."
  },
  {
    "objectID": "slides/03_import_and_process_data.html#anzahl-korrekter-antworten",
    "href": "slides/03_import_and_process_data.html#anzahl-korrekter-antworten",
    "title": "3. Sitzung",
    "section": "Anzahl korrekter Antworten",
    "text": "Anzahl korrekter Antworten\n\nAccuracy pro Person und pro Bedingung berechnen.\nBedingungen:\n\n   - valid\n   - invalid\n   - neutral"
  },
  {
    "objectID": "slides/03_import_and_process_data.html#probieren-sie-es-selber",
    "href": "slides/03_import_and_process_data.html#probieren-sie-es-selber",
    "title": "3. Sitzung",
    "section": "Probieren Sie es selber!",
    "text": "Probieren Sie es selber!\n\nLearning by doing! Expertise in R erreicht man nur, wenn man selber ausprobiert.\n\n\nKapitel Daten importieren bearbeiten.\nFragen stellen.\nDaten mit esquisse visualisieren:\n\ninstall.packages(\"esquisse\")\nlibrary(esquisse)\nesquisser()\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/04_process_data.html#data-cleaning",
    "href": "slides/04_process_data.html#data-cleaning",
    "title": "4. Sitzung",
    "section": "Data cleaning",
    "text": "Data cleaning\nNun wollen wir versuchen, einzelne Trials, zu identifizieren, in denen Versuchpersonen nicht aufgepasst haben, oder einfach geraten wurde. In solchen F√§llen w√§re die Qualit√§t der Daten nicht gut genug.\n\nAm h√§ufigsten werden die folgenden beiden Kriterien verwendet, um entweder einzelne Datenpunkte, oder Versuchspersonen, auszuschliessen:\n\nVersuchspersonen, deren Accuracy < 50% ist.\nTrials, in denen die Antwort zu schnell oder zu langsam war."
  },
  {
    "objectID": "slides/04_process_data.html#daten-zusammenfassen",
    "href": "slides/04_process_data.html#daten-zusammenfassen",
    "title": "4. Sitzung",
    "section": "Daten zusammenfassen",
    "text": "Daten zusammenfassen\n\nPro VP / Bedingung\n√ºber VPs aggregieren"
  },
  {
    "objectID": "slides/04_process_data.html#probieren-sie-es-selber",
    "href": "slides/04_process_data.html#probieren-sie-es-selber",
    "title": "4. Sitzung",
    "section": "Probieren Sie es selber!",
    "text": "Probieren Sie es selber!\n\nLearning by doing! Expertise in R erreicht man nur, wenn man selber ausprobiert.\n\n\nDaten mit esquisse visualisieren:\n\ninstall.packages(\"esquisse\")\nlibrary(esquisse)\nesquisser()\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/05_signal_detection.html#signal-detection-theory",
    "href": "slides/05_signal_detection.html#signal-detection-theory",
    "title": "5. Sitzung",
    "section": "Signal detection theory",
    "text": "Signal detection theory\n\nWhat happpens when a person must whether or not an event has occured, using information that insufficient to determine the answer?\nSignal detection theory is a widely-used method for analyzing this type of situation and of separating characteristics of the signal from those of the person who is detecting it.\nWe can begin to investigate how information is used to guide a decision."
  },
  {
    "objectID": "slides/05_signal_detection.html#signal-detection-theory-1",
    "href": "slides/05_signal_detection.html#signal-detection-theory-1",
    "title": "5. Sitzung",
    "section": "Signal detection theory",
    "text": "Signal detection theory\n\n\nWhat is a decision?\n\nA doctor is examining a patient and trying to make a diagnosis.\nA witness to a crime is asked to identify a suspect. Was this person present at the time of the crime or not?\nPurchase decision, gambling decisions, perceptual decisions, etc.\n\n\nBinary choice\n\n\n\n\nSignal\n\n\n\n\n\nResponse\nYes\nNo\n\n\n‚Äî‚Äî‚Äì\n‚Äî‚Äî‚Äî‚Äî‚Äì\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n\n\nYes\nHit\nFalse alarm (FA)\n\n\nNo\nMiss\nCorrect rejection (CR)"
  },
  {
    "objectID": "slides/05_signal_detection.html#signal-detection-theory-2",
    "href": "slides/05_signal_detection.html#signal-detection-theory-2",
    "title": "5. Sitzung",
    "section": "Signal detection theory",
    "text": "Signal detection theory\n\nAssumptions of SDTGraphical representation\n\n\nThe key assumptions of SDT\n\nSeparate representation and decision-making.\nSignal and noise trials can be represented as values along a uni-dimensional ‚Äústrength‚Äù construct.\nBoth types trials produce Gaussian random variables that represent strength.\nBoth variances are assumed to be the same.\nThe decision-making assumption of is that yes and no responses are produced by comparing the strength of the current trial to a fixed criterion. If the strength exceeds the criterion a ‚Äúyes‚Äù response is made, otherwise a ‚Äúno‚Äù response is made. ]\n\n\n\n\n\n\n\n\nEqual-variance Signal Detection Model"
  },
  {
    "objectID": "slides/05_signal_detection.html#c-and-d",
    "href": "slides/05_signal_detection.html#c-and-d",
    "title": "5. Sitzung",
    "section": "c and d'",
    "text": "c and d'\n\n\n\n\n\n\n\nThe mean of the signal distribution is d'. This makes d' a measure of the discriminability of the signal trials from the noise trials ‚Äì it corresponds to the distance between the two distributions.\nThe strength value d/2 is special, because it is the criterion value that maximizes the probability of a correct classification when signal and noise trials are equally likely to occur.\nc a measure of bias, because it corresponds to how different the actual criterion is from the unbiased one. Positive values of c correspond to a bias towards saying no, and so to an increase in correct rejections at the expense of an increase in misses. Negative values of c correspond to a bias towards saying yes, and so to an increase in hits at the expense of an increase in false alarms."
  },
  {
    "objectID": "slides/05_signal_detection.html#computing-sdt-measures-in-r",
    "href": "slides/05_signal_detection.html#computing-sdt-measures-in-r",
    "title": "5. Sitzung",
    "section": "Computing SDT Measures in R",
    "text": "Computing SDT Measures in R\n\nDistribution function: codeDistribution function: figure\n\n\n\nlibrary(tidyverse)\ntibble(x = seq(-3, 3, by = 0.1)) %>% \n  ggplot(aes(x)) +\n  stat_function(fun = pnorm, colour = \"steelblue3\", \n                  args = list(mean = 0, sd = 1),\n                  size = 1.5) +\n  labs(y = \"Probability\", x = \"Z Score\") +\n  scale_y_continuous(limits = c(0, 1)) +\n  scale_x_continuous(limits = c(-3.5, 3.5), breaks = -3:3) + \n  ggtitle(\"Cumulative distribution function / pnorm()\") +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nCumulative distribution function: pnorm()"
  },
  {
    "objectID": "slides/05_signal_detection.html#using-signal-detection-to-analyze-data-in-r",
    "href": "slides/05_signal_detection.html#using-signal-detection-to-analyze-data-in-r",
    "title": "5. Sitzung",
    "section": "Using Signal Detection to Analyze Data in R",
    "text": "Using Signal Detection to Analyze Data in R\nWe will look at an example using data from a recognition memory test.\n\nSubjects first learn words (study phase).\nIn the test phase, they are presented with words that were on the list, and with new words.\nSubjects have to classify words into\n\nold (yes, this was on the list)\nnew (no, this wasn‚Äôt on the list)\n\nThese kind of data are usually analyzed using a Signal Detection model.\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/06_signal_detection_examples.html#visual-motion",
    "href": "slides/06_signal_detection_examples.html#visual-motion",
    "title": "6. Sitzung",
    "section": "Visual Motion",
    "text": "Visual Motion\n\nDecision variable: Lateral Intraparietal Cortex (LIP)\n\n\n\n\n\n\n\n\n\n\nShadlen, M. N., & Kiani, R. (2013). Decision Making as a Window on Cognition. Neuron, 80(3), 791‚Äì806. https://doi.org/10.1016/j.neuron.2013.10.047"
  },
  {
    "objectID": "slides/06_signal_detection_examples.html#cognitive-fatigue",
    "href": "slides/06_signal_detection_examples.html#cognitive-fatigue",
    "title": "6. Sitzung",
    "section": "Cognitive Fatigue",
    "text": "Cognitive Fatigue\n\n\nWhen we are fatigued, we feel that our performance is worse than when we are fresh.\nThe metrics of signal detection theory (SDT)‚Äîresponse bias (criterion) and perceptual certainty (d‚Äô) may change as a function of fatigue, but no work has yet been done to examine whether these metrics covary with fatigue.\nWe induced fatigue through repetitive performance of the n-back working memory task, while functional magnetic resonance imaging (fMRI) data was acquired.\nOur results show that both criterion and d‚Äô were correlated with changes in cognitive fatigue: as fatigue increased, subjects became more conservative in their response bias and their perceptual certainty declined. Furthermore, activation in the striatum of the basal ganglia was also related to cognitive fatigue, criterion, and d‚Äô.\nThe following behavioral data were analyzed: overall accuracy, the reaction times (RTs) of the correct trials, and signal detection metrics.\nParticipants were presented with a visual analog scale (VAS) before and after each block. Participants were asked: ‚ÄúHow mentally fatigued are you right now?‚Äù\n\nWylie, G. R., Yao, B., Sandry, J., & DeLuca, J. (2020). Using Signal Detection Theory to Better Understand Cognitive Fatigue. Frontiers in Psychology, 11, 579188. https://doi.org/10.3389/fpsyg.2020.579188"
  },
  {
    "objectID": "slides/06_signal_detection_examples.html#cognitive-fatigue-1",
    "href": "slides/06_signal_detection_examples.html#cognitive-fatigue-1",
    "title": "6. Sitzung",
    "section": "Cognitive Fatigue",
    "text": "Cognitive Fatigue\n\n\n\n\n\n\n\n\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/07_response_times.html#what-are-response-times",
    "href": "slides/07_response_times.html#what-are-response-times",
    "title": "7. Sitzung",
    "section": "What are Response Times?",
    "text": "What are Response Times?\n\n\n\n\nTypical distribution of response times.\n\nTime taken to complete a task.\nTypically, some observed RTs are not a result of the process of interest,but contaminants. Luce (1986) demonstrated that genuine RTs have a minimum value of \\(> 100\\) ms: the time needed for physiological processes (stimulus perception, motor responses).\n\n\n\nWhelan, R. (2008). Effective Analysis of Reaction Time Data. The Psychological Record, 58(3), 475‚Äì482. https://doi.org/10.1007/BF03395630"
  },
  {
    "objectID": "slides/07_response_times.html#tadpoles",
    "href": "slides/07_response_times.html#tadpoles",
    "title": "7. Sitzung",
    "section": "Tadpoles",
    "text": "Tadpoles\n\n\n\n\n\n\n\n\nRoberts, A., Borisyuk, R., Buhl, E., Ferrario, A., Koutsikou, S., Li, W.-C., & Soffe, S. R. (2019). The decision to move: Response times, neuronal circuits and sensory memory in a simple vertebrate. Proceedings of the Royal Society B: Biological Sciences, 286(1899), 20190297. https://doi.org/10.1098/rspb.2019.0297"
  },
  {
    "objectID": "slides/07_response_times.html#adhd",
    "href": "slides/07_response_times.html#adhd",
    "title": "7. Sitzung",
    "section": "ADHD",
    "text": "ADHD\n\n\n\n\nPerformance variability is the essence of ADHD.\nChildren with ADHD demonstrate greater RT variability than nonclinical controls on speeded RT tasks.\n\n\nResponse variability is related to a distributed brain network including frontal lobes (implicated in the pathophysiology of ADHD).\n\n\nTraditional RT measures (sample mean and standard deviation) showed that children with ADHD were slower and more variable in responding than controls.\n\n\n\n\n\nChildren with ADHD had a greater number of RTs beyond their mean performance than the control group. Were not generally slower but were prone to attentional lapses.\n\n\n\n\n\n\nHervey, A. S., Epstein, J. N., Curry, J. F., Tonev, S., Eugene Arnold, L., Keith Conners, C., Hinshaw, S. P., Swanson, J. M., & Hechtman, L. (2006). Reaction Time Distribution Analysis of Neuropsychological Performance in an ADHD Sample. Child Neuropsychology, 12(2), 125‚Äì140. https://doi.org/10.1080/09297040500499081"
  },
  {
    "objectID": "slides/07_response_times.html#central-tendency-mean",
    "href": "slides/07_response_times.html#central-tendency-mean",
    "title": "7. Sitzung",
    "section": "Central tendency: Mean",
    "text": "Central tendency: Mean\n\nMost common method of analyzing RT data is to report a central tendency parameter (e.g., the mean or median) and a dispersion parameter (e.g., the standard deviation). The mean difference in RT across conditions is then often analyzed by using ANOVA.\nUsing hypothesis tests on data that are skewed, contain outliers, are heteroscedastic, or have a combination of these characteristics (raw RT data typically have at least the first two) reduces the power of these tests and can result in a failure to detect a real difference between conditions (Wilcox, 1998).\nNeither the mean nor standard deviation are said to be robust measures. That is, the mean is not reflective of the typical response if the distribution is skewed, because the mean is distorted in the direction of the skew."
  },
  {
    "objectID": "slides/07_response_times.html#central-tendency-median",
    "href": "slides/07_response_times.html#central-tendency-median",
    "title": "7. Sitzung",
    "section": "Central tendency: Median",
    "text": "Central tendency: Median\n\nMany researchers report the median RT as a central tendency parameter, because it is less susceptible to departures from normality (i.e., robust). The interquartile range (IQR, the range between the third and first quartiles) is a robust method of estimating the dispersion.\nA difficulty with using the median is that unlike the sample mean, it is a biased estimator of the population median when the population is skewed (a biased estimator does not, on average, equal the value of the parameter or function that it estimates).\nThe bias becomes more extreme as the sample size becomes smaller (Miller, 1988) and thus the median is more likely to be overestimated in the condition with fewer trials."
  },
  {
    "objectID": "slides/07_response_times.html#beyond-mean-differences",
    "href": "slides/07_response_times.html#beyond-mean-differences",
    "title": "7. Sitzung",
    "section": "Beyond mean differences",
    "text": "Beyond mean differences\n\n\n\nMost analyses of RT data are conducted by using the statistical techniques with which psychologists are most familiar, such as analysis of variance (ANOVA) on the sample mean. Unfortunately, these methods are usually inappropriate for RT data, because they have little power to detect genuine differences in RT between conditions.\n\n\n\n\n\n\nRousselet, G. A., Pernet, C. R., & Wilcox, R. R. (2017). Beyond differences in means: Robust graphical methods to compare two groups in neuroscience. European Journal of Neuroscience, 46(2), 1738‚Äì1748. [https://doi.org/10.1111/ejn.13610](https://doi.org/10.1111/"
  },
  {
    "objectID": "slides/07_response_times.html#beyond-mean-differences-1",
    "href": "slides/07_response_times.html#beyond-mean-differences-1",
    "title": "7. Sitzung",
    "section": "Beyond mean differences",
    "text": "Beyond mean differences\n\n\nDeciles\n\n\nShift function\n\n\n\n\nFor each decile, the shift function illustrates by how much one distribution needs to be shifted to match another one.\nIn our example, we illustrate by how much we need to shift deciles from group 2 to match deciles from group 1."
  },
  {
    "objectID": "slides/07_response_times.html#beyond-mean-differences-2",
    "href": "slides/07_response_times.html#beyond-mean-differences-2",
    "title": "7. Sitzung",
    "section": "Beyond mean differences",
    "text": "Beyond mean differences\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/08_response_times_ii.html#next-sessions",
    "href": "slides/08_response_times_ii.html#next-sessions",
    "title": "8. Sitzung",
    "section": "Next sessions",
    "text": "Next sessions\n\nEvidence accumulation\nBayesian data analysis\nIs the brain Bayesian?"
  },
  {
    "objectID": "slides/08_response_times_ii.html#adhd-response-time-variability",
    "href": "slides/08_response_times_ii.html#adhd-response-time-variability",
    "title": "8. Sitzung",
    "section": "ADHD: Response Time Variability",
    "text": "ADHD: Response Time Variability\n\nChildren with ADHD show cognitive impairments in attention, inhibitory control, and working memory compared with typically-developing children.\nNo specific cognitive impairment is universal across patients with ADHD. One of the more consistent findings in the ADHD neuropsychology literature is increased reaction time variability (RTV), also known as intra-individual variability, on computerized tasks.\nChildren with ADHD have also been shown to be less able to deactivate the DMN than controls, and this inability to deactivate the DMN has been directly related to RTV.\nDoes RTV reflect a single construct or process? The vast majority of studies have used a reaction time standard deviation (RTSD)."
  },
  {
    "objectID": "slides/08_response_times_ii.html#facilitation",
    "href": "slides/08_response_times_ii.html#facilitation",
    "title": "8. Sitzung",
    "section": "Facilitation",
    "text": "Facilitation\n\nStroop task vs Simon task\nFacilitation effects in both task.\nStroop effects are smallest for fast responses and increase as responses slow.\nSimon effects are largest for fast responses but decrease, and even reverse, as responses slow (see DEMO).\n\nPratte, M. S., Rouder, J. N., Morey, R. D., & Feng, C. (2010). Exploring the differences in distributional properties between Stroop and Simon effects using delta plots. Attention, Perception, & Psychophysics, 72(7), 2013‚Äì2025. https://doi.org/10.3758/APP.72.7.2013"
  },
  {
    "objectID": "slides/08_response_times_ii.html#experimental-manipulations",
    "href": "slides/08_response_times_ii.html#experimental-manipulations",
    "title": "8. Sitzung",
    "section": "Experimental manipulations",
    "text": "Experimental manipulations\n\n\n\naffects most strongly slow behavioural responses, but with limited effects on fast responses.\naffects all responses, fast and slow, similarly.\nhas stronger effects on fast responses, and weaker ones for slow responses.\n\n\n\n\n\nDistribution analysis provides much stronger constraints on the underlying cognitive architecture than comparisons limited to e.g.¬†mean or median reaction times across participants."
  },
  {
    "objectID": "slides/08_response_times_ii.html#shift-function-independent-groups",
    "href": "slides/08_response_times_ii.html#shift-function-independent-groups",
    "title": "8. Sitzung",
    "section": "Shift Function: Independent Groups",
    "text": "Shift Function: Independent Groups\n\nUniform shiftShift functionPlot shift function\n\n\n\n\n\n\n\n\n\n\n\nout <- shifthd_pbci(df, nboot = 200, adj_ci = FALSE)\np <- plot_sf(out, plot_theme = 1)[[1]] + \n     theme(axis.text = element_text(size = 16, colour=\"black\"))"
  },
  {
    "objectID": "slides/08_response_times_ii.html#hierarchical-shift-function",
    "href": "slides/08_response_times_ii.html#hierarchical-shift-function",
    "title": "8. Sitzung",
    "section": "Hierarchical Shift Function",
    "text": "Hierarchical Shift Function\n\nquantiles are computed for the distribution of measurements from each condition and each participant.\nthe quantiles are subtracted in each participant.\na trimmed mean is computed across participants for each quantile.\nConfidence intervals are computed using the percentile bootstrap."
  },
  {
    "objectID": "slides/08_response_times_ii.html#hierarchical-shift-function-1",
    "href": "slides/08_response_times_ii.html#hierarchical-shift-function-1",
    "title": "8. Sitzung",
    "section": "Hierarchical Shift Function",
    "text": "Hierarchical Shift Function\n\n\n\n\nShift functionPlot shift functionStochastic dominance\n\n\n\n\nout <- hsf(df, rt ~ cond + id)\n\n\n\n\n\n\np <- plot_hsf(out)\np\n\n\n\n\n\n\n\nParticipants with all quantile differences > 0\n\nnq <- length(out$quantiles)\npdmt0 <- apply(out$individual_sf > 0, 2, sum)\nprint(paste0('In ',sum(pdmt0 == nq),' participants (',round(100 * sum(pdmt0 == nq) / np, digits = 1),'%), all quantile differences are more than to zero'))\n\n[1] \"In 0 participants (0%), all quantile differences are more than to zero\"\n\n\nParticipants with all quantile differences < 0\n\npdlt0 <- apply(out$individual_sf < 0, 2, sum)\nprint(paste0('In ',sum(pdlt0 == nq),' participants (',round(100 * sum(pdlt0 == nq) / np, digits = 1),'%), all quantile differences are less than to zero'))\n\n[1] \"In 22 participants (73.3%), all quantile differences are less than to zero\""
  },
  {
    "objectID": "slides/08_response_times_ii.html#hierarchical-shift-function-2",
    "href": "slides/08_response_times_ii.html#hierarchical-shift-function-2",
    "title": "8. Sitzung",
    "section": "Hierarchical Shift Function",
    "text": "Hierarchical Shift Function\nAlternative bootstrapping method: hsf_pb()\n\nset.seed(8899)\nout <- rogme::hsf_pb(df, rt ~ cond + id)\n\n\nrogme::plot_hsf_pb(out, interv = \"ci\")"
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping",
    "href": "slides/08_response_times_ii.html#bootstrapping",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n‚ÄúThe bootstrap is a computer-based method for assigning measures of accuracy to statistical estimates.‚Äù Efron & Tibshirani, An introduction to the bootstrap, 1993\n\n\n‚ÄúThe central idea is that it may sometimes be better to draw conclusions about the characteristics of a population strictly from the sample at hand, rather than by making perhaps unrealistic assumptions about the population.‚Äù Mooney & Duval, Bootstrapping, 1993\n\n\nLike all bootstrap methods, the percentile bootstrap relies on a simple & intuitive idea: instead of making assumptions about the underlying distributions from which our observations could have been sampled, we use the data themselves to estimate sampling distributions.\nIn turn, we can use these estimated sampling distributions to compute confidence intervals, estimate standard errors, estimate bias, and test hypotheses (Efron & Tibshirani, 1993; Mooney & Duval, 1993; Wilcox, 2012).\nThe core principle to estimate sampling distributions is resampling. The technique was developed & popularised by Brad Efron as the bootstrap."
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping-1",
    "href": "slides/08_response_times_ii.html#bootstrapping-1",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nheights <- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185)\n\n\nv1 <- sample(heights, replace = TRUE)\nv1\n\n [1] 188 183 188 183 182 177 188 188 188 192\n\n\n\nv2 <- sample(heights, replace = TRUE)\nv2\n\n [1] 192 183 185 185 177 185 185 185 182 192\n\n\nEssentially, we are doing fake experiments using only the observations from our sample. And for each of these fake experiments, or bootstrap sample, we can compute any estimate of interest, for instance the median.\nFor a visualization, see this demo."
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping-2",
    "href": "slides/08_response_times_ii.html#bootstrapping-2",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nFunction to resample and compute meanRepeat N timesVisualize\n\n\n\nf <- function(v) {\n  vv <- sample(v, replace = TRUE)\n  mean(vv)\n}\n\n\n\n\nN <- 4000\n\n\nb1 <- replicate(n = N, f(heights))\n\n\nlibrary(purrr)\nb2 <- map_dbl(1:N, ~f(heights))\n\n\nd <- tibble(b1 = b1, b2 = b2) |> \n  pivot_longer(everything(), names_to = \"bootstrap\", values_to = \"mean\")\n\n\n\n\n\nd |> \n  ggplot(aes(mean, fill = bootstrap)) + \n  geom_histogram() +\n  facet_wrap(~bootstrap)"
  },
  {
    "objectID": "slides/08_response_times_ii.html#bootstrapping-3",
    "href": "slides/08_response_times_ii.html#bootstrapping-3",
    "title": "8. Sitzung",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapped standard error of the mean:\n\nMethod 1Method 2\n\n\n\nd |> \n  group_by(bootstrap) |> \n  tidybayes::mean_qi()\n\n# A tibble: 2 √ó 7\n  bootstrap  mean .lower .upper .width .point .interval\n  <chr>     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 b1         185.    182   187    0.95 mean   qi       \n2 b2         185.    182   187.   0.95 mean   qi       \n\n\n\n\n\nd |> \n  group_by(bootstrap) |> \n  summarize(estimate = mean(mean), \n            lower = quantile(mean, probs = 0.025),\n            upper = quantile(mean, probs = 0.975))\n\n# A tibble: 2 √ó 4\n  bootstrap estimate lower upper\n  <chr>        <dbl> <dbl> <dbl>\n1 b1            185.   182  187 \n2 b2            185.   182  187.\n\n\n\n\n\n\n\n\nüè† Neurowissenschaft im Computerlab FS22"
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#decision",
    "href": "slides/09_evidence_accumulation.html#decision",
    "title": "9. Sitzung",
    "section": "Decision",
    "text": "Decision\n\nJeden Tag treffen wir Tausende von kleinen Entscheidungen (unter Zeitdruck).\nViele davon sind trivial (z. B. welches Paar Socken man anziehen oder welche Fernsehserie man schauen soll) und automatisch (z. B. wie man den Kollegen morgens begr√ºsst oder welches Wort man als n√§chstes in eine E-Mail schreiben soll).\nNach einiger √úberlegung muss eine Entscheidung auf der Grundlage der Daten getroffen werden.\nDie meisten Entscheidungen im wirklichen Leben setzen sich aus zwei separaten Entscheidungen zusammen: zuerst die Entscheidung, mit dem √úberlegen aufzuh√∂ren und zu handeln, und dann die Entscheidung oder Handlung selbst."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#sequential-sampling",
    "href": "slides/09_evidence_accumulation.html#sequential-sampling",
    "title": "9. Sitzung",
    "section": "Sequential Sampling",
    "text": "Sequential Sampling\n\nStatistik: Daten (evidence) werden im Laufe der Zeit gesammelt, und der Statistiker muss entscheiden, wann er die Datenerfassung beenden und eine Entscheidung treffen muss.\nDer sequentielle Charakter der Entscheidungsfindung ist eine grundlegende Eigenschaft des menschlichen Nervensystems und spiegelt seine Unf√§higkeit wider, Informationen sofort zu verarbeiten..\nUm die Dynamik der Entscheidungsfindung zu verstehen, konzentrieren sich die meisten Studien auf einfache, wiederholbare Wahlprobleme mit nur zwei (bin√§ren) Alternativen."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#gesetzm√§ssigkeiten",
    "href": "slides/09_evidence_accumulation.html#gesetzm√§ssigkeiten",
    "title": "9. Sitzung",
    "section": "Gesetzm√§ssigkeiten",
    "text": "Gesetzm√§ssigkeiten\n\nDaten von elementaren Entscheidungsaufgaben lassen mehrere gesetzes√§hnliche Muster erkennen, die jedes Modell der Entscheidungsfindung ber√ºcksichtigen muss.\nDie mittlere RT ist f√ºr einfache Stimuli k√ºrzer als f√ºr schwierige Reize..\n‚ÄúSpeed stress‚Äù verk√ºrzt die mittlere RT, erh√∂ht aber den Anteil der Fehlerrate.\nMittlere RT ist proportional yur Standardabweichung.\nManipulationen, die die Geschwindigkeit der richtigen Antworten erh√∂hen, erh√∂hen auch die Geschwindigkeit der Fehlerantworten.\nRT-Verteilungen sind rechtsschief, und diese Schiefe nimmt mit der Schwierigkeit der Aufgabe zu.\nBei schwierigen Aufgaben ist die mittlere Fehler-RT oft langsamer als die mittlere korrekte RT - dieses Muster kann durch ‚ÄúGeschwindigkeitsstress‚Äùspeed stress‚Äù umgekehrt werden."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#diffusion-decision-model",
    "href": "slides/09_evidence_accumulation.html#diffusion-decision-model",
    "title": "9. Sitzung",
    "section": "Diffusion Decision Model",
    "text": "Diffusion Decision Model\n\nDas von Roger Ratcliff entwickelte Modell hat seinen Ursprung in Modellen zu den Bewegungen von Partikeln in einer Fl√ºssigkeit, und geht auf Arbeiten von Albert Einstein und Norbert Wiener zur√ºck.\nBin√§re Entscheidungen basieren auf der Anh√§ufung von verrauschten Beweisen, beginnend am Ausgangspunkt und endend an einer Entscheidungsschwelle, die mit einer bestimmten Entscheidung verbunden ist."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#ddm-parameter",
    "href": "slides/09_evidence_accumulation.html#ddm-parameter",
    "title": "9. Sitzung",
    "section": "DDM Parameter",
    "text": "DDM Parameter\nDas Modell hat vier Parameter. 1\n\nDrift rate steht f√ºr die durchschnittliche Anzahl von Beweisen pro Zeiteinheit und ist ein Index f√ºr die Schwierigkeit der Aufgabe oder die F√§higkeit des Subjekts.\nBoundary separation stellt die Vorsicht dar; eine gr√∂√üere Trennung der Grenzen f√ºhrt zu weniger Fehlern (wegen geringerer Auswirkung des Diffusionsrauschens innerhalb des Trials), jedoch um den Preis einer langsameren Reaktion (speed-accuracy tradeoff).\nStarting point repr√§sentiert die a-priori Pr√§ferenz f√ºr eine der Wahlalternativen.\nNon-decision time ist ein Verz√∂gerungsparameter, der die Zeit f√ºr periphere Prozesse (Kodierung eines Reizes, Umwandlung der Repr√§sentation des Reizes in eine entscheidungsbezogene Repr√§sentation) und Ausf√ºhrung einer Reaktion misst.\n\n\nGesamtzeit f√ºr eine Reaktion ist die Zeit f√ºr die Ausbreitung vom Startpunkt bis zur Grenze plus die Non-decision time\n\nHier schauen wir uns eine vereinfachte Version dieses Modells an, welches nur 4 Parameter. Im ‚Äústandard‚Äù DDM hat es 7 Parameter."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#beispiele",
    "href": "slides/09_evidence_accumulation.html#beispiele",
    "title": "9. Sitzung",
    "section": "Beispiele",
    "text": "Beispiele\n\nAltern: √§ltere Erwachsene sind oft langsamer als j√ºngere, machen aber nicht mehr Fehler. DDM Resultate haben gezeigt, dass es nicht immer kognitive F√§higkeiten sind, welche mit dem alter abnehmen, sondern oftmals periphere Prozesse und gr√∂ssere Vorsicht. Dies konnte in Studien zu numerosity judgments, lexical decisions und recognition memory gezeigt werden.\nArbeitsged√§chtnis und IQ: ein h√∂herer IQ geht mit einer gr√∂sserern drift rate einher.\nKlinische Studien: Patienten mit Angstst√∂rungen haben eine h√∂here drift rate f√ºr bedrohliche W√∂rter/Bilder mit bedrohlichem Inhalt.\n\nAnhand von DDM k√∂nnen Neuowissenschaftler Hirnmessungen mit kognitiven Prozessen assoziieren, anstelle von behavioralen ‚ÄúEffekten‚Äù."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#ddm-annahmen",
    "href": "slides/09_evidence_accumulation.html#ddm-annahmen",
    "title": "9. Sitzung",
    "section": "DDM Annahmen",
    "text": "DDM Annahmen\n\nBinary decision making: DDM ist ein Model f√ºr bin√§re Entscheidungen.\n\n\nStroop-Aufgabe. Obwohl es mehrere Antworten gibt (eine f√ºr jede Farbe), k√∂nnte man versuchen, die Genauigkeitsdaten mit dem Diffusionsmodell zu modellieren und dabei unterschiedliche Driftraten f√ºr kongruente und inkongruente Versuche zu ber√ºcksichtigen.\n\n\nContinuous sampling: Entscheidungen beruhen auf einem kontinuierlichen Verarbeitung von Daten.\nSingle-stage processing: Entscheidungen basieren auf einer einstufigen Verarbeitung.\nParameter sind konstant. Das heisst z.B. drift rate kann sich nicht √ºber Zeit ver√§ndern."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#fitting-parameters",
    "href": "slides/09_evidence_accumulation.html#fitting-parameters",
    "title": "9. Sitzung",
    "section": "Fitting Parameters",
    "text": "Fitting Parameters\n\nWir simulieren nun einen Evidenz Accumulation Process Schritt-f√ºr-Schritt. Damit k√∂nnen wir Zufallszahlen generieren.\nUm Parameter Fitting zu machen, brauchen wir jedoch die Wahrscheinlichkeitsdichte, d.h. die Wahrscheinlichkeit, die gegebene Grenze in der gegebenen Zeit zu erreichen (gegeben die Parameter).\nDies machen wir in der n√§chsten Sitzung."
  },
  {
    "objectID": "slides/09_evidence_accumulation.html#literature",
    "href": "slides/09_evidence_accumulation.html#literature",
    "title": "9. Sitzung",
    "section": "Literature",
    "text": "Literature\n\nVoss, A., Nagler, M., & Lerche, V. (2013). Diffusion Models in Experimental Psychology: A Practical Introduction. Experimental Psychology, 60(6), 385‚Äì402. https://doi.org/10.1027/1618-3169/a000218\nForstmann, B. U., Ratcliff, R., & Wagenmakers, E.-J. (2016). Sequential Sampling Models in Cognitive Neuroscience: Advantages, Applications, and Extensions. Annual Review of Psychology, 67, 641‚Äì666. https://doi.org/10.1146/annurev-psych-122414-033645\nRatcliff, R., & McKoon, G. (2008). The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks. Neural Computation, 20(4), 873‚Äì922. https://doi.org/10.1162/neco.2008.12-06-420\n\n\n\n\nüè† Neurowissenschaft Computerlab FS22"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#simulating-rt-and-choice-data",
    "href": "slides/10_fitting_ddm.html#simulating-rt-and-choice-data",
    "title": "10. Sitzung",
    "section": "Simulating RT and choice data",
    "text": "Simulating RT and choice data\nLoad library\n\n\nShow code\nlibrary(rtdists)"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#parameters",
    "href": "slides/10_fitting_ddm.html#parameters",
    "title": "10. Sitzung",
    "section": "Parameters",
    "text": "Parameters\nMit rdiffusion() k√∂nnen wir ein Experiment simulieren, bei dem die Fehler im Schnitt schneller als die korrekten Antworten sind, indem wir eine A Priori Pr√§ferenz f√ºr die Untergrenze definieren (z = 0.2).\nDie 5 wichtigsten Argumente der Funktion sind:\nn: Anzahl Zufallszahlen\na: boundary separation\nv: drift rate\nt0: non-decision time\nz: bias"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#simulate",
    "href": "slides/10_fitting_ddm.html#simulate",
    "title": "10. Sitzung",
    "section": "Simulate",
    "text": "Simulate\n\n\nShow code\nrts <- rdiffusion(500, a = 1, v = 2, t0 = 0.5, z = 0.2)\n\nglimpse(rts)\n\n\nRows: 500\nColumns: 2\n$ rt       <dbl> 0.5987220, 0.5237413, 0.5069358, 0.7882125, 0.7826192, 0.6223‚Ä¶\n$ response <fct> lower, lower, lower, upper, upper, lower, lower, upper, upper‚Ä¶\n\n\n\n\nShow code\nhead(rts)\n\n\n         rt response\n1 0.5987220    lower\n2 0.5237413    lower\n3 0.5069358    lower\n4 0.7882125    upper\n5 0.7826192    upper\n6 0.6223894    lower"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#grafisch-darstellen",
    "href": "slides/10_fitting_ddm.html#grafisch-darstellen",
    "title": "10. Sitzung",
    "section": "Grafisch darstellen",
    "text": "Grafisch darstellen\n\n\nShow code\nrts |> \n  ggplot(aes(rt, response, fill = response)) +\n  geom_violin() +\n  geom_jitter(height = 0.1, alpha = 0.5) +\n  scale_fill_viridis_d(option = \"B\", direction = -1, \n                       begin = 1/3, end = 3/3) +\n  xlim(c(0, 2))"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#effect-of-bias",
    "href": "slides/10_fitting_ddm.html#effect-of-bias",
    "title": "10. Sitzung",
    "section": "Effect of Bias",
    "text": "Effect of Bias\n\n\nShow code\nd <- bind_rows(\n  rdiffusion(500, a = 2, v = 1.5, t0 = 0.2, z = 0.5) |> \n  mutate(type = \"unbiased\"),\n  rdiffusion(500, a = 2, v = 1.5, t0 = 0.2, z = 0.2) |> \n  mutate(type = \"biased\"))\n\n\n\n\nShow code\nd |> \n  ggplot(aes(rt, response, fill = response)) +\n  geom_violin() +\n  geom_jitter(height = 0.1, alpha = 0.5) +\n  scale_fill_viridis_d(option = \"B\", direction = -1, \n                       begin = 1/3, end = 3/3) +\n  xlim(c(0, 2)) +\n  facet_wrap(~type, ncol = 1)"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#zusammenfassen",
    "href": "slides/10_fitting_ddm.html#zusammenfassen",
    "title": "10. Sitzung",
    "section": "Zusammenfassen",
    "text": "Zusammenfassen\n\n\nShow code\nrts |> \n    group_by(response) |> \n    summarise(mean = mean(rt),\n              median = median(rt),\n              sd = sd(rt))\n\n\n# A tibble: 2 √ó 4\n  response  mean median     sd\n  <fct>    <dbl>  <dbl>  <dbl>\n1 lower    0.572  0.540 0.0889\n2 upper    0.766  0.731 0.151"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#maximum-likelihood-sch√§tzung",
    "href": "slides/10_fitting_ddm.html#maximum-likelihood-sch√§tzung",
    "title": "10. Sitzung",
    "section": "Maximum Likelihood Sch√§tzung",
    "text": "Maximum Likelihood Sch√§tzung\n\nDie Likelihood - genauer gesagt die Likelihood-Funktion - ist eine Funktion, die angibt, wie wahrscheinlich es ist, einen bestimmten Satz von Beobachtungen mit einem bestimmten Modell zu erhalten.\nWir betrachten die Menge der Beobachtungen (Daten) als gegeben.\nNun √ºberlegen wir, bei welchem Satz von Modellparametern wir die Daten am wahrscheinlichsten beobachten w√ºrden."
  },
  {
    "objectID": "slides/10_fitting_ddm.html#wahrscheinlichkeit-der-daten",
    "href": "slides/10_fitting_ddm.html#wahrscheinlichkeit-der-daten",
    "title": "10. Sitzung",
    "section": "Wahrscheinlichkeit der Daten",
    "text": "Wahrscheinlichkeit der Daten\nWenn wir eine Reihe von Datenpunkten haben (wie es in der Regel bei psychologischen Experimenten der Fall ist), k√∂nnen wir eine gemeinsame Wahrscheinlichkeit oder Wahrscheinlichkeitsdichte f√ºr die Daten in einem Datenvektor y erhalten, indem wir die einzelnen Wahrscheinlichkeiten oder Wahrscheinlichkeitsdichten miteinander multiplizieren, wobei wir davon ausgehen, dass die Beobachtungen in y unabh√§ngig sind:\n\\[\nf(\\bf{y}|\\bf{\\theta} = \\prod^k{f(y_k | \\bf{\\theta})})\n\\]\n\n\\(k\\) indiziert die Datenpunkte \\(y_k\\) im Vektor \\(\\bf{y}\\)."
  },
  {
    "objectID": "slides/10_fitting_ddm.html#likelihood-funktion",
    "href": "slides/10_fitting_ddm.html#likelihood-funktion",
    "title": "10. Sitzung",
    "section": "Likelihood Funktion",
    "text": "Likelihood Funktion\nDer Unterschied zwischen Wahrscheinlichkeit und Likelihood besteht darin, dass wir uns hier f√ºr die verschiedenen m√∂gliche Parameterwerte beziehen, und die Likelihood-Funktion sagt uns, wie wahrscheinlich jeder dieser Parameterwerte angesichts der von uns beobachteten Daten ist.\n\\[\nL(\\bf{\\theta} | \\bf{y}) = \\prod^k{L(\\bf{\\theta} | y_k})\n\\]"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#beisiel-binomial-likelihood",
    "href": "slides/10_fitting_ddm.html#beisiel-binomial-likelihood",
    "title": "10. Sitzung",
    "section": "Beisiel: Binomial likelihood",
    "text": "Beisiel: Binomial likelihood\nWir werfen eine M√ºnze:\n\nzwei m√∂gliche Ergebnisse (Kopf und Zahl),\nfeste Anzahl von ‚ÄúVersuchen‚Äù (100 M√ºnzw√ºrfe)\nfeste Wahrscheinlichkeit f√ºr ‚ÄúErfolg‚Äù (d.¬†h. Kopf)\n\n\n\nShow code\nset.seed(91)\n\nheads <- rbinom(1, 100, 0.5)\nheads\n\n\n[1] 48\n\n\nWahrschienlichkeit der Daten gegeben \\(p = 0.6\\)\n\n\nShow code\nbiased_prob <- 0.6\n\n# Using R's dbinom function (density function for a given binomial distribution)\ndbinom(heads, 100, biased_prob)\n\n\n[1] 0.004244495"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#beispiel-kartenspiel-zwischen-2-spielern",
    "href": "slides/10_fitting_ddm.html#beispiel-kartenspiel-zwischen-2-spielern",
    "title": "10. Sitzung",
    "section": "Beispiel Kartenspiel zwischen 2 Spielern",
    "text": "Beispiel Kartenspiel zwischen 2 Spielern\n\nSpieler A gewinnt 6 Mal in 9 Spielen:\n\n\n\nShow code\nwins <- 6\ngames <- 9\n\n\n\nWir definieren einen Vektor mit 100 m√∂glichen Parameterwerten:\n\n\n\nShow code\nn_points <- 100\np_grid <- seq( from=0.001 , to=0.999 , length.out = n_points )\n\n\n\nNun berechnen wir die Wahrscheinlichkeit der Daten, gegeben die Parameterwerte:\n\n\n\nShow code\nlikelihood <- dbinom(wins, size = games , prob = p_grid)"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#beispiel-kartenspiel-zwischen-2-spielern-1",
    "href": "slides/10_fitting_ddm.html#beispiel-kartenspiel-zwischen-2-spielern-1",
    "title": "10. Sitzung",
    "section": "Beispiel Kartenspiel zwischen 2 Spielern",
    "text": "Beispiel Kartenspiel zwischen 2 Spielern\n\n\nShow code\nplot(p_grid , likelihood, type=\"l\", main=\"Likelihood\", col = \"firebrick3\", lwd = 2)\n\n\n\n\nIndex des Parameterwertes, welcher die Likelihood maximiert:\n\n\n\nShow code\nwhich.max(likelihood)\n\n\n[1] 67\n\n\n\nParameterwert andiesem Index:\n\n\n\nShow code\np_grid[which.max(likelihood)]\n\n\n[1] 0.6663333"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#mle-negative-log-likelihood-minimieren",
    "href": "slides/10_fitting_ddm.html#mle-negative-log-likelihood-minimieren",
    "title": "10. Sitzung",
    "section": "MLE: Negative Log Likelihood minimieren",
    "text": "MLE: Negative Log Likelihood minimieren\n\nDefiniere eine Funktion, welche die negative log Wahrscheinlichkeitsfunktion f√ºr einen bestimmten Wert von p berechnet.\nSuche nach dem Wert von p, f√ºr den diese Funktion minimiert wird."
  },
  {
    "objectID": "slides/10_fitting_ddm.html#binomial-likelihood",
    "href": "slides/10_fitting_ddm.html#binomial-likelihood",
    "title": "10. Sitzung",
    "section": "Binomial Likelihood",
    "text": "Binomial Likelihood\n\\[\nf(k | p_{heads}, N) = \\binom{N}{k} p_{heads}^k (1-p_{heads}^{N-k})\n\\] \\[\nlnL(\\bf{\\theta} | \\bf{y}) = \\sum^k_{k=1}{ln L(\\bf{\\theta} | y_k})\n\\]"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#funktion-definieren",
    "href": "slides/10_fitting_ddm.html#funktion-definieren",
    "title": "10. Sitzung",
    "section": "Funktion definieren",
    "text": "Funktion definieren\n\n\nShow code\nloglik <- function(p){\n  likelihoods <- dbinom(heads, 100, p)\n   return(-sum(log(likelihoods)))\n}"
  },
  {
    "objectID": "slides/10_fitting_ddm.html#minimieren",
    "href": "slides/10_fitting_ddm.html#minimieren",
    "title": "10. Sitzung",
    "section": "Minimieren",
    "text": "Minimieren\n\n\nShow code\nnlm(loglik, 0.1, stepmax=0.1)\n\n\n$minimum\n[1] 2.530081\n\n$estimate\n[1] 0.4799995\n\n$gradient\n[1] 0\n\n$code\n[1] 1\n\n$iterations\n[1] 8\n\n\n\n\n\nüè† Neurowissenschaft Computerlab FS22"
  },
  {
    "objectID": "pages/exercises/exercise_01.html",
    "href": "pages/exercises/exercise_01.html",
    "title": "√úbung 1",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {√úbung 1},\n  date = {2022-02-22},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 2022. ‚Äú√úbung 1.‚Äù February 22, 2022. https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_01.html."
  },
  {
    "objectID": "pages/exercises/exercise_02.html",
    "href": "pages/exercises/exercise_02.html",
    "title": "√úbung 2",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {√úbung 2},\n  date = {2022-03-01},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 2022. ‚Äú√úbung 2.‚Äù March 1, 2022. https://kogpsy.github.io/neuroscicomplabFS22//pages/exercises/exercise_02.html."
  },
  {
    "objectID": "pages/exercises/exercise_03.html",
    "href": "pages/exercises/exercise_03.html",
    "title": "√úbung 3",
    "section": "",
    "text": "Note\n\n\n\nDie Aufgaben, die Sie bearbeiten sollen, finden Sie in einem gelben Kasten. Optionale Aufgaben sind in orangen K√§sten.\nIn diesem File finden Sie Beispielscode. Manche Zeilen enthalten ___. Hier m√ºssen Sie den Code vervollst√§ndigen.\nLaden Sie bitte Ihre L√∂sung als ZIP File bis Freitag, 25.03.2022, um 00:00 Uhr, in den Order f√ºr √úbung 3 auf ILIAS. Das ZIP File sollte ein R Skript enthalten, sowie den bereinigten Datensatz.\nNennen Sie Ihr File Matrikelnummer_Nachname_uebung-3.zip."
  },
  {
    "objectID": "pages/exercises/exercise_03.html#aufgaben",
    "href": "pages/exercises/exercise_03.html#aufgaben",
    "title": "√úbung 3",
    "section": "Aufgaben",
    "text": "Aufgaben\n\n\n\n\n\n\nNote\n\n\n\nAufgabe 1\n\nSpeichern Sie das CSV File in Ihren Projektordner.\nLesen Sie das CSV File ein. Per Konvention verwenden wir den Variablennamen d f√ºr den Datensatz.\n√úberpr√ºfen Sie, ob alle Variablen vorhanden sind. Verwenden Sie z.B. die Funktion glimpse().\nKonvertieren Sie die Gruppierungsvariablen subject und condition zu Faktoren.\n\n\n\n\nlibrary(tidyverse)\n\n\nd <- read_csv(\"___\")\n\nSchauen Sie sich die Variablen an:\n\nglimpse(d)\n\nKonvertieren Sie die Gruppierungsvariablen zu Faktoren:\n\nd <- d |>\n    mutate(___ = as_factor(___),\n           ___ = as_factor(___))\n\n\n\n\n\n\n\nNote\n\n\n\nAufgabe 2\nGibt es Versuchspersonen die in einer der Bedingungen Reaktionszeiten hat, welche mehr als zwei Standardabweichungen √ºber dem Bedingungsmittelwert liegen?\n\n\n\n# summary stats (means) for subjects/conditions\nsum_stats_participants <- d |>\n    group_by(___, ___) |>\n    dplyr::summarise(\n        mean_P = mean(___))\n\n\n# summary stats (means and SDs) for conditions\nsum_stats_conditions <- d |>\n    group_by(___) |>\n    dplyr::summarise(\n        mean_C = mean(__),\n        sd_C = sd(___))\n\n\nsum_stats_participants <-\n    full_join(\n        sum_stats_participants,\n        sum_stats_conditions,\n        by = \"condition\") |>\n    mutate(outlier_P = ___)\n\n\n# show outlier participants\nsum_stats_participants |>\n    filter(outlier_P == 1) |>\n    show()\n\n\nexcluded <- sum_stats_participants |>\n    filter(outlier_P == 1)\n\nexcluded\n\n\nd_cleaned <- d |>\n    filter(!(subject %in% excluded$subject)) |>\n    mutate(subject = fct_drop(subject))\n\n\n\n\n\n\n\nNote\n\n\n\nAufgabe 3\n\nGibt es einzelne Trials, in denen Versuchpersonen l√§nger als 4 Standardabweichungen √ºber dem Bedingungsmittelwert gebraucht haben, um zu Antworten?\nGibt es einzelne Trials, in denen Versuchpersonen zu schnell (unter 100 ms) geantwortet haben?\nSpeichern Sie den bearbeiteten Datensatz als CSV File.\n\n\n\n\nd_cleaned <- d_cleaned |>\n    full_join(\n        sum_stats_conditions,\n        by = \"condition\") |>\n    mutate(\n        trial_type = case_when(\n            ___ > ___ ~ \"too slow\",\n            ___ < ___ ~ \"too fast\",\n            TRUE ~ \"OK\") |>\n            factor(levels = c(\"OK\", \"too fast\", \"too slow\")))\n\n\nd_cleaned |>\n    ggplot(aes(x = trial_num, y = rt, color = trial_type, shape = trial_type)) +\n    geom_point(alpha = 0.6) +\n    facet_grid(~condition) +\n    scale_color_manual(values = c(\"gray70\", \"red\", \"steelblue\"))\n\n\nd_cleaned |>\n    filter(trial_type != \"OK\")\n\n\nd_cleaned <- d_cleaned |>\n    filter(trial_type == \"OK\") |>\n    select(subject, trial_num, condition, signal_present, correct, rt)\n\n\nd_cleaned |>\n    ggplot(aes(x = trial_num, y = rt)) +\n    geom_point(alpha = 0.6) +\n    facet_grid(~condition) +\n    scale_color_manual(values = c(\"gray70\", \"red\", \"steelblue\"))\n\n\nd_cleaned |> write_csv(___)\n\n\n\n\n\n\n\nTip\n\n\n\nOptionale Aufgabe\nDie Aufgaben oben bieten lediglich Voschl√§ge, wie man ‚ÄúAusreisser‚Äù identifizieren k√∂nnte. Wenn Sie andere Voschl√§ge haben, k√∂nnen Sie den Code anpassen, oder selber Code schreiben. K√∂nnen Sie Ihr Vorgehen begr√ºnden?"
  },
  {
    "objectID": "pages/exercises/exercise_04.html",
    "href": "pages/exercises/exercise_04.html",
    "title": "√úbung 4",
    "section": "",
    "text": "In dieser √úbung berechnen aus den Daten von 15 Versuchspersonen aus dem PsychoPy Experiment die Signal Detection Kennzahlen \\(d'\\), \\(k\\) und \\(c\\). Anschliessen berechnen Sie Mittelwerte der drei Bedingungen f√ºr \\(d'\\) und \\(c\\) unter Ber√ºcksichtigung der Messwiederholung."
  },
  {
    "objectID": "pages/exercises/exercise_04.html#variablen-bearbeiten",
    "href": "pages/exercises/exercise_04.html#variablen-bearbeiten",
    "title": "√úbung 4",
    "section": "Variablen bearbeiten",
    "text": "Variablen bearbeiten\nZu factor konvertieren, etc.\n\nd <- d |>\n    select(ID, condition, cue, direction, choice) |>\n    mutate(across(where(is.character), ~as_factor(.)),\n           cue = fct_relevel(cue, \"left\", \"none\", \"right\")) |>\n    drop_na()\n\n\nsdt <- d |>\n    mutate(type = case_when(\n        direction == \"right\" & choice == \"right\" ~ \"Hit\",\n        direction == \"right\" & choice == \"left\" ~ \"Miss\",\n        direction == \"left\" & choice == \"left\" ~ \"CR\",\n        direction == \"left\" & choice == \"right\" ~ \"FA\"))\n\nF√ºr jede Vpn in jeder der drei cue Bedingungen die verschiedenen Antworttypen z√§hlen.\n\nsdt_summary <- sdt |>\n    group_by(ID, cue) |>\n    count(type)\n\n\nsdt_summary\n\n# A tibble: 170 √ó 4\n# Groups:   ID, cue [45]\n   ID     cue   type      n\n   <fct>  <fct> <chr> <int>\n 1 chch04 left  CR       29\n 2 chch04 left  FA        3\n 3 chch04 left  Hit       7\n 4 chch04 left  Miss      1\n 5 chch04 none  CR       38\n 6 chch04 none  FA        2\n 7 chch04 none  Hit      34\n 8 chch04 none  Miss      6\n 9 chch04 right CR        5\n10 chch04 right FA        3\n# ‚Ä¶ with 160 more rows"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#von-wide-zu-long-konvertieren",
    "href": "pages/exercises/exercise_04.html#von-wide-zu-long-konvertieren",
    "title": "√úbung 4",
    "section": "Von wide zu long konvertieren",
    "text": "Von wide zu long konvertieren\n\nsdt_summary <- sdt_summary |>\n    pivot_wider(names_from = type, values_from = n)\n\n\nsdt_summary\n\n# A tibble: 45 √ó 6\n# Groups:   ID, cue [45]\n   ID     cue      CR    FA   Hit  Miss\n   <fct>  <fct> <int> <int> <int> <int>\n 1 chch04 left     29     3     7     1\n 2 chch04 none     38     2    34     6\n 3 chch04 right     5     3    25     7\n 4 chmi14 left     21    10     5     3\n 5 chmi14 none     18    19    29     7\n 6 chmi14 right     3     4    26     4\n 7 J      left     19    12     5     3\n 8 J      none     23    16    33     6\n 9 J      right     6     2    20    12\n10 jh     left     32    NA     5     3\n# ‚Ä¶ with 35 more rows"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#funktionen-definieren",
    "href": "pages/exercises/exercise_04.html#funktionen-definieren",
    "title": "√úbung 4",
    "section": "Funktionen definieren",
    "text": "Funktionen definieren\n\nreplace_NA <- function(x) {\n    x = ifelse(is.na(x), 0, x)\n    x\n}\n\ncorrect_zero_one <- function(x) {\n    if (identical(x, 0)) {\n        x = x + 0.001\n    } else if (identical(x, 1)) {\n        x = x - 0.001\n    }\n    x\n}"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#nas-ersetzen",
    "href": "pages/exercises/exercise_04.html#nas-ersetzen",
    "title": "√úbung 4",
    "section": "NAs ersetzen",
    "text": "NAs ersetzen\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(Hit, Miss, FA, CR), replace_NA))"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#hit-rate-und-false-alarm-rate-berechnen",
    "href": "pages/exercises/exercise_04.html#hit-rate-und-false-alarm-rate-berechnen",
    "title": "√úbung 4",
    "section": "Hit Rate und False Alarm Rate berechnen",
    "text": "Hit Rate und False Alarm Rate berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(hit_rate = ___,\n           fa_rate = ___)"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#werte-0-und-1-korrigieren",
    "href": "pages/exercises/exercise_04.html#werte-0-und-1-korrigieren",
    "title": "√úbung 4",
    "section": "Werte 0 und 1 korrigieren",
    "text": "Werte 0 und 1 korrigieren\n\nsdt_summary <- sdt_summary |>\n    mutate(across(c(hit_rate, fa_rate), correct_zero_one))"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#z-transformation",
    "href": "pages/exercises/exercise_04.html#z-transformation",
    "title": "√úbung 4",
    "section": "Z-Transformation",
    "text": "Z-Transformation\n\nsdt_summary <- sdt_summary |>\n    mutate(zhr = qnorm(hit_rate),\n           zfa = qnorm(fa_rate))\n\n\nsdt_summary\n\n# A tibble: 45 √ó 10\n# Groups:   ID, cue [45]\n   ID     cue      CR    FA   Hit  Miss hit_rate fa_rate   zhr     zfa\n   <fct>  <fct> <int> <dbl> <int> <dbl>    <dbl>   <dbl> <dbl>   <dbl>\n 1 chch04 left     29     3     7     1    0.875  0.0938 1.15  -1.32  \n 2 chch04 none     38     2    34     6    0.85   0.05   1.04  -1.64  \n 3 chch04 right     5     3    25     7    0.781  0.375  0.776 -0.319 \n 4 chmi14 left     21    10     5     3    0.625  0.323  0.319 -0.460 \n 5 chmi14 none     18    19    29     7    0.806  0.514  0.862  0.0339\n 6 chmi14 right     3     4    26     4    0.867  0.571  1.11   0.180 \n 7 J      left     19    12     5     3    0.625  0.387  0.319 -0.287 \n 8 J      none     23    16    33     6    0.846  0.410  1.02  -0.227 \n 9 J      right     6     2    20    12    0.625  0.25   0.319 -0.674 \n10 jh     left     32     0     5     3    0.625  0.001  0.319 -3.09  \n# ‚Ä¶ with 35 more rows"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#sdt-kennzahlen-berechnen",
    "href": "pages/exercises/exercise_04.html#sdt-kennzahlen-berechnen",
    "title": "√úbung 4",
    "section": "SDT Kennzahlen berechnen",
    "text": "SDT Kennzahlen berechnen\n\nsdt_summary <- sdt_summary |>\n    mutate(dprime = ___,\n           k = ___,\n           c = ___) |>\n    mutate(across(c(dprime, k, c), round, 2))"
  },
  {
    "objectID": "pages/exercises/exercise_04.html#variablen-ausw√§hlen",
    "href": "pages/exercises/exercise_04.html#variablen-ausw√§hlen",
    "title": "√úbung 4",
    "section": "Variablen ausw√§hlen",
    "text": "Variablen ausw√§hlen\n\nsdt_final <- sdt_summary |>\n    select(ID, cue, dprime, k, c)\n\nIm finalen Datensatz haben wir nun d', k und c f√ºr jede Person in jeder Bedingung.\n\nsdt_final\n\n# A tibble: 45 √ó 5\n# Groups:   ID, cue [45]\n   ID     cue   dprime     k     c\n   <fct>  <fct>  <dbl> <dbl> <dbl>\n 1 chch04 left    2.47  1.32  0.08\n 2 chch04 none    2.68  1.64  0.3 \n 3 chch04 right   1.1   0.32 -0.23\n 4 chmi14 left    0.78  0.46  0.07\n 5 chmi14 none    0.83 -0.03 -0.45\n 6 chmi14 right   0.93 -0.18 -0.65\n 7 J      left    0.61  0.29 -0.02\n 8 J      none    1.25  0.23 -0.4 \n 9 J      right   0.99  0.67  0.18\n10 jh     left    3.41  3.09  1.39\n# ‚Ä¶ with 35 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWir erwarten, dass sich d' zwischen den Bedingungen nicht unterscheidet. k und c (bias) sollte sich hingegen zwischen den cue Bedingungen unterscheiden. Uns interessiert hier vor allem c: in der neutralen Bedingung sollte c etwa 0 sein, in der ‚Äòleft‚Äô Bedingung sollte \\(c > 0\\) sein, und in der ‚Äòright‚Äô Bedingung sollte \\(c < 0\\) sein.\nVersuchen Sie die untenstehende Grafiken f√ºr d' und c zu reproduzieren.\n\n\n\n\n\n\n\nSie brauchen zuerst eine (separate) Zusammenfassung der d' und c Werte, welche die Messwiederholung respektiert. Sie k√∂nnen dazu die Funktion summarySEwithin aus dem Rmisc Package verwenden.\nDie Funktion braucht die Argumente measurevar, withinvars und idvar.\n\n\n\n\n\n\nArgument\nBeschreibung\n\n\n\nmeasurevar\nVariable, f√ºr welche eine Messwiederholung vorliegt\n\n\nwithinvars\nMesswiederholung\n\n\nidvar\nIdentit√§t der messwiederholten Einheit\n\n\n\n\ncs <- sdt_final |>\n    select(ID, cue, c) |>\n    ___\n\n\ndprimes <- sdt_final |>\n    select(ID, cue, ___) |>\n    ___\n\nWenn Sie, wie ich, die Datens√§tze mit den Mittelwerten, Standardfehlern und \\(95%\\) Konfidenzintervallen primes und cs genannt haben, k√∂nnen Sie die Plots beispielsweise so erstellen.\n\ncs |>\n    ggplot(aes(x = cue, y = c, group = 1)) + \n    geom_hline(yintercept = 0, \n               linetype = \"dashed\",\n               color = \"grey60\") +\n    geom_line() +\n    geom_errorbar(width = 0.1, aes(ymin = c - ci,\n                                   ymax = c + ci)) +\n    geom_point(shape = 21, size = 3, fill = \"white\") +\n    ggtitle(\"c (bias)\")\n\nFalls Sie wollen, k√∂nnen Sie die individuellen c Sch√§tzungen dem Plot hinzuf√ºgen, mit folgendem Code:\ngeom_jitter(aes(cue, c), data = sdt_final, width = 0.05)"
  },
  {
    "objectID": "pages/exercises/exercise_05.html",
    "href": "pages/exercises/exercise_05.html",
    "title": "√úbung 5",
    "section": "",
    "text": "Note\n\n\n\nDie Daten f√ºr diese √úbung finden Sie hier: üëâ Download Data\nDie Aufgaben, die Sie bearbeiten sollen, finden Sie in einem gelben Kasten. Optionale Aufgaben sind in orangen K√§sten.\nIn diesem File finden Sie Beispielscode. Manche Zeilen enthalten ___. Hier m√ºssen Sie den Code vervollst√§ndigen.\nLaden Sie bitte Ihre L√∂sung als R Skript bis Mittwoch, 27.4.2022, um 00:30 Uhr, in den Order f√ºr √úbung 5 auf ILIAS.\nNennen Sie Ihr File Matrikelnummer_Nachname_uebung-5.R."
  },
  {
    "objectID": "pages/exercises/exercise_05.html#stroop-task",
    "href": "pages/exercises/exercise_05.html#stroop-task",
    "title": "√úbung 5",
    "section": "Stroop Task",
    "text": "Stroop Task\nDer durchgef√ºhre Task ist eine Standardversion eines Stroop Tasks:\n\nStroop task. Participants performed the color-word version of the Stroop task (Botvinick et al.¬†2001; Gratton et al.¬†1992; Macleod 1991; Stroop 1935) comprised of congruent, incongruent, and neutral conditions while in the MR scanner. Participants were instructed to ignore the meaning of the printed word and respond to the ink color in which the word was printed. For example, in the congruent condition the words ‚ÄúRED,‚Äù ‚ÄúGREEN,‚Äù and ‚ÄúBLUE‚Äù were displayed in the ink colors red, green, and blue, respectively. In this condition, attentional demands were low because the ink color matched the prepotent response of reading the word, so response conflict was at a minimum. However, for the incongruent condition the printed words were different from the ink color in which they were printed (e.g., the word ‚ÄúRED‚Äù printed in blue ink). This condition elicited conflict because responding according to the printed word would result in an incorrect response. As a result, attentional demands were high and participants needed to inhibit the prepotent response of reading the word and respond according to the ink color in which the word was printed. On the other hand, the neutral condition consisted of noncolor words presented in an ink color (e.g., the word ‚ÄúCHAIR‚Äù printed in red ink) and had a low level of conflict and low attentional demands.\n\nVersuchspersonen musste mit einer Response Box eine von drei Tasten dr√ºcken. Es wurde festgehalten, ob die Antwort korrekt war.\n\nParticipants were instructed to respond to the ink color in which the text appeared by pressing buttons under the index, middle, and ring fingers on their right hand, each button corresponding to one of the three colors (red, green, and blue, respectively) on an MR-safe response box. The task was briefly practiced in the scanner to acquaint the participant with the task and to ensure understanding of the instructions. The task began with the presentation of a fixation cross hair for 1,000 ms followed by the Stroop stimulus for 2,000 ms, during which participants were instructed to respond as quickly as possible. A total of 120 trials were presented to each participant (42 congruent, 42 neutral, 36 incongruent). A lower number of incongruent trials was used in order to reduce the expectancy of a stimulus conflict relative to the other conditions."
  },
  {
    "objectID": "pages/exercises/exercise_05.html#datenanalyse",
    "href": "pages/exercises/exercise_05.html#datenanalyse",
    "title": "√úbung 5",
    "section": "Datenanalyse",
    "text": "Datenanalyse\nVon Interesse sind die Reaktionszeiten der korrekten Antworten in den Bedingungen kongruent, inkongruent und neutral.\n\nBehavioral analysis. The primary behavioral variable of interest was response time (RT), recorded as the time between cue onset and registered key press (in milliseconds). All first-level analyses were restricted to correct responses. To determine condition-level effects, a one-way repeated-measures ANOVA was used, as were post hoc one-sample t-tests."
  },
  {
    "objectID": "pages/exercises/exercise_05.html#daten-importieren",
    "href": "pages/exercises/exercise_05.html#daten-importieren",
    "title": "√úbung 5",
    "section": "Daten importieren",
    "text": "Daten importieren\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úî ggplot2 3.3.6     ‚úî purrr   0.3.4\n‚úî tibble  3.1.7     ‚úî dplyr   1.0.9\n‚úî tidyr   1.2.0     ‚úî stringr 1.4.0\n‚úî readr   2.1.2     ‚úî forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n\nAngenommen, Sie haben das CSV File in einem Subordner namens data gespeichert:\n\nd <- read_csv(\"data/stroop-data.csv\")\n\nRows: 3337 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): ID, condition, correct\ndbl (1): RT\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nglimpse(d)\n\nRows: 3,337\nColumns: 4\n$ ID        <chr> \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001‚Ä¶\n$ condition <chr> \"neutral\", \"congruent\", \"congruent\", \"neutral\", \"neutral\", \"‚Ä¶\n$ correct   <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", ‚Ä¶\n$ RT        <dbl> 1.186, 0.667, 0.614, 0.696, 0.752, 0.864, 0.793, 0.662, 0.71‚Ä¶\n\n\n\nd <- d |> \n    mutate(across(where(is.character), ~as_factor(.)),\n           condition = fct_relevel(condition, \"congruent\"))\n\nIm Datensatz befinden sich die Daten 28 Personen:\n\nlength(levels(d$ID))\n\n[1] 28\n\n\nDie Variable correct ist mit ‚ÄúY‚Äù und ‚ÄúN‚Äù codiert. Wir bevorzugen die Werte 1 und 0.\n\nd <- d |> \n  mutate(correct = if_else(correct == \"Y\", 1, 0))"
  },
  {
    "objectID": "pages/exercises/exercise_05.html#mittlere-rt-vs-fehlerrate",
    "href": "pages/exercises/exercise_05.html#mittlere-rt-vs-fehlerrate",
    "title": "√úbung 5",
    "section": "Mittlere RT vs Fehlerrate",
    "text": "Mittlere RT vs Fehlerrate\nWir plotten zuerst median RT versus Fehlerrate (beides pro VP/Bedingung), um uns einen √úberblick zu verschaffen.\n\nd_individual_summary <- d |> \n  group_by(ID, condition) |>        \n  summarize(RT = median(RT),\n            error_rate = 1 - mean(correct))\n\n`summarise()` has grouped output by 'ID'. You can override using the `.groups`\nargument.\n\nhead(d_individual_summary)\n\n# A tibble: 6 √ó 4\n# Groups:   ID [2]\n  ID    condition      RT error_rate\n  <fct> <fct>       <dbl>      <dbl>\n1 001   congruent   0.788     0     \n2 001   neutral     0.817     0.0952\n3 001   incongruent 0.953     0.167 \n4 002   congruent   0.774     0.146 \n5 002   neutral     0.839     0.286 \n6 002   incongruent 0.908     0.389 \n\n\n\nd_individual_summary |> \n  ggplot(aes(x = RT, y = error_rate)) +\n  geom_point() +\n  facet_wrap(~condition)\n\n\n\n\nEs sieht aus, als sei die Fehlerrate im Mittel in der inkongruenten Bedingung gr√∂sser als in den kongruenten neutralen Bedingungen. Dies ist zu erwarten.\nWir wollen hier nur die Reaktionszeiten korrekter Antworten analysieren. Filtern Sie den Datensatz, so dass nur noch die korrekten Antworten bleiben.\n\nd <- d |> \n  filter(correct == 1)"
  },
  {
    "objectID": "pages/exercises/exercise_05.html#rt-histogramme",
    "href": "pages/exercises/exercise_05.html#rt-histogramme",
    "title": "√úbung 5",
    "section": "RT Histogramme",
    "text": "RT Histogramme\nPlotten Sie mit folgender Funktion die RT Histogramme zuf√§llig ausgew√§hlter Personen (28 VP sind zuviele, um sie so in einer Grafik darzustellen).\n\nplot_hist <- function(d) {\n  d |> \n  ggplot(aes(x = RT,\n             fill = condition,\n             color = condition,)) + \n    geom_histogram(aes(y = ..density..), \n                   alpha = 0.5, bins = 30) + \n    # facet_grid(condition~ID) +\n      facet_wrap(~ID) +\n    coord_cartesian(xlim=c(0, 1.8)) +\n    scale_fill_viridis(discrete = TRUE, option = \"E\") +\n    scale_color_viridis(discrete = TRUE, option = \"E\")\n}\n\n\nd |> \n  filter(ID %in% sample(levels(ID), 4)) |> \n  plot_hist()"
  },
  {
    "objectID": "pages/exercises/exercise_05.html#aufgaben",
    "href": "pages/exercises/exercise_05.html#aufgaben",
    "title": "√úbung 5",
    "section": "Aufgaben",
    "text": "Aufgaben\nAufgabe 1\n\n\nBerechnen Sie die mittlere RT pro Peron/Bedingung, und stellen Sie die mittlere RT pro Bedingung, gemittelt √ºber Personen, mit Fehlerbalken grafisch dar.\nBeschreiben Sie in 1-2 S√§tzen, was Sie gefunden haben.\n\n\n\nby_subject <- d |> \n  group_by(___) |> \n  summarise(RT = ___)\n\nagg <- Rmisc::summarySEwithin(by_subject,\n                       measurevar = \"RT\",\n                       withinvars = \"condition\",\n                       idvar = \"ID\",\n                       na.rm = FALSE,\n                       conf.interval = .95)\n\n\nagg |> \n  ggplot(aes(condition, RT)) +\n  geom_line(aes(group = 1), linetype = 3) +   \n  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),\n                width = 0.1, size=1, color=\"black\") +\n  geom_point(size = 4) +\n  theme(legend.position = \"none\")\n\nAufgabe 2\n\n\nUntersuchen Sie mittels einer Shift Function den Unterschied zwischen den Bedingungen congruent und incongruent.\nBeschreiben Sie in 1-2 S√§tzen, was Sie gefunden haben.\nPratte et al. (2010) haben festgestellt, dass Stroop Effekte bei l√§ngeren Reaktionszeiten ansteigen. Ist diese Aussage mit Ihren Befunden vereinbar?\n\n\nZuerst entfernen wir die Faktorstufe neutral:\n\ndd <- d |> \n  filter(!(condition %in% \"neutral\")) |> \n  mutate(condition = fct_drop(condition))\n\nNun verwenden wir die Funktion hsf() aus dem rogme Package, um Shift Functions f√ºr jede Person in den beiden Bedingungen zu berechnen, und die gemittelten Differenzen der Dezile zu erhalten.\nDie Formel, welche Sie f√ºr die Funktion brauchen, lautet\nRT ~ condition + ID \nDiese Formel ist so zu lesen: RT Quantile werden durch condition und ID vorhergesagt.\n\nout <- dd |> \n  rogme::hsf(___)\n\nStellen Sie die Shift Function grafisch dar.\n\nrogme::plot_hsf(out)"
  },
  {
    "objectID": "pages/exercises/exercise_06.html",
    "href": "pages/exercises/exercise_06.html",
    "title": "√úbung 6",
    "section": "",
    "text": "Note\n\n\n\nDie Daten f√ºr diese √úbung finden Sie hier:\nüëâ Download Data\nüëâ Download True Parameters\nLaden Sie bitte Ihre L√∂sung als Word Dokument oder R Skript bis Dienstag, 24.5.2022, um 00:30 Uhr, in den Order f√ºr √úbung 6 auf ILIAS.\nNennen Sie Ihr File Matrikelnummer_Nachname_uebung-6.R oder Matrikelnummer_Nachname_uebung-6.docx."
  },
  {
    "objectID": "pages/exercises/exercise_06.html#daten-einlesen",
    "href": "pages/exercises/exercise_06.html#daten-einlesen",
    "title": "√úbung 6",
    "section": "Daten einlesen",
    "text": "Daten einlesen\nDownloaden Sie die Daten, und die wahren Parameter (zum Vergleich).\nWir laden die Daten in ein DataFrame, d. Ich habe ein RT Experiment simuliert, in dem 4 Versuchspersonen in 2 Bedingungen (‚ÄúA‚Äù und ‚ÄúB‚Äù) getestet wurden. Die experimentelle Manipulation sollte sich vor allem auf einen Parameter unterschieden. In dieser √úbung geht es darum, herauszufinden, auf welchen Parameter die Manipulation einen Einfluss hat."
  },
  {
    "objectID": "pages/exercises/exercise_06.html#aufgaben",
    "href": "pages/exercises/exercise_06.html#aufgaben",
    "title": "√úbung 6",
    "section": "Aufgaben",
    "text": "Aufgaben\n\n\n\n\n\n\nImportant\n\n\n\n\nF√ºhren Sie die Parametersch√§tzung mehrmals aus, und berichten Sie die resultierenden Parameter. Es kann sein, dass nicht immer dasselbe rauskommt (deshlab machen wir es mehrmals). Welcher Parameter wurde beeinflusst?\n√úberlegen Sie sich, wie Sie weiterf√ºhren w√ºrden. Wie k√∂nnen Sie zeigen, dass es in einem Parameter einen Unterschied zwischen den Bedingungen gibt? Wie k√∂nnen Sie zeigen, dass es in den anderen Parameter keine Unterschied gibt? Beschreiben Sie in einem Paragraphen, was Sie sich √ºberlegt haben.\n\n\n\n\nlibrary(tidyverse)\nlibrary(rtdists)\n\nd <- read_csv(\"ddm-data.csv\")\nparticipant_params <- read_csv(\"participant-params.csv\")"
  },
  {
    "objectID": "pages/exercises/exercise_06.html#daten-vorbereiten",
    "href": "pages/exercises/exercise_06.html#daten-vorbereiten",
    "title": "√úbung 6",
    "section": "Daten vorbereiten",
    "text": "Daten vorbereiten\nWir schauen uns die Daten an:\n\nd |> glimpse()\n\nRows: 9,600\nColumns: 4\n$ ID        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ condition <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", ‚Ä¶\n$ rt        <dbl> 0.6888584, 0.3411717, 1.6588621, 0.8845731, 0.7925147, 0.440‚Ä¶\n$ response  <chr> \"lower\", \"upper\", \"upper\", \"upper\", \"upper\", \"upper\", \"lower‚Ä¶\n\n\nDie Variablen ID, condition und response sollten Faktoren sein.\n\nd <- d |> \n  mutate(across(c(ID, condition, response), ~as_factor(.)))\n\nd |> glimpse()\n\nRows: 9,600\nColumns: 4\n$ ID        <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ condition <fct> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ‚Ä¶\n$ rt        <dbl> 0.6888584, 0.3411717, 1.6588621, 0.8845731, 0.7925147, 0.440‚Ä¶\n$ response  <fct> lower, upper, upper, upper, upper, upper, lower, upper, uppe‚Ä¶\n\n\nWeil es nur 4 VP sind, k√∂nnen wir die RTs von allen in einem Plot anschauen. Die upper Responses k√∂nnen wir hier als korrekte Antworten auffassen, die lower Responses als inkorrekte.\n\nd |>\n  ggplot(aes(rt, response, fill = response)) +\n  geom_violin() +\n  geom_jitter(height = 0.1, alpha = 0.2, size = 0.25) +\n  scale_fill_viridis_d(option = \"B\", direction = 1,\n                       begin = 1/2, end = 2/2) +\n  xlim(c(0, 1.5)) +\n  facet_grid(condition ~ ID)\n\nWarning: Removed 488 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 488 rows containing missing values (geom_point)."
  },
  {
    "objectID": "pages/exercises/exercise_06.html#daten-zusammenfassen",
    "href": "pages/exercises/exercise_06.html#daten-zusammenfassen",
    "title": "√úbung 6",
    "section": "Daten zusammenfassen",
    "text": "Daten zusammenfassen\nUM uns einen √úberblick zu verschaffen, fassen wir die Daten durch mittlere RT und ‚ÄúAccuracy‚Äù zusammen.\n\nsummary <- d |> group_by(ID, condition) |> \n  summarise(mean_rt = mean(rt),\n            median_rt = median(rt),\n            accuracy = mean(response == \"upper\"))\n\n`summarise()` has grouped output by 'ID'. You can override using the `.groups`\nargument.\n\nsummary\n\n# A tibble: 8 √ó 5\n# Groups:   ID [4]\n  ID    condition mean_rt median_rt accuracy\n  <fct> <fct>       <dbl>     <dbl>    <dbl>\n1 1     A           0.716     0.589    0.615\n2 1     B           0.643     0.536    0.863\n3 2     A           0.725     0.591    0.61 \n4 2     B           0.617     0.519    0.842\n5 3     A           0.843     0.696    0.598\n6 3     B           0.685     0.578    0.864\n7 4     A           0.715     0.591    0.602\n8 4     B           0.617     0.518    0.85 \n\n\n\nsummary |> \n  ggplot(aes(mean_rt, accuracy, color = condition)) +\n  geom_line(aes(group = ID), color = \"black\", linetype = \"dotted\") +\n  geom_point(size = 4) +\n  scale_color_viridis_d(end = 0.8)\n\n\n\n\nIn der Grafik sehen wir, dass die Accuracy in Bedingung ‚ÄúA‚Äù h√∂her ist. Gleichzeitig ist die mittlere RT niedriger. Das ist ein Hinweis, dass hier kein Speed-Accuracy Tradeoff vorliegt."
  },
  {
    "objectID": "pages/exercises/exercise_06.html#negative-loglikelihood-funktion-definieren",
    "href": "pages/exercises/exercise_06.html#negative-loglikelihood-funktion-definieren",
    "title": "√úbung 6",
    "section": "Negative loglikelihood Funktion definieren",
    "text": "Negative loglikelihood Funktion definieren\nNun wollen wir f√ºr jede Person in den beiden Bedingungen die Parameter des DDM fitten. Wir definieren daf√ºr eine Funktion, welche die negative log likelihood als Output hat.\n\ndiffusionloglik <- function(pars, condition, rt, response) {\n\n  conditions <- levels(condition)\n  likelihoods <- vector(\"numeric\", length(rt))\n\n\n  likelihoods <- ddiffusion(rt = rt,\n                             response = response,\n                             a = pars[\"a\"],\n                             v =  pars[\"v\"],\n                             t0 = pars[\"t0\"],\n                             z = pars[\"z\"] * pars[\"a\"],\n                             s = 1.0)\n  \n  if (any(likelihoods == 0)) return(1e6)\n  return(-sum(log(likelihoods)))\n}"
  },
  {
    "objectID": "pages/exercises/exercise_06.html#startwerte-f√ºr-maxmimum-likelihood-sch√§tzung",
    "href": "pages/exercises/exercise_06.html#startwerte-f√ºr-maxmimum-likelihood-sch√§tzung",
    "title": "√úbung 6",
    "section": "Startwerte f√ºr Maxmimum Likelihood Sch√§tzung",
    "text": "Startwerte f√ºr Maxmimum Likelihood Sch√§tzung\nF√ºr die Minimierung brauchen wir Anfangswerte. Diese werden f√ºr jeden zu sch√§tzenden Parameter zuf√§llig gew√§hlt.\n\ninit_params <- function() {\n  params <- c(a = runif(1, 0.2, 1.2),\n              v = rnorm(1, 0.5, 0.5),\n              z = runif(1, 0.45, 0.55),\n              t0 = runif(1, 0.01, 0.3))\n  params\n}"
  },
  {
    "objectID": "pages/exercises/exercise_06.html#maxmimum-likelihood-sch√§tzung",
    "href": "pages/exercises/exercise_06.html#maxmimum-likelihood-sch√§tzung",
    "title": "√úbung 6",
    "section": "Maxmimum Likelihood Sch√§tzung",
    "text": "Maxmimum Likelihood Sch√§tzung\nNun definieren wir zuerst ein paar Variablen.\n\nparticipants <- levels(d$ID)\nn_participants <- length(participants)\nconditions <- levels(d$condition)\nn_conditions <- length(conditions)\n\n# no. parameters (a, v, z, t0)\nn_pars <- length(init_params())\n\nUnd nun schreiben wir eine zweifache for-Loop, √ºber die Versuchspersonen, und √ºber die Bedingungen innerhalb der Personen.\nF√ºhren Sie diesen Teil mehrmals aus.\n\np <- vector(\"list\", n_participants)\n\nfor (i in seq_along(participants)) {\n  \n  estimates <- array(NA, c(n_conditions, n_pars))\n  colnames(estimates) <- c(\"a\", \"v\", \"z\", \"t0\")\n  rownames(estimates) <- c(\"A\", \"B\")\n  \n  for (j in seq_along(conditions)) {\n    data <- filter(d, ID == i, condition == conditions[j])\n  \n    fit <- nlminb(init_params(),\n                diffusionloglik,\n                lower = 0,\n                condition = data$condition,\n                rt = data$rt,\n                response = data$response)\n    \n    estimates[j, ] <- fit$par |> round(3)\n  }\n  p[[i]] <- estimates\n}"
  },
  {
    "objectID": "pages/exercises/exercise_06.html#gesch√§tzte-parameterwerte",
    "href": "pages/exercises/exercise_06.html#gesch√§tzte-parameterwerte",
    "title": "√úbung 6",
    "section": "Gesch√§tzte Parameterwerte",
    "text": "Gesch√§tzte Parameterwerte\nDie gesch√§tzten Parameter sind in einer List gespeichert.\n\np\n\n[[1]]\n      a     v     z    t0\nA 0.684 0.000 0.537 0.280\nB 0.208 0.069 0.453 0.246\n\n[[2]]\n      a     v     z    t0\nA 1.462 0.293 0.505 0.200\nB 1.078 0.389 0.473 0.293\n\n[[3]]\n      a     v     z    t0\nA 1.631 0.284 0.484 0.188\nB 1.583 1.268 0.469 0.192\n\n[[4]]\n      a     v     z    t0\nA 1.436 0.237 0.517 0.203\nB 1.439 1.207 0.497 0.196"
  },
  {
    "objectID": "pages/exercises/exercise_06.html#vergleich-mit-wahren-werten",
    "href": "pages/exercises/exercise_06.html#vergleich-mit-wahren-werten",
    "title": "√úbung 6",
    "section": "Vergleich mit wahren Werten",
    "text": "Vergleich mit wahren Werten\nDie gesch√§tzten Parameter k√∂nnen nun mit den ‚Äúwahren‚Äù Werten verglichen werden.\n\nparticipant_params\n\n# A tibble: 4 √ó 6\n     ID     a    v1    v2     z    t0\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1  1.48 0.301  1.19 0.515 0.199\n2     2  1.48 0.299  1.23 0.495 0.199\n3     3  1.57 0.277  1.21 0.484 0.197\n4     4  1.47 0.296  1.23 0.498 0.195"
  },
  {
    "objectID": "pages/solutions/solution_03.html",
    "href": "pages/solutions/solution_03.html",
    "title": "√úbung 3: L√∂sung",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{ellis2022,\n  author = {Andrew Ellis},\n  title = {√úbung 3: {L√∂sung}},\n  date = {04/03/2022},\n  url = {https://kogpsy.github.io/neuroscicomplabFS22//pages/solutions/solution_03.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Ellis. 4AD‚Äì3AD. ‚Äú√úbung 3: L√∂sung.‚Äù 4AD‚Äì3AD. https://kogpsy.github.io/neuroscicomplabFS22//pages/solutions/solution_03.html."
  }
]