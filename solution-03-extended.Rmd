---
title: "Übung 3: Lösung"
description: |
  Daten importieren und bereinigen.
date: "04-03-2022" 
author:
  - first_name: "Andrew"
    last_name: "Ellis"
    url: https://github.com/awellis
    affiliation: Kognitive Psychologie, Wahrnehmung und Methodenlehre, Universität Bern 
    affiliation_url: https://www.kog.psy.unibe.ch
    orcid_id: 0000-0002-2788-936X

citation_url: https://kogpsy.github.io/neuroscicomplab/solution-3.html
# slug: ellis2021overview
bibliography: bibliography.bib
output: 
    distill::distill_article:
      toc: true
      toc_float: true
      toc_depth: 2
      code_folding: false
---

```{r, xaringanExtra-clipboard, echo=FALSE, include=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clone fa-2x\" style=\"color: #301e64\"></i>",
    success_text = "<i class=\"fa fa-check fa-2x\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times fa-2x\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```




# Aufgabenstellung

In dieser Aufgabe bearbeiten Sie Daten aus einem Detektionssexperiment. Versuchspersonen mussten in zwei Bedingungen (`bias` und `no_bias`) ein Signal, welches in Rauschen eingebettet war, detektieren. Im Datensatz sind folgende Variablen:

```
subject: Subjekt ID
trial_num: Trialnummer, durchnummeriert in jeder Bedingung
condition: Bedingung (_Bias_ und _No Bias_)
signal_present: Indikatorvariable für Signal (0: absent, 1: present)
correct: Indikatorvariable für korrekte Antwort (0: incorrekt, 1: correct)
rt: Reaktionszeit in Sekunden

```



## Aufgaben 

:::exercise
**Aufgabe 1**
a) Speichern Sie das *CSV* File in Ihren Projektordner.

b) Lesen Sie das *CSV* File ein. Per Konvention verwenden wir den Variablennamen `d` für den Datensatz.

c) Überprüfen Sie, ob alle Variablen vorhanden sind. Verwenden Sie z.B. die Funktion `glimpse()`.

d) Konvertieren Sie die Gruppierungsvariablen `subject` und `condition` zu Faktoren.
:::


```{r}
library(tidyverse)
```

```{r eval=TRUE, include=TRUE}
d <- read_csv("data/data-exercise-03.csv")
```


Schauen Sie sich die Variablen an:

```{r eval=TRUE, include=TRUE}
glimpse(d)
```


Konvertieren Sie die Gruppierungsvariablen zu Faktoren:



```{r eval=TRUE, include=TRUE}
d <- d |>
    mutate(subject = as_factor(subject),
           condition = as_factor(condition))
```



:::exercise
**Aufgabe 2**

Gibt es Versuchspersonen die in einer der Bedingungen Reaktionszeiten hat, welche mehr als zwei Standardabweichungen **über** dem Bedingungsmittelwert liegen?
:::



```{r}
# summary stats (means) for subjects/conditions
sum_stats_participants <- d |>
    group_by(subject, condition) |>
    dplyr::summarise(
        mean_P = mean(rt))

```


```{r}
# summary stats (means and SDs) for conditions
sum_stats_conditions <- d |>
    group_by(condition) |>
    dplyr::summarise(
        mean_C = mean(rt),
        sd_C = sd(rt))
```

```{r}
sum_stats_participants <-
    full_join(
        sum_stats_participants,
        sum_stats_conditions,
        by = "condition") |>
    mutate(outlier_P = (mean_P - mean_C) > 2 * sd_C)
```



```{r}
# show outlier participants
sum_stats_participants |>
    filter(outlier_P == 1) |>
    show()
```

Es gibt keine Versuchsperson, deren mittlere Reaktionszeit in einer Bedingung mehr als zwei Standardabweichungen **über** dem Bedingungsmittelwert liegt. Dies bedeutet, dass sich in `excluded` keine Personen befinden, und der Dataframe folglich $0$ Zeilen hat.


```{r}
excluded <- sum_stats_participants |>
    filter(outlier_P == 1)

excluded
```

Der nächste Schritt wäre also nicht unbedingt notwendig.

```{r}
d_cleaned <- d |>
    filter(!(subject %in% excluded$subject)) |>
    mutate(subject = fct_drop(subject))
```



:::exercise
**Aufgabe 3**


a) Gibt es einzelne Trials, in denen Versuchpersonen länger als 4 Standardabweichungen über dem  Bedingungsmittelwert gebraucht haben, um zu Antworten?

b) Gibt es einzelne Trials, in denen Versuchpersonen zu schnell (unter 100 ms) geantwortet haben?

c) Speichern Sie den bearbeiteten Datensatz als _CSV_ File.
:::



__Zu Aufgabe 3.a)__

Wir wollen Trials identifizieren, bei denen Vpn länger gebraicht haben, als 4 Standardabweichungen über dem  Bedingungsmittelwert. Das bedeutet `(rt - mean_C) > 4 * sd_C`, und nicht `abs(rt - mean_C) > 4 * sd_C`. Letzteres würde auch Trials als Ausreisser identifizieren, welche 4 Standardabweichungen unter dem  Bedingungsmittelwert liegen.


__Zu Aufgabe 3.b)__

Die Reaktionszeiten sind hier in Sekunden, nicht Millisekunden. Dies bedeutet, wir brauchen `rt < 0.100`, und nicht `rt < 100`.

```{r}
d_cleaned <- d_cleaned |>
    full_join(
        sum_stats_conditions,
        by = "condition") |>
    mutate(
        trial_type = case_when(
            (rt - mean_C) > 4 * sd_C ~ "too slow",
            rt < 0.100 ~ "too fast",
            TRUE ~ "OK") |>
            factor(levels = c("OK", "too fast", "too slow")))

```

```{r}
d_cleaned |>
    ggplot(aes(x = trial_num, y = rt, color = trial_type, shape = trial_type)) +
    geom_point(alpha = 0.6) +
    facet_grid(~condition) +
    scale_color_manual(values = c("gray70", "red", "steelblue"))
```

```{r}
d_cleaned |>
    filter(trial_type != "OK")
```
Vor dem Entfernen der Ausreisser Trials haben wir 5756 Datenpunkte.

```{r}
nrow(d_cleaned)
```

```{r}
d_cleaned <- d_cleaned |>
    filter(trial_type == "OK") |>
    select(subject, trial_num, condition, signal_present, correct, rt)
```

Nach dem Entfernen haben wir noch 5591.

```{r}
nrow(d_cleaned)
```

```{r}
d_cleaned |>
    ggplot(aes(x = trial_num, y = rt)) +
    geom_point(alpha = 0.6) +
    facet_grid(~condition) +
    scale_color_manual(values = c("gray70", "red", "steelblue"))
```

```{r eval=FALSE, include=TRUE}
d_cleaned |> write_csv("data/data-cleaned.csv")
```



:::puzzle
**Optionale Aufgabe**


Die Aufgaben oben bieten lediglich Voschläge, wie man "Ausreisser" identifizieren könnte. Wenn Sie andere Voschläge haben, können Sie den Code anpassen, oder selber Code schreiben. Können Sie Ihr Vorgehen begründen?
:::



Eine Möglichkeit wäre, die Reaktionszeiten zuerst zu logarithmieren. 

```{r}
dlog <- tibble(x = seq(0, 10, 0.01),
               logx = log(x))
dlog |> 
  ggplot(aes(x, logx)) +
  geom_line()
```

Wenn wir danach auf der Logarithmusskala nach Trials suchen, welche weit weg vom Mittelwert liegen, werden wir vor allem diejenigen Trials finden, die zu schnell sind

```{r}
d <- d |> 
  mutate(logrt = log(rt))
```



```{r}
# summary stats (means and SDs) for conditions
sum_stats_conditions_logrt <- d |>
    group_by(condition) |>
    dplyr::summarise(
        mean_C = mean(logrt),
        sd_C = sd(logrt))
```

```{r}
d_cleaned_logrt <- d |>
    full_join(
        sum_stats_conditions_logrt,
        by = "condition") |>
    mutate(
        trial_type = case_when(
            abs(logrt - mean_C) > 3 * sd_C ~ "too far from mean",
            TRUE ~ "OK") |>
            factor(levels = c("OK", "too far from mean")))

```

```{r}
d_cleaned_logrt |>
    ggplot(aes(x = trial_num, y = rt, color = trial_type, shape = trial_type)) +
    geom_point(alpha = 0.6) +
    facet_grid(~condition) +
    scale_color_manual(values = c("gray70", "red"))
```

```{r}
d_cleaned_logrt |>
    filter(trial_type != "OK")
```

```{r}
d_cleaned_logrt <- d_cleaned_logrt |>
    filter(trial_type == "OK") |>
    select(subject, trial_num, condition, signal_present, correct, rt)
```
